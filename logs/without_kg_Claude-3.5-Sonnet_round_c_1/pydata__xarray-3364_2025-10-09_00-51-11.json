{
  "zero_patches": {
    "Location 1 (/root/hy/projects_1/xarray/xarray/core/concat.py lines 222-233)": "\n            # Check if all variables in opt exist in at least one dataset\n            all_vars = set().union(*[getattr(ds, subset) for ds in datasets])\n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
    "Location 2 (/root/hy/projects_1/xarray/xarray/core/concat.py lines 315-320)": "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    # This enables outer join behavior like pandas.concat()\n                    dims = tuple(d for d in ds.dims if d != dim)\n                    shape = tuple(ds.dims[d] for d in dims)\n                    if shape:\n                        data = utils.full_like(next(iter(ds.data_vars.values())).data, fill_value=fill_value, shape=shape, dtype=object)\n                    else:\n                        data = fill_value\n                    placeholder_var = Variable(dims, data)\n                    to_merge[var].append(placeholder_var)\n\n        for var in variables_to_merge:\n            result_vars[var] = unique_variable(\n                var, to_merge[var], compat=compat, equals=equals.get(var, None)\n            )\n    else:\n        result_vars = {}\n",
    "Location 3 (/root/hy/projects_1/xarray/xarray/core/concat.py lines 362-364)": "\n            if var.dims != common_dims:\n                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                # Handle missing variables by creating NaN-filled placeholders\n                vars_list = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars_list.append(ds.variables[k])\n                    else:\n                        # Create a placeholder variable filled with NaN\n                        template_var = datasets[0].variables[k]\n                        placeholder_dims = template_var.dims\n                        placeholder_shape = tuple(ds.dims.get(d, template_var.sizes[d]) \n                                                for d in placeholder_dims)\n                        placeholder_data = utils.full_like(template_var.data, fill_value=fill_value, \n                                                         shape=placeholder_shape, dtype=template_var.dtype)\n                        placeholder_var = Variable(placeholder_dims, placeholder_data, \n                                                 template_var.attrs)\n                        vars_list.append(placeholder_var)\n                vars = ensure_common_dims(vars_list)\n            except KeyError:\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
    "Location 4 (/root/hy/projects_1/xarray/xarray/core/concat.py lines 370-375)": "\n    result = Dataset(result_vars, attrs=result_attrs)\n    # Filter coord_names to only include coordinates that exist in result\n    existing_coord_names = coord_names & set(result.variables)\n    if existing_coord_names != coord_names:\n        # Only warn about missing coordinates, don't raise error\n        missing_coords = coord_names - existing_coord_names\n        import warnings\n        warnings.warn(\n            f\"Coordinates {missing_coords} are not present in all datasets and will be ignored.\",\n            UserWarning,\n            stacklevel=2\n        )\n    result = result.set_coords(existing_coord_names)\n",
    "Location 5 (/root/hy/projects_1/xarray/xarray/core/concat.py lines 190-196)": "\n                # all nonindexes that are not the same in each dataset\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                for k in all_vars:\n                    if k not in concat_over:\n                        # Compare the variable across all datasets that contain it\n                        # Perform the minimum amount of loads in order to avoid \n                        # multiple loads from disk while keeping the RAM footprint low.\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) <= 1:\n                            continue\n                            \n                        v_lhs = datasets_with_var[0].variables[k].load()\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_1/xarray/xarray/core/concat.py lines 222-233)": [
      "\n            # Check if all variables in opt exist in at least one dataset\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n        else:\n            # Check if variables exist in any dataset, not just the first one\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # Get all variables from all datasets for this subset\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # For outer join behavior, collect all variables from all datasets\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # Get all variables from all datasets for this subset\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n        else:\n            # Get all variables from all datasets for this subset\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # Check if variables exist in ANY dataset, not just the first one\n            all_vars = set().union(*[getattr(ds, subset) for ds in datasets])\n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # For outer join behavior, validate against all variables across all datasets\n            all_vars = set().union(*[getattr(ds, subset) for ds in datasets])\n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates in \"\n                        \"any of the datasets: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"in any of the datasets: %s\" % (invalid_vars,)\n                    )\n"
    ],
    "Location 2 (/root/hy/projects_1/xarray/xarray/core/concat.py lines 315-320)": [
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    if var in coord_names:\n                        # For coordinates, create an appropriate placeholder\n                        dims = ()  # scalar coordinate\n                        shape = ()\n                    else:\n                        # For data variables, infer dimensions from other datasets\n                        ref_var = None\n                        for other_ds in datasets:\n                            if var in other_ds.variables:\n                                ref_var = other_ds.variables[var]\n                                break\n                        if ref_var is not None:\n                            dims = ref_var.dims\n                            shape = tuple(ds.sizes.get(d, 1) for d in dims)\n                        else:\n                            dims = ()\n                            shape = ()\n                    \n                    import numpy as np\n                    data = np.full(shape, fill_value if fill_value is not None else np.nan)\n                    placeholder_var = Variable(dims, data)\n                    to_merge[var].append(placeholder_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    # Get the shape from the first available dataset that has this variable\n                    template_var = None\n                    for other_ds in datasets:\n                        if var in other_ds.variables:\n                            template_var = other_ds.variables[var]\n                            break\n                    \n                    if template_var is not None:\n                        # Create NaN-filled variable with same dims and dtype as template\n                        shape = tuple(ds.dims.get(d, template_var.sizes[d]) for d in template_var.dims)\n                        data = utils.full_like(template_var.data, fill_value=dtypes.NA, dtype=template_var.dtype)\n                        if hasattr(data, 'shape') and data.shape != shape:\n                            import numpy as np\n                            data = np.full(shape, fill_value=dtypes.NA, dtype=template_var.dtype)\n                        missing_var = Variable(template_var.dims, data, template_var.attrs)\n                        to_merge[var].append(missing_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    # Get the shape from the first available variable or use empty shape\n                    if to_merge[var]:\n                        ref_var = to_merge[var][0]\n                        shape = ref_var.shape\n                        dims = ref_var.dims\n                        dtype = ref_var.dtype\n                    else:\n                        # If this is the first dataset and variable is missing, \n                        # we need to find a reference from other datasets\n                        ref_var = None\n                        for other_ds in datasets:\n                            if var in other_ds.variables:\n                                ref_var = other_ds.variables[var]\n                                break\n                        if ref_var is not None:\n                            shape = ref_var.shape\n                            dims = ref_var.dims\n                            dtype = ref_var.dtype\n                        else:\n                            # Fallback: create minimal variable\n                            shape = ()\n                            dims = ()\n                            dtype = float\n                    \n                    # Create NaN-filled data\n                    import numpy as np\n                    if dtype.kind in 'SU':  # string types\n                        fill_value = ''\n                    elif dtype.kind in 'fc':  # float/complex types\n                        fill_value = np.nan\n                    else:\n                        fill_value = 0\n                    \n                    data = np.full(shape, fill_value, dtype=dtype)\n                    placeholder_var = Variable(dims, data)\n                    to_merge[var].append(placeholder_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    # Get the dimensions from a dataset that has this variable\n                    ref_var = None\n                    for ref_ds in datasets:\n                        if var in ref_ds.variables:\n                            ref_var = ref_ds.variables[var]\n                            break\n                    \n                    if ref_var is not None:\n                        # Create NaN-filled variable with same dims and dtype as reference\n                        shape = tuple(ds.dims.get(dim, 1) for dim in ref_var.dims)\n                        import numpy as np\n                        if ref_var.dtype.kind in 'fc':  # float or complex\n                            fill_val = np.nan\n                        else:\n                            fill_val = None\n                        \n                        data = np.full(shape, fill_val, dtype=ref_var.dtype)\n                        placeholder_var = Variable(ref_var.dims, data, ref_var.attrs.copy())\n                        to_merge[var].append(placeholder_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    # Get the shape from the first dataset that has this variable\n                    ref_var = None\n                    for ref_ds in datasets:\n                        if var in ref_ds.variables:\n                            ref_var = ref_ds.variables[var]\n                            break\n                    \n                    if ref_var is not None:\n                        # Create variable with same dims and dtype but filled with NaN\n                        shape = tuple(ds.dims.get(dim, ref_var.sizes[dim]) for dim in ref_var.dims)\n                        import numpy as np\n                        if ref_var.dtype.kind in 'fc':  # float or complex\n                            fill_value = np.nan\n                        else:\n                            fill_value = None\n                        \n                        data = np.full(shape, fill_value, dtype=ref_var.dtype)\n                        placeholder_var = Variable(ref_var.dims, data, ref_var.attrs.copy())\n                        to_merge[var].append(placeholder_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    # Use the same dims as the dataset's dimensions that are relevant\n                    dims = tuple(d for d in ds.dims if d in ds.variables.get(var, Variable((), None)).dims) if var in ds.variables else ()\n                    if not dims and var in coord_names:\n                        # For coordinates, try to infer dims from other datasets\n                        for other_ds in datasets:\n                            if var in other_ds.variables:\n                                dims = other_ds.variables[var].dims\n                                break\n                    if not dims:\n                        dims = ()\n                    \n                    shape = tuple(ds.sizes.get(d, 0) for d in dims)\n                    data = utils.NAN_LIKE_ARRAY if shape == () else pd.array([utils.NAN_LIKE_ARRAY] * (shape[0] if shape else 1)).reshape(shape)\n                    placeholder_var = Variable(dims, data)\n                    to_merge[var].append(placeholder_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    # Use the same dims as the first dataset that has this variable\n                    ref_var = None\n                    for ref_ds in datasets:\n                        if var in ref_ds.variables:\n                            ref_var = ref_ds.variables[var]\n                            break\n                    \n                    if ref_var is not None:\n                        # Create NaN-filled variable with same shape and dims as reference\n                        import numpy as np\n                        nan_data = np.full(ref_var.shape, np.nan, dtype=ref_var.dtype)\n                        nan_var = Variable(ref_var.dims, nan_data, ref_var.attrs)\n                        to_merge[var].append(nan_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    # Use the dimensions from the first dataset that has this variable\n                    ref_var = None\n                    for ref_ds in datasets:\n                        if var in ref_ds.variables:\n                            ref_var = ref_ds.variables[var]\n                            break\n                    \n                    if ref_var is not None:\n                        # Create dims that exist in current dataset, filling missing dims with size 1\n                        dims = []\n                        shape = []\n                        for dim in ref_var.dims:\n                            if dim in ds.dims:\n                                dims.append(dim)\n                                shape.append(ds.dims[dim])\n                            else:\n                                dims.append(dim)\n                                shape.append(1)\n                        \n                        # Create NaN-filled variable with appropriate shape and dtype\n                        import numpy as np\n                        if ref_var.dtype.kind in 'SU':  # string dtypes\n                            fill_value = ''\n                        else:\n                            fill_value = np.nan\n                        \n                        data = np.full(shape, fill_value, dtype=ref_var.dtype)\n                        placeholder_var = Variable(dims, data, attrs=ref_var.attrs.copy())\n                        to_merge[var].append(placeholder_var)\n"
    ],
    "Location 3 (/root/hy/projects_1/xarray/xarray/core/concat.py lines 362-364)": [
      "\n            if var.dims != common_dims:\n                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                # Handle missing variables by creating NaN-filled placeholders\n                vars_to_concat = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars_to_concat.append(ds.variables[k])\n                    else:\n                        # Create a variable filled with NaN for missing cases\n                        template_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                        missing_var = Variable(\n                            template_var.dims,\n                            utils.full_like(template_var, fill_value, template_var.dtype),\n                            template_var.attrs\n                        )\n                        vars_to_concat.append(missing_var)\n                vars = ensure_common_dims(vars_to_concat)\n            except KeyError:\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n            if var.dims != common_dims:\n                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                # Handle missing variables by creating NaN-filled placeholders\n                vars_list = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars_list.append(ds.variables[k])\n                    else:\n                        # Create a placeholder variable filled with NaN\n                        template_var = datasets[0].variables[k]\n                        placeholder_dims = template_var.dims\n                        placeholder_shape = tuple(ds.dims.get(d, template_var.sizes[d]) for d in placeholder_dims)\n                        placeholder_data = utils.full_like(template_var.data, fill_value=dtypes.NA, shape=placeholder_shape)\n                        placeholder_var = Variable(placeholder_dims, placeholder_data, template_var.attrs)\n                        vars_list.append(placeholder_var)\n                vars = ensure_common_dims(vars_list)\n            except KeyError:\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n            if var.dims != common_dims:\n                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                vars = ensure_common_dims([ds.variables[k] if k in ds.variables else None for ds in datasets])\n                # Filter out None values and handle missing variables\n                valid_vars = []\n                for i, var in enumerate(vars):\n                    if var is not None:\n                        valid_vars.append(var)\n                    else:\n                        # Create a placeholder variable filled with NaN for missing variables\n                        ref_var = next(v for v in vars if v is not None)\n                        fill_value = dtypes.NA if ref_var.dtype.kind in 'SU' else dtypes.get_fill_value(ref_var.dtype)\n                        placeholder_var = Variable(ref_var.dims, \n                                                 pd.array([fill_value] * ref_var.size).reshape(ref_var.shape),\n                                                 attrs=ref_var.attrs)\n                        valid_vars.append(placeholder_var)\n                vars = ensure_common_dims(valid_vars)\n            except KeyError:\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n            if var.dims != common_dims:\n                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                # Handle missing variables by creating NaN-filled placeholders\n                vars_list = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars_list.append(ds.variables[k])\n                    else:\n                        # Create a placeholder variable filled with NaN\n                        var_dims = ds.variables[list(ds.variables.keys())[0]].dims if ds.variables else ()\n                        var_shape = tuple(ds.dims.get(d, 1) for d in var_dims)\n                        placeholder = Variable(var_dims, utils.NumpyVIndexAdapter(pd.array([pd.NA] * pd.array(var_shape).prod()).reshape(var_shape)))\n                        vars_list.append(placeholder)\n                vars = ensure_common_dims(vars_list)\n            except KeyError:\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n            try:\n                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            except KeyError:\n                # Handle missing variables by creating NaN-filled placeholders\n                vars = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars.append(ds.variables[k])\n                    else:\n                        # Create a placeholder variable filled with NaN\n                        # Use the same dims as the first available variable of this name\n                        ref_var = next((d.variables[k] for d in datasets if k in d.variables), None)\n                        if ref_var is not None:\n                            # Create NaN-filled variable with same shape as reference\n                            shape = tuple(ds.dims.get(d, ref_var.sizes[d]) for d in ref_var.dims)\n                            data = utils.full_like(ref_var.data, fill_value=dtypes.NA, shape=shape)\n                            placeholder = Variable(ref_var.dims, data, ref_var.attrs)\n                            vars.append(placeholder)\n                vars = ensure_common_dims(vars)\n",
      "\n            if var.dims != common_dims:\n                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                vars = ensure_common_dims([ds.variables[k] if k in ds.variables else \n                                         Variable(ds.variables[list(ds.variables.keys())[0]].dims, \n                                                pd.array([fill_value] * ds.variables[list(ds.variables.keys())[0]].size).reshape(ds.variables[list(ds.variables.keys())[0]].shape))\n                                         for ds in datasets])\n            except KeyError:\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n            try:\n                vars = ensure_common_dims([ds.variables.get(k, None) for ds in datasets])\n                # Filter out None values for missing variables\n                vars = [var for var in vars if var is not None]\n                if not vars:\n                    continue  # Skip if variable is missing in all datasets\n            except KeyError:\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n",
      "\n            if var.dims != common_dims:\n                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                # Handle missing variables by creating NaN-filled placeholders\n                vars_to_concat = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars_to_concat.append(ds.variables[k])\n                    else:\n                        # Create a placeholder variable filled with NaN\n                        template_var = datasets[0].variables[k]\n                        shape = tuple(ds.dims.get(d, 1) if d != dim else 1 \n                                    for d in template_var.dims)\n                        placeholder = Variable(\n                            template_var.dims,\n                            utils.full_like(template_var.data, fill_value, shape=shape),\n                            template_var.attrs\n                        )\n                        vars_to_concat.append(placeholder)\n                vars = ensure_common_dims(vars_to_concat)\n            except KeyError:\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n"
    ],
    "Location 4 (/root/hy/projects_1/xarray/xarray/core/concat.py lines 370-375)": [
      "\n    result = Dataset(result_vars, attrs=result_attrs)\n    # Filter coord_names to only include coordinates that actually exist in result\n    existing_coord_names = coord_names.intersection(set(result.variables))\n    absent_coord_names = coord_names - existing_coord_names\n    if absent_coord_names:\n        # Only warn about truly problematic coordinates - those that are coordinates\n        # in some datasets but completely missing from the concatenation result\n        problematic_coords = set()\n        for coord_name in absent_coord_names:\n            # Check if this coordinate exists in any dataset but was lost during concat\n            exists_somewhere = any(coord_name in ds.coords for ds in datasets)\n            if exists_somewhere:\n                problematic_coords.add(coord_name)\n        \n        if problematic_coords:\n            raise ValueError(\n                \"Variables %r are coordinates in some datasets but not others.\"\n                % problematic_coords\n            )\n    result = result.set_coords(existing_coord_names)\n",
      "\n    result = Dataset(result_vars, attrs=result_attrs)\n    # For outer join behavior, only validate coordinates that exist in result\n    present_coord_names = coord_names & set(result.variables)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names and compat == \"identical\":\n        raise ValueError(\n            \"Variables %r are coordinates in some datasets but not others.\"\n            % absent_coord_names\n        )\n",
      "\n    result = Dataset(result_vars, attrs=result_attrs)\n    \n    # Handle missing coordinates gracefully - only require coordinates that exist in result\n    present_coord_names = coord_names & set(result.variables)\n    absent_coord_names = coord_names - set(result.variables)\n    \n    if absent_coord_names:\n        # Only warn about truly problematic cases where coordinates exist in some datasets\n        # but are completely missing from the concatenated result\n        problematic_coords = []\n        for coord_name in absent_coord_names:\n            # Check if this coordinate exists in any dataset but wasn't included in result\n            exists_somewhere = any(coord_name in ds.coords for ds in datasets)\n            if exists_somewhere:\n                problematic_coords.append(coord_name)\n        \n        if problematic_coords:\n            raise ValueError(\n                \"Variables %r are coordinates in some datasets but not others.\"\n                % problematic_coords\n            )\n    \n    # Only set coordinates that actually exist in the result\n    if present_coord_names:\n        result = result.set_coords(present_coord_names)\n",
      "\n    result = Dataset(result_vars, attrs=result_attrs)\n    # Handle missing coordinates gracefully - only require coordinates that exist in result\n    present_coord_names = coord_names & set(result.variables)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names:\n        # Only warn or create placeholder coordinates instead of raising error\n        import warnings\n        warnings.warn(\n            f\"Coordinates {absent_coord_names} are present in some datasets but not others. \"\n            \"They will be excluded from the result.\",\n            UserWarning\n        )\n    result = result.set_coords(present_coord_names)\n",
      "\n    result = Dataset(result_vars, attrs=result_attrs)\n    # Handle missing coordinates gracefully - only validate coordinates that should exist\n    present_coord_names = coord_names & set(result.variables)\n    absent_coord_names = coord_names - set(result.variables)\n    \n    # Only raise error if fill_value is None and we have missing coordinates\n    if absent_coord_names and fill_value is None:\n        raise ValueError(\n            \"Variables %r are coordinates in some datasets but not others. \"\n            \"Consider using fill_value parameter to handle missing coordinates.\"\n            % absent_coord_names\n        )\n",
      "\n    result = Dataset(result_vars, attrs=result_attrs)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names:\n        # Handle missing coordinates by creating them with NaN values\n        for coord_name in absent_coord_names:\n            # Find a dataset that has this coordinate to get its properties\n            coord_var = None\n            for ds in datasets:\n                if coord_name in ds.coords:\n                    coord_var = ds.coords[coord_name]\n                    break\n            \n            if coord_var is not None:\n                # Create coordinate with NaN values for missing entries\n                if dim in coord_var.dims:\n                    # This coordinate varies along the concat dimension\n                    result_vars[coord_name] = coord_var\n                else:\n                    # This coordinate is constant, use first available value\n                    result_vars[coord_name] = coord_var\n        \n        # Recreate result with added coordinates\n        result = Dataset(result_vars, attrs=result_attrs)\n",
      "\n    result = Dataset(result_vars, attrs=result_attrs)\n    # Handle missing coordinates gracefully - allow outer join behavior\n    present_coord_names = coord_names & set(result.variables)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names and compat == 'no_conflicts':\n        # Only raise error in strict compatibility mode\n        raise ValueError(\n            \"Variables %r are coordinates in some datasets but not others.\"\n            % absent_coord_names\n        )\n    # Only set coordinates that are actually present in the result\n    if present_coord_names:\n        result = result.set_coords(present_coord_names)\n",
      "\n    result = Dataset(result_vars, attrs=result_attrs)\n    # Only validate coordinates that actually exist in the result\n    existing_coord_names = coord_names & set(result.variables)\n    absent_coord_names = coord_names - set(result.variables)\n    \n    # For absent coordinates, create them as index coordinates if they exist in any dataset\n    for coord_name in absent_coord_names:\n        for ds in datasets:\n            if coord_name in ds.coords:\n                # Create a coordinate filled with NaN for datasets that don't have it\n                coord_vars = []\n                for dataset in datasets:\n                    if coord_name in dataset.coords:\n                        coord_vars.append(dataset.coords[coord_name].variable)\n                    else:\n                        # Create NaN-filled coordinate with appropriate shape\n                        coord_var = dataset.coords[coord_name].variable if coord_name in dataset.coords else None\n                        if coord_var is None:\n                            # Find a reference coordinate from another dataset\n                            ref_coord = next((d.coords[coord_name] for d in datasets if coord_name in d.coords), None)\n                            if ref_coord is not None:\n                                import numpy as np\n                                shape = tuple(dataset.dims.get(d, ref_coord.sizes.get(d, 1)) for d in ref_coord.dims)\n                                data = np.full(shape, np.nan, dtype=ref_coord.dtype)\n                                coord_vars.append(Variable(ref_coord.dims, data))\n                if coord_vars:\n                    combined_coord = concat_vars(coord_vars, dim, positions)\n                    result_vars[coord_name] = combined_coord\n                break\n    \n    # Update result with any new coordinate variables\n    if absent_coord_names:\n        result = Dataset(result_vars, attrs=result_attrs)\n    \n    result = result.set_coords(existing_coord_names | (absent_coord_names & set(result.variables)))\n"
    ],
    "Location 5 (/root/hy/projects_1/xarray/xarray/core/concat.py lines 190-196)": [
      "\n                # all nonindexes that are not the same in each dataset\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                for k in all_vars:\n                    if k not in concat_over:\n                        # Compare the variable across all datasets that contain it\n                        # Perform the minimum amount of loads in order to avoid \n                        # multiple loads from disk while keeping the RAM footprint low.\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) < len(datasets):\n                            # Variable is missing in some datasets, add to concat_over\n                            concat_over.add(k)\n                            equals[k] = False\n                        elif len(datasets_with_var) > 1:\n                            v_lhs = datasets_with_var[0].variables[k].load()\n                            # We'll need to know later on if variables are equal.\n                            computed = []\n                            for ds_rhs in datasets_with_var[1:]:\n                                v_rhs = ds_rhs.variables[k].compute()\n                                computed.append(v_rhs)\n                                if not getattr(v_lhs, compat)(v_rhs):\n                                    concat_over.add(k)\n                                    equals[k] = False\n                                    break\n",
      "\n                # all nonindexes that are not the same in each dataset\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                for k in all_vars:\n                    if k not in concat_over:\n                        # Compare the variable across all datasets that have it\n                        # Perform the minimum amount of loads in order to avoid \n                        # multiple loads from disk while keeping the RAM footprint low.\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) <= 1:\n                            continue\n                            \n                        v_lhs = datasets_with_var[0].variables[k].load()\n",
      "\n                # all nonindexes that are not the same in each dataset\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                for k in all_vars:\n                    if k not in concat_over:\n                        # Compare the variable across all datasets that contain it\n                        # Perform the minimum amount of loads in order to avoid \n                        # multiple loads from disk while keeping the RAM footprint low.\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) <= 1:\n                            continue  # Variable only exists in one dataset, skip comparison\n                            \n                        v_lhs = datasets_with_var[0].variables[k].load()\n",
      "\n                # all nonindexes that are not the same in each dataset\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                for k in all_vars:\n                    if k not in concat_over:\n                        # Compare the variable across all datasets that contain it\n                        # Perform the minimum amount of loads to avoid multiple disk loads\n                        # while keeping the RAM footprint low.\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) <= 1:\n                            continue  # Variable exists in only one dataset, no comparison needed\n                        \n                        v_lhs = datasets_with_var[0].variables[k].load()\n",
      "\n                # all nonindexes that are not the same in each dataset\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                for k in all_vars:\n                    if k not in concat_over:\n                        # Compare the variable across all datasets that have it\n                        # Perform the minimum amount of loads in order to avoid \n                        # multiple loads from disk while keeping the RAM footprint low.\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) < len(datasets):\n                            # Variable is missing from some datasets, add to concat_over\n                            concat_over.add(k)\n                            equals[k] = False\n                            continue\n                            \n                        v_lhs = datasets_with_var[0].variables[k].load()\n",
      "\n                # all nonindexes that are not the same in each dataset\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                for k in all_vars:\n                    if k not in concat_over:\n                        # Compare the variable across all datasets that contain it\n                        # Perform the minimum amount of loads in order to avoid \n                        # multiple loads from disk while keeping the RAM footprint low.\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) <= 1:\n                            continue\n                            \n                        v_lhs = datasets_with_var[0].variables[k].load()\n",
      "\n                # all nonindexes that are not the same in each dataset\n                all_subset_vars = set()\n                for ds in datasets:\n                    all_subset_vars.update(getattr(ds, subset))\n                \n                for k in all_subset_vars:\n                    if k not in concat_over:\n                        # Compare the variable across all datasets that contain it\n                        # Perform the minimum amount of loads in order to avoid \n                        # multiple loads from disk while keeping the RAM footprint low.\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) < len(datasets):\n                            # Variable is missing in some datasets, add to concat_over\n                            concat_over.add(k)\n                            equals[k] = False\n                            continue\n                            \n                        v_lhs = datasets_with_var[0].variables[k].load()\n",
      "\n                # all nonindexes that are not the same in each dataset\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                for k in all_vars:\n                    if k not in concat_over:\n                        # Compare the variable across all datasets that contain it\n                        # Perform the minimum amount of loads in order to avoid \n                        # multiple loads from disk while keeping the RAM footprint low.\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) < len(datasets):\n                            # Variable is missing from some datasets, include it for concat\n                            concat_over.add(k)\n                            equals[k] = False\n                            continue\n                            \n                        v_lhs = datasets_with_var[0].variables[k].load()\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..01c848bb 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,13 +187,26 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # Check variables across ALL datasets, not just the first\n+                all_subset_vars = set()\n+                for ds in datasets:\n+                    all_subset_vars.update(getattr(ds, subset))\n+                \n+                for k in all_subset_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        # Compare the variable across all datasets that have it\n+                        # Find the first dataset that has this variable\n+                        v_lhs = None\n+                        for ds in datasets:\n+                            if k in ds.variables:\n+                                v_lhs = ds.variables[k].load()\n+                                break\n+                        \n+                        if v_lhs is None:\n+                            continue\n+                            \n+                        # Compare with other datasets that have this variable\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n@@ -219,17 +232,22 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # Check if variables exist in ANY dataset, not just the first\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +330,34 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n+\n+            # Handle missing variables gracefully for outer join behavior\n+            present_merge_vars = variables_to_merge & set(ds.variables)\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+            \n+            # Only process variables that are present in this dataset\n+            for var in present_merge_vars:\n+                to_merge[var].append(ds.variables[var])\n+            \n+            # For absent variables, create placeholder with NaN values\n+            for var in absent_merge_vars:\n+                # Create a placeholder variable filled with NaN\n+                if var in coord_names:\n+                    # For coordinates, we'll handle them separately\n+                    continue\n+                else:\n+                    # Create a dummy variable with appropriate dimensions\n+                    # We'll use the first dataset's dimensions as a template\n+                    template_var = None\n+                    for other_ds in datasets:\n+                        if var in other_ds.variables:\n+                            template_var = other_ds.variables[var]\n+                            break\n+                    if template_var is not None:\n+                        import numpy as np\n+                        dummy_data = np.full_like(template_var.data, fill_value, dtype=template_var.dtype)\n+                        dummy_var = Variable(template_var.dims, dummy_data, template_var.attrs)\n+                        to_merge[var].append(dummy_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -359,19 +399,30 @@ def _dataset_concat(\n     for k in datasets[0].variables:\n         if k in concat_over:\n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+\n+                vars = ensure_common_dims([ds.variables[k] if k in ds.variables else None for ds in datasets])\n+                # Filter out None values and handle missing variables\n+                vars = [v for v in vars if v is not None]\n+                if not vars:\n+                    raise ValueError(\"%r is not present in any datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n \n     result = Dataset(result_vars, attrs=result_attrs)\n-    absent_coord_names = coord_names - set(result.variables)\n-    if absent_coord_names:\n+\n+    # Allow missing coordinate variables for outer join behavior\n+    # Only warn about coordinates that are completely absent from all datasets\n+    all_coord_names = set()\n+    for ds in datasets:\n+        all_coord_names.update(ds.coords)\n+    \n+    truly_absent_coords = coord_names - all_coord_names\n+    if truly_absent_coords:\n+        # Only raise error for coordinates that don't exist in any dataset\n         raise ValueError(\n-            \"Variables %r are coordinates in some datasets but not others.\"\n-            % absent_coord_names\n+            \"Variables %r are not coordinates in any dataset.\"\n+            % truly_absent_coords\n         )\n     result = result.set_coords(coord_names)\n     result.encoding = result_encoding\n",
    "variant_0": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..bac7e0b1 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,13 +187,22 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # all nonindexes that are not the same in each dataset\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        # Compare the variable across all datasets that contain it\n+                        # Perform the minimum amount of loads in order to avoid \n+                        # multiple loads from disk while keeping the RAM footprint low.\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) <= 1:\n+                            continue\n+                            \n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n@@ -219,17 +228,20 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # Check if all variables in opt exist in at least one dataset\n+            all_vars = set().union(*[getattr(ds, subset) for ds in datasets])\n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +324,36 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    # This enables outer join behavior like pandas.concat()\n+                    dims = tuple(d for d in ds.dims if d != dim)\n+                    shape = tuple(ds.dims[d] for d in dims)\n+                    if shape:\n+                        data = utils.full_like(next(iter(ds.data_vars.values())).data, fill_value=fill_value, shape=shape, dtype=object)\n+                    else:\n+                        data = fill_value\n+                    placeholder_var = Variable(dims, data)\n+                    to_merge[var].append(placeholder_var)\n+\n+        for var in variables_to_merge:\n+            result_vars[var] = unique_variable(\n+                var, to_merge[var], compat=compat, equals=equals.get(var, None)\n+            )\n+    else:\n+        result_vars = {}\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -359,20 +395,58 @@ def _dataset_concat(\n     for k in datasets[0].variables:\n         if k in concat_over:\n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+\n+            if var.dims != common_dims:\n+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+                var = var.set_dims(common_dims, common_shape)\n+            yield var\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    for k in datasets[0].variables:\n+        if k in concat_over:\n+            try:\n+                # Handle missing variables by creating NaN-filled placeholders\n+                vars_list = []\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars_list.append(ds.variables[k])\n+                    else:\n+                        # Create a placeholder variable filled with NaN\n+                        template_var = datasets[0].variables[k]\n+                        placeholder_dims = template_var.dims\n+                        placeholder_shape = tuple(ds.dims.get(d, template_var.sizes[d]) \n+                                                for d in placeholder_dims)\n+                        placeholder_data = utils.full_like(template_var.data, fill_value=fill_value, \n+                                                         shape=placeholder_shape, dtype=template_var.dtype)\n+                        placeholder_var = Variable(placeholder_dims, placeholder_data, \n+                                                 template_var.attrs)\n+                        vars_list.append(placeholder_var)\n+                vars = ensure_common_dims(vars_list)\n             except KeyError:\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n \n     result = Dataset(result_vars, attrs=result_attrs)\n-    absent_coord_names = coord_names - set(result.variables)\n-    if absent_coord_names:\n-        raise ValueError(\n-            \"Variables %r are coordinates in some datasets but not others.\"\n-            % absent_coord_names\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n+    # Filter coord_names to only include coordinates that exist in result\n+    existing_coord_names = coord_names & set(result.variables)\n+    if existing_coord_names != coord_names:\n+        # Only warn about missing coordinates, don't raise error\n+        missing_coords = coord_names - existing_coord_names\n+        import warnings\n+        warnings.warn(\n+            f\"Coordinates {missing_coords} are not present in all datasets and will be ignored.\",\n+            UserWarning,\n+            stacklevel=2\n         )\n+    result = result.set_coords(existing_coord_names)\n     result = result.set_coords(coord_names)\n     result.encoding = result_encoding\n \n",
    "variant_1": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..2e834e33 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,13 +187,33 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # all nonindexes that are not the same in each dataset\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        # Compare the variable across all datasets that contain it\n+                        # Perform the minimum amount of loads in order to avoid \n+                        # multiple loads from disk while keeping the RAM footprint low.\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) < len(datasets):\n+                            # Variable is missing in some datasets, add to concat_over\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                        elif len(datasets_with_var) > 1:\n+                            v_lhs = datasets_with_var[0].variables[k].load()\n+                            # We'll need to know later on if variables are equal.\n+                            computed = []\n+                            for ds_rhs in datasets_with_var[1:]:\n+                                v_rhs = ds_rhs.variables[k].compute()\n+                                computed.append(v_rhs)\n+                                if not getattr(v_lhs, compat)(v_rhs):\n+                                    concat_over.add(k)\n+                                    equals[k] = False\n+                                    break\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n@@ -219,17 +239,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # Check if all variables in opt exist in at least one dataset\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +338,42 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    if var in coord_names:\n+                        # For coordinates, create an appropriate placeholder\n+                        dims = ()  # scalar coordinate\n+                        shape = ()\n+                    else:\n+                        # For data variables, infer dimensions from other datasets\n+                        ref_var = None\n+                        for other_ds in datasets:\n+                            if var in other_ds.variables:\n+                                ref_var = other_ds.variables[var]\n+                                break\n+                        if ref_var is not None:\n+                            dims = ref_var.dims\n+                            shape = tuple(ds.sizes.get(d, 1) for d in dims)\n+                        else:\n+                            dims = ()\n+                            shape = ()\n+                    \n+                    import numpy as np\n+                    data = np.full(shape, fill_value if fill_value is not None else np.nan)\n+                    placeholder_var = Variable(dims, data)\n+                    to_merge[var].append(placeholder_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -359,20 +415,63 @@ def _dataset_concat(\n     for k in datasets[0].variables:\n         if k in concat_over:\n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+\n+            if var.dims != common_dims:\n+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+                var = var.set_dims(common_dims, common_shape)\n+            yield var\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    for k in datasets[0].variables:\n+        if k in concat_over:\n+            try:\n+                # Handle missing variables by creating NaN-filled placeholders\n+                vars_to_concat = []\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars_to_concat.append(ds.variables[k])\n+                    else:\n+                        # Create a variable filled with NaN for missing cases\n+                        template_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n+                        missing_var = Variable(\n+                            template_var.dims,\n+                            utils.full_like(template_var, fill_value, template_var.dtype),\n+                            template_var.attrs\n+                        )\n+                        vars_to_concat.append(missing_var)\n+                vars = ensure_common_dims(vars_to_concat)\n             except KeyError:\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n \n     result = Dataset(result_vars, attrs=result_attrs)\n-    absent_coord_names = coord_names - set(result.variables)\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n+    # Filter coord_names to only include coordinates that actually exist in result\n+    existing_coord_names = coord_names.intersection(set(result.variables))\n+    absent_coord_names = coord_names - existing_coord_names\n     if absent_coord_names:\n-        raise ValueError(\n-            \"Variables %r are coordinates in some datasets but not others.\"\n-            % absent_coord_names\n-        )\n+        # Only warn about truly problematic coordinates - those that are coordinates\n+        # in some datasets but completely missing from the concatenation result\n+        problematic_coords = set()\n+        for coord_name in absent_coord_names:\n+            # Check if this coordinate exists in any dataset but was lost during concat\n+            exists_somewhere = any(coord_name in ds.coords for ds in datasets)\n+            if exists_somewhere:\n+                problematic_coords.add(coord_name)\n+        \n+        if problematic_coords:\n+            raise ValueError(\n+                \"Variables %r are coordinates in some datasets but not others.\"\n+                % problematic_coords\n+            )\n+    result = result.set_coords(existing_coord_names)\n     result = result.set_coords(coord_names)\n     result.encoding = result_encoding\n \n",
    "variant_2": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..719750e7 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,13 +187,22 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # all nonindexes that are not the same in each dataset\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        # Compare the variable across all datasets that have it\n+                        # Perform the minimum amount of loads in order to avoid \n+                        # multiple loads from disk while keeping the RAM footprint low.\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) <= 1:\n+                            continue\n+                            \n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n@@ -219,17 +228,24 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+        else:\n+            # Check if variables exist in any dataset, not just the first one\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +328,36 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    # Get the shape from the first available dataset that has this variable\n+                    template_var = None\n+                    for other_ds in datasets:\n+                        if var in other_ds.variables:\n+                            template_var = other_ds.variables[var]\n+                            break\n+                    \n+                    if template_var is not None:\n+                        # Create NaN-filled variable with same dims and dtype as template\n+                        shape = tuple(ds.dims.get(d, template_var.sizes[d]) for d in template_var.dims)\n+                        data = utils.full_like(template_var.data, fill_value=dtypes.NA, dtype=template_var.dtype)\n+                        if hasattr(data, 'shape') and data.shape != shape:\n+                            import numpy as np\n+                            data = np.full(shape, fill_value=dtypes.NA, dtype=template_var.dtype)\n+                        missing_var = Variable(template_var.dims, data, template_var.attrs)\n+                        to_merge[var].append(missing_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -359,16 +399,47 @@ def _dataset_concat(\n     for k in datasets[0].variables:\n         if k in concat_over:\n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+\n+            if var.dims != common_dims:\n+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+                var = var.set_dims(common_dims, common_shape)\n+            yield var\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    for k in datasets[0].variables:\n+        if k in concat_over:\n+            try:\n+                # Handle missing variables by creating NaN-filled placeholders\n+                vars_list = []\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars_list.append(ds.variables[k])\n+                    else:\n+                        # Create a placeholder variable filled with NaN\n+                        template_var = datasets[0].variables[k]\n+                        placeholder_dims = template_var.dims\n+                        placeholder_shape = tuple(ds.dims.get(d, template_var.sizes[d]) for d in placeholder_dims)\n+                        placeholder_data = utils.full_like(template_var.data, fill_value=dtypes.NA, shape=placeholder_shape)\n+                        placeholder_var = Variable(placeholder_dims, placeholder_data, template_var.attrs)\n+                        vars_list.append(placeholder_var)\n+                vars = ensure_common_dims(vars_list)\n             except KeyError:\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n \n     result = Dataset(result_vars, attrs=result_attrs)\n+    # For outer join behavior, only validate coordinates that exist in result\n+    present_coord_names = coord_names & set(result.variables)\n     absent_coord_names = coord_names - set(result.variables)\n-    if absent_coord_names:\n+    if absent_coord_names and compat == \"identical\":\n         raise ValueError(\n             \"Variables %r are coordinates in some datasets but not others.\"\n             % absent_coord_names\n",
    "variant_3": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..36c784fe 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,13 +187,22 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # all nonindexes that are not the same in each dataset\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        # Compare the variable across all datasets that contain it\n+                        # Perform the minimum amount of loads in order to avoid \n+                        # multiple loads from disk while keeping the RAM footprint low.\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) <= 1:\n+                            continue  # Variable only exists in one dataset, skip comparison\n+                            \n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n@@ -219,17 +228,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # Get all variables from all datasets for this subset\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +327,56 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    # Get the shape from the first available variable or use empty shape\n+                    if to_merge[var]:\n+                        ref_var = to_merge[var][0]\n+                        shape = ref_var.shape\n+                        dims = ref_var.dims\n+                        dtype = ref_var.dtype\n+                    else:\n+                        # If this is the first dataset and variable is missing, \n+                        # we need to find a reference from other datasets\n+                        ref_var = None\n+                        for other_ds in datasets:\n+                            if var in other_ds.variables:\n+                                ref_var = other_ds.variables[var]\n+                                break\n+                        if ref_var is not None:\n+                            shape = ref_var.shape\n+                            dims = ref_var.dims\n+                            dtype = ref_var.dtype\n+                        else:\n+                            # Fallback: create minimal variable\n+                            shape = ()\n+                            dims = ()\n+                            dtype = float\n+                    \n+                    # Create NaN-filled data\n+                    import numpy as np\n+                    if dtype.kind in 'SU':  # string types\n+                        fill_value = ''\n+                    elif dtype.kind in 'fc':  # float/complex types\n+                        fill_value = np.nan\n+                    else:\n+                        fill_value = 0\n+                    \n+                    data = np.full(shape, fill_value, dtype=dtype)\n+                    placeholder_var = Variable(dims, data)\n+                    to_merge[var].append(placeholder_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -359,20 +418,68 @@ def _dataset_concat(\n     for k in datasets[0].variables:\n         if k in concat_over:\n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+\n+            if var.dims != common_dims:\n+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+                var = var.set_dims(common_dims, common_shape)\n+            yield var\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    for k in datasets[0].variables:\n+        if k in concat_over:\n+            try:\n+                vars = ensure_common_dims([ds.variables[k] if k in ds.variables else None for ds in datasets])\n+                # Filter out None values and handle missing variables\n+                valid_vars = []\n+                for i, var in enumerate(vars):\n+                    if var is not None:\n+                        valid_vars.append(var)\n+                    else:\n+                        # Create a placeholder variable filled with NaN for missing variables\n+                        ref_var = next(v for v in vars if v is not None)\n+                        fill_value = dtypes.NA if ref_var.dtype.kind in 'SU' else dtypes.get_fill_value(ref_var.dtype)\n+                        placeholder_var = Variable(ref_var.dims, \n+                                                 pd.array([fill_value] * ref_var.size).reshape(ref_var.shape),\n+                                                 attrs=ref_var.attrs)\n+                        valid_vars.append(placeholder_var)\n+                vars = ensure_common_dims(valid_vars)\n             except KeyError:\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n \n     result = Dataset(result_vars, attrs=result_attrs)\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n+    \n+    # Handle missing coordinates gracefully - only require coordinates that exist in result\n+    present_coord_names = coord_names & set(result.variables)\n     absent_coord_names = coord_names - set(result.variables)\n+    \n     if absent_coord_names:\n-        raise ValueError(\n-            \"Variables %r are coordinates in some datasets but not others.\"\n-            % absent_coord_names\n-        )\n+        # Only warn about truly problematic cases where coordinates exist in some datasets\n+        # but are completely missing from the concatenated result\n+        problematic_coords = []\n+        for coord_name in absent_coord_names:\n+            # Check if this coordinate exists in any dataset but wasn't included in result\n+            exists_somewhere = any(coord_name in ds.coords for ds in datasets)\n+            if exists_somewhere:\n+                problematic_coords.append(coord_name)\n+        \n+        if problematic_coords:\n+            raise ValueError(\n+                \"Variables %r are coordinates in some datasets but not others.\"\n+                % problematic_coords\n+            )\n+    \n+    # Only set coordinates that actually exist in the result\n+    if present_coord_names:\n+        result = result.set_coords(present_coord_names)\n     result = result.set_coords(coord_names)\n     result.encoding = result_encoding\n \n",
    "variant_4": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..46575317 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,13 +187,22 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # all nonindexes that are not the same in each dataset\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n+                        # Compare the variable across all datasets that contain it\n+                        # Perform the minimum amount of loads to avoid multiple disk loads\n                         # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) <= 1:\n+                            continue  # Variable exists in only one dataset, no comparison needed\n+                        \n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n@@ -219,17 +228,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # For outer join behavior, collect all variables from all datasets\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +327,39 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    # Get the dimensions from a dataset that has this variable\n+                    ref_var = None\n+                    for ref_ds in datasets:\n+                        if var in ref_ds.variables:\n+                            ref_var = ref_ds.variables[var]\n+                            break\n+                    \n+                    if ref_var is not None:\n+                        # Create NaN-filled variable with same dims and dtype as reference\n+                        shape = tuple(ds.dims.get(dim, 1) for dim in ref_var.dims)\n+                        import numpy as np\n+                        if ref_var.dtype.kind in 'fc':  # float or complex\n+                            fill_val = np.nan\n+                        else:\n+                            fill_val = None\n+                        \n+                        data = np.full(shape, fill_val, dtype=ref_var.dtype)\n+                        placeholder_var = Variable(ref_var.dims, data, ref_var.attrs.copy())\n+                        to_merge[var].append(placeholder_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -359,20 +401,53 @@ def _dataset_concat(\n     for k in datasets[0].variables:\n         if k in concat_over:\n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+\n+            if var.dims != common_dims:\n+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+                var = var.set_dims(common_dims, common_shape)\n+            yield var\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    for k in datasets[0].variables:\n+        if k in concat_over:\n+            try:\n+                # Handle missing variables by creating NaN-filled placeholders\n+                vars_list = []\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars_list.append(ds.variables[k])\n+                    else:\n+                        # Create a placeholder variable filled with NaN\n+                        var_dims = ds.variables[list(ds.variables.keys())[0]].dims if ds.variables else ()\n+                        var_shape = tuple(ds.dims.get(d, 1) for d in var_dims)\n+                        placeholder = Variable(var_dims, utils.NumpyVIndexAdapter(pd.array([pd.NA] * pd.array(var_shape).prod()).reshape(var_shape)))\n+                        vars_list.append(placeholder)\n+                vars = ensure_common_dims(vars_list)\n             except KeyError:\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n \n     result = Dataset(result_vars, attrs=result_attrs)\n+    # Handle missing coordinates gracefully - only require coordinates that exist in result\n+    present_coord_names = coord_names & set(result.variables)\n     absent_coord_names = coord_names - set(result.variables)\n     if absent_coord_names:\n-        raise ValueError(\n-            \"Variables %r are coordinates in some datasets but not others.\"\n-            % absent_coord_names\n+        # Only warn or create placeholder coordinates instead of raising error\n+        import warnings\n+        warnings.warn(\n+            f\"Coordinates {absent_coord_names} are present in some datasets but not others. \"\n+            \"They will be excluded from the result.\",\n+            UserWarning\n         )\n+    result = result.set_coords(present_coord_names)\n     result = result.set_coords(coord_names)\n     result.encoding = result_encoding\n \n",
    "variant_5": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..fbbd8338 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,13 +187,25 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # all nonindexes that are not the same in each dataset\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        # Compare the variable across all datasets that have it\n+                        # Perform the minimum amount of loads in order to avoid \n+                        # multiple loads from disk while keeping the RAM footprint low.\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) < len(datasets):\n+                            # Variable is missing from some datasets, add to concat_over\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            continue\n+                            \n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n@@ -219,17 +231,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # Get all variables from all datasets for this subset\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +330,39 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    # Get the shape from the first dataset that has this variable\n+                    ref_var = None\n+                    for ref_ds in datasets:\n+                        if var in ref_ds.variables:\n+                            ref_var = ref_ds.variables[var]\n+                            break\n+                    \n+                    if ref_var is not None:\n+                        # Create variable with same dims and dtype but filled with NaN\n+                        shape = tuple(ds.dims.get(dim, ref_var.sizes[dim]) for dim in ref_var.dims)\n+                        import numpy as np\n+                        if ref_var.dtype.kind in 'fc':  # float or complex\n+                            fill_value = np.nan\n+                        else:\n+                            fill_value = None\n+                        \n+                        data = np.full(shape, fill_value, dtype=ref_var.dtype)\n+                        placeholder_var = Variable(ref_var.dims, data, ref_var.attrs.copy())\n+                        to_merge[var].append(placeholder_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -358,19 +403,43 @@ def _dataset_concat(\n     # n.b. this loop preserves variable order, needed for groupby.\n     for k in datasets[0].variables:\n         if k in concat_over:\n+            try:\n+\n             try:\n                 vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n             except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+                # Handle missing variables by creating NaN-filled placeholders\n+                vars = []\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars.append(ds.variables[k])\n+                    else:\n+                        # Create a placeholder variable filled with NaN\n+                        # Use the same dims as the first available variable of this name\n+                        ref_var = next((d.variables[k] for d in datasets if k in d.variables), None)\n+                        if ref_var is not None:\n+                            # Create NaN-filled variable with same shape as reference\n+                            shape = tuple(ds.dims.get(d, ref_var.sizes[d]) for d in ref_var.dims)\n+                            data = utils.full_like(ref_var.data, fill_value=dtypes.NA, shape=shape)\n+                            placeholder = Variable(ref_var.dims, data, ref_var.attrs)\n+                            vars.append(placeholder)\n+                vars = ensure_common_dims(vars)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n \n     result = Dataset(result_vars, attrs=result_attrs)\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n+    # Handle missing coordinates gracefully - only validate coordinates that should exist\n+    present_coord_names = coord_names & set(result.variables)\n     absent_coord_names = coord_names - set(result.variables)\n-    if absent_coord_names:\n+    \n+    # Only raise error if fill_value is None and we have missing coordinates\n+    if absent_coord_names and fill_value is None:\n         raise ValueError(\n-            \"Variables %r are coordinates in some datasets but not others.\"\n+            \"Variables %r are coordinates in some datasets but not others. \"\n+            \"Consider using fill_value parameter to handle missing coordinates.\"\n             % absent_coord_names\n         )\n     result = result.set_coords(coord_names)\n",
    "variant_6": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..490a3014 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,13 +187,22 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # all nonindexes that are not the same in each dataset\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        # Compare the variable across all datasets that contain it\n+                        # Perform the minimum amount of loads in order to avoid \n+                        # multiple loads from disk while keeping the RAM footprint low.\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) <= 1:\n+                            continue\n+                            \n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n@@ -219,17 +228,24 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+        else:\n+            # Get all variables from all datasets for this subset\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +328,35 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    # Use the same dims as the dataset's dimensions that are relevant\n+                    dims = tuple(d for d in ds.dims if d in ds.variables.get(var, Variable((), None)).dims) if var in ds.variables else ()\n+                    if not dims and var in coord_names:\n+                        # For coordinates, try to infer dims from other datasets\n+                        for other_ds in datasets:\n+                            if var in other_ds.variables:\n+                                dims = other_ds.variables[var].dims\n+                                break\n+                    if not dims:\n+                        dims = ()\n+                    \n+                    shape = tuple(ds.sizes.get(d, 0) for d in dims)\n+                    data = utils.NAN_LIKE_ARRAY if shape == () else pd.array([utils.NAN_LIKE_ARRAY] * (shape[0] if shape else 1)).reshape(shape)\n+                    placeholder_var = Variable(dims, data)\n+                    to_merge[var].append(placeholder_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -359,20 +398,55 @@ def _dataset_concat(\n     for k in datasets[0].variables:\n         if k in concat_over:\n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+\n+            if var.dims != common_dims:\n+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+                var = var.set_dims(common_dims, common_shape)\n+            yield var\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    for k in datasets[0].variables:\n+        if k in concat_over:\n+            try:\n+                vars = ensure_common_dims([ds.variables[k] if k in ds.variables else \n+                                         Variable(ds.variables[list(ds.variables.keys())[0]].dims, \n+                                                pd.array([fill_value] * ds.variables[list(ds.variables.keys())[0]].size).reshape(ds.variables[list(ds.variables.keys())[0]].shape))\n+                                         for ds in datasets])\n             except KeyError:\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n \n     result = Dataset(result_vars, attrs=result_attrs)\n     absent_coord_names = coord_names - set(result.variables)\n     if absent_coord_names:\n-        raise ValueError(\n-            \"Variables %r are coordinates in some datasets but not others.\"\n-            % absent_coord_names\n-        )\n+        # Handle missing coordinates by creating them with NaN values\n+        for coord_name in absent_coord_names:\n+            # Find a dataset that has this coordinate to get its properties\n+            coord_var = None\n+            for ds in datasets:\n+                if coord_name in ds.coords:\n+                    coord_var = ds.coords[coord_name]\n+                    break\n+            \n+            if coord_var is not None:\n+                # Create coordinate with NaN values for missing entries\n+                if dim in coord_var.dims:\n+                    # This coordinate varies along the concat dimension\n+                    result_vars[coord_name] = coord_var\n+                else:\n+                    # This coordinate is constant, use first available value\n+                    result_vars[coord_name] = coord_var\n+        \n+        # Recreate result with added coordinates\n+        result = Dataset(result_vars, attrs=result_attrs)\n     result = result.set_coords(coord_names)\n     result.encoding = result_encoding\n \n",
    "variant_7": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..c374f9f0 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,13 +187,25 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # all nonindexes that are not the same in each dataset\n+                all_subset_vars = set()\n+                for ds in datasets:\n+                    all_subset_vars.update(getattr(ds, subset))\n+                \n+                for k in all_subset_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        # Compare the variable across all datasets that contain it\n+                        # Perform the minimum amount of loads in order to avoid \n+                        # multiple loads from disk while keeping the RAM footprint low.\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) < len(datasets):\n+                            # Variable is missing in some datasets, add to concat_over\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            continue\n+                            \n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n@@ -219,17 +231,20 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # Check if variables exist in ANY dataset, not just the first one\n+            all_vars = set().union(*[getattr(ds, subset) for ds in datasets])\n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +327,33 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    # Use the same dims as the first dataset that has this variable\n+                    ref_var = None\n+                    for ref_ds in datasets:\n+                        if var in ref_ds.variables:\n+                            ref_var = ref_ds.variables[var]\n+                            break\n+                    \n+                    if ref_var is not None:\n+                        # Create NaN-filled variable with same shape and dims as reference\n+                        import numpy as np\n+                        nan_data = np.full(ref_var.shape, np.nan, dtype=ref_var.dtype)\n+                        nan_var = Variable(ref_var.dims, nan_data, ref_var.attrs)\n+                        to_merge[var].append(nan_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -359,7 +395,13 @@ def _dataset_concat(\n     for k in datasets[0].variables:\n         if k in concat_over:\n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+\n+            try:\n+                vars = ensure_common_dims([ds.variables.get(k, None) for ds in datasets])\n+                # Filter out None values for missing variables\n+                vars = [var for var in vars if var is not None]\n+                if not vars:\n+                    continue  # Skip if variable is missing in all datasets\n             except KeyError:\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n@@ -367,12 +409,20 @@ def _dataset_concat(\n             result_vars[k] = combined\n \n     result = Dataset(result_vars, attrs=result_attrs)\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n+    # Handle missing coordinates gracefully - allow outer join behavior\n+    present_coord_names = coord_names & set(result.variables)\n     absent_coord_names = coord_names - set(result.variables)\n-    if absent_coord_names:\n+    if absent_coord_names and compat == 'no_conflicts':\n+        # Only raise error in strict compatibility mode\n         raise ValueError(\n             \"Variables %r are coordinates in some datasets but not others.\"\n             % absent_coord_names\n         )\n+    # Only set coordinates that are actually present in the result\n+    if present_coord_names:\n+        result = result.set_coords(present_coord_names)\n     result = result.set_coords(coord_names)\n     result.encoding = result_encoding\n \n",
    "variant_8": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..70af766e 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,13 +187,25 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # all nonindexes that are not the same in each dataset\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        # Compare the variable across all datasets that contain it\n+                        # Perform the minimum amount of loads in order to avoid \n+                        # multiple loads from disk while keeping the RAM footprint low.\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) < len(datasets):\n+                            # Variable is missing from some datasets, include it for concat\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            continue\n+                            \n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n@@ -219,17 +231,20 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # For outer join behavior, validate against all variables across all datasets\n+            all_vars = set().union(*[getattr(ds, subset) for ds in datasets])\n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"some variables in coords are not coordinates in \"\n+                        \"any of the datasets: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"in any of the datasets: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +327,49 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    # Use the dimensions from the first dataset that has this variable\n+                    ref_var = None\n+                    for ref_ds in datasets:\n+                        if var in ref_ds.variables:\n+                            ref_var = ref_ds.variables[var]\n+                            break\n+                    \n+                    if ref_var is not None:\n+                        # Create dims that exist in current dataset, filling missing dims with size 1\n+                        dims = []\n+                        shape = []\n+                        for dim in ref_var.dims:\n+                            if dim in ds.dims:\n+                                dims.append(dim)\n+                                shape.append(ds.dims[dim])\n+                            else:\n+                                dims.append(dim)\n+                                shape.append(1)\n+                        \n+                        # Create NaN-filled variable with appropriate shape and dtype\n+                        import numpy as np\n+                        if ref_var.dtype.kind in 'SU':  # string dtypes\n+                            fill_value = ''\n+                        else:\n+                            fill_value = np.nan\n+                        \n+                        data = np.full(shape, fill_value, dtype=ref_var.dtype)\n+                        placeholder_var = Variable(dims, data, attrs=ref_var.attrs.copy())\n+                        to_merge[var].append(placeholder_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -359,20 +411,80 @@ def _dataset_concat(\n     for k in datasets[0].variables:\n         if k in concat_over:\n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+\n+            if var.dims != common_dims:\n+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+                var = var.set_dims(common_dims, common_shape)\n+            yield var\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    for k in datasets[0].variables:\n+        if k in concat_over:\n+            try:\n+                # Handle missing variables by creating NaN-filled placeholders\n+                vars_to_concat = []\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars_to_concat.append(ds.variables[k])\n+                    else:\n+                        # Create a placeholder variable filled with NaN\n+                        template_var = datasets[0].variables[k]\n+                        shape = tuple(ds.dims.get(d, 1) if d != dim else 1 \n+                                    for d in template_var.dims)\n+                        placeholder = Variable(\n+                            template_var.dims,\n+                            utils.full_like(template_var.data, fill_value, shape=shape),\n+                            template_var.attrs\n+                        )\n+                        vars_to_concat.append(placeholder)\n+                vars = ensure_common_dims(vars_to_concat)\n             except KeyError:\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n \n     result = Dataset(result_vars, attrs=result_attrs)\n+    # Only validate coordinates that actually exist in the result\n+    existing_coord_names = coord_names & set(result.variables)\n     absent_coord_names = coord_names - set(result.variables)\n+    \n+    # For absent coordinates, create them as index coordinates if they exist in any dataset\n+    for coord_name in absent_coord_names:\n+        for ds in datasets:\n+            if coord_name in ds.coords:\n+                # Create a coordinate filled with NaN for datasets that don't have it\n+                coord_vars = []\n+                for dataset in datasets:\n+                    if coord_name in dataset.coords:\n+                        coord_vars.append(dataset.coords[coord_name].variable)\n+                    else:\n+                        # Create NaN-filled coordinate with appropriate shape\n+                        coord_var = dataset.coords[coord_name].variable if coord_name in dataset.coords else None\n+                        if coord_var is None:\n+                            # Find a reference coordinate from another dataset\n+                            ref_coord = next((d.coords[coord_name] for d in datasets if coord_name in d.coords), None)\n+                            if ref_coord is not None:\n+                                import numpy as np\n+                                shape = tuple(dataset.dims.get(d, ref_coord.sizes.get(d, 1)) for d in ref_coord.dims)\n+                                data = np.full(shape, np.nan, dtype=ref_coord.dtype)\n+                                coord_vars.append(Variable(ref_coord.dims, data))\n+                if coord_vars:\n+                    combined_coord = concat_vars(coord_vars, dim, positions)\n+                    result_vars[coord_name] = combined_coord\n+                break\n+    \n+    # Update result with any new coordinate variables\n     if absent_coord_names:\n-        raise ValueError(\n-            \"Variables %r are coordinates in some datasets but not others.\"\n-            % absent_coord_names\n-        )\n+        result = Dataset(result_vars, attrs=result_attrs)\n+    \n+    result = result.set_coords(existing_coord_names | (absent_coord_names & set(result.variables)))\n     result = result.set_coords(coord_names)\n     result.encoding = result_encoding\n \n"
  }
}