{
  "zero_patches": {
    "Location 1 (/root/hy/projects_1/scikit-learn/sklearn/ensemble/iforest.py lines 167-176)": "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n",
    "Location 2 (/root/hy/projects_1/scikit-learn/sklearn/ensemble/iforest.py lines 177-190)": "\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            warm_start=warm_start,\n            verbose=verbose)\n",
    "Location 3 (/root/hy/projects_1/scikit-learn/sklearn/ensemble/iforest.py lines 115-125)": "\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_1/scikit-learn/sklearn/ensemble/iforest.py lines 167-176)": [
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n",
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n",
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n",
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 warm_start=False,\n                 verbose=0):\n",
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n",
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n",
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n",
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n"
    ],
    "Location 2 (/root/hy/projects_1/scikit-learn/sklearn/ensemble/iforest.py lines 177-190)": [
      "\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start)\n",
      "\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start)\n",
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start)\n",
      "\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start)\n",
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start)\n",
      "\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            warm_start=warm_start,\n            verbose=verbose)\n",
      "\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start)\n",
      "\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start)\n"
    ],
    "Location 3 (/root/hy/projects_1/scikit-learn/sklearn/ensemble/iforest.py lines 115-125)": [
      "\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n",
      "\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n",
      "\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n",
      "\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n",
      "\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n",
      "\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n",
      "\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n",
      "\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..66f2b5186 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -112,11 +112,17 @@ class IsolationForest(BaseBagging, OutlierMixin):\n            0.24.\n \n     random_state : int, RandomState instance or None, optional (default=None)\n+\n         If int, random_state is the seed used by the random number generator;\n         If RandomState instance, random_state is the random number generator;\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n@@ -164,6 +170,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n \n     \"\"\"\n \n+\n     def __init__(self,\n                  n_estimators=100,\n                  max_samples=\"auto\",\n@@ -173,7 +180,9 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n+\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,7 +196,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\n",
    "variant_0": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..f6250555b 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -111,18 +111,24 @@ class IsolationForest(BaseBagging, OutlierMixin):\n            ``behaviour`` parameter will be deprecated in 0.22 and removed in\n            0.24.\n \n+    random_state : int, RandomState instance or None, optional (default=None)\n+\n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n         If RandomState instance, random_state is the random number generator;\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n \n     Attributes\n-    ----------\n     estimators_ : list of DecisionTreeClassifier\n         The collection of fitted sub-estimators.\n \n@@ -164,6 +170,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n \n     \"\"\"\n \n+\n     def __init__(self,\n                  n_estimators=100,\n                  max_samples=\"auto\",\n@@ -173,7 +180,9 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n+\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,6 +196,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n+            warm_start=warm_start,\n             verbose=verbose)\n \n         self.behaviour = behaviour\n",
    "variant_1": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..a17d8a69b 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -111,18 +111,24 @@ class IsolationForest(BaseBagging, OutlierMixin):\n            ``behaviour`` parameter will be deprecated in 0.22 and removed in\n            0.24.\n \n+    random_state : int, RandomState instance or None, optional (default=None)\n+\n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n         If RandomState instance, random_state is the random number generator;\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n \n     Attributes\n-    ----------\n     estimators_ : list of DecisionTreeClassifier\n         The collection of fitted sub-estimators.\n \n@@ -164,6 +170,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n \n     \"\"\"\n \n+\n     def __init__(self,\n                  n_estimators=100,\n                  max_samples=\"auto\",\n@@ -173,7 +180,9 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n+\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,7 +196,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\n",
    "variant_2": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..a17d8a69b 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -111,18 +111,24 @@ class IsolationForest(BaseBagging, OutlierMixin):\n            ``behaviour`` parameter will be deprecated in 0.22 and removed in\n            0.24.\n \n+    random_state : int, RandomState instance or None, optional (default=None)\n+\n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n         If RandomState instance, random_state is the random number generator;\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n \n     Attributes\n-    ----------\n     estimators_ : list of DecisionTreeClassifier\n         The collection of fitted sub-estimators.\n \n@@ -164,6 +170,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n \n     \"\"\"\n \n+\n     def __init__(self,\n                  n_estimators=100,\n                  max_samples=\"auto\",\n@@ -173,7 +180,9 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n+\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,7 +196,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\n",
    "variant_3": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..db7cbdc0f 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -111,18 +111,24 @@ class IsolationForest(BaseBagging, OutlierMixin):\n            ``behaviour`` parameter will be deprecated in 0.22 and removed in\n            0.24.\n \n+    random_state : int, RandomState instance or None, optional (default=None)\n+\n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n         If RandomState instance, random_state is the random number generator;\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n \n     Attributes\n-    ----------\n     estimators_ : list of DecisionTreeClassifier\n         The collection of fitted sub-estimators.\n \n@@ -164,6 +170,19 @@ class IsolationForest(BaseBagging, OutlierMixin):\n \n     \"\"\"\n \n+\n+    def __init__(self,\n+                 n_estimators=100,\n+                 max_samples=\"auto\",\n+                 contamination=\"legacy\",\n+                 max_features=1.,\n+                 bootstrap=False,\n+                 n_jobs=None,\n+                 behaviour='old',\n+                 random_state=None,\n+                 verbose=0,\n+                 warm_start=False):\n+\n     def __init__(self,\n                  n_estimators=100,\n                  max_samples=\"auto\",\n@@ -173,7 +192,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,7 +207,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\n",
    "variant_4": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..ba6c17531 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -111,18 +111,24 @@ class IsolationForest(BaseBagging, OutlierMixin):\n            ``behaviour`` parameter will be deprecated in 0.22 and removed in\n            0.24.\n \n+    random_state : int, RandomState instance or None, optional (default=None)\n+\n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n         If RandomState instance, random_state is the random number generator;\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n \n     Attributes\n-    ----------\n     estimators_ : list of DecisionTreeClassifier\n         The collection of fitted sub-estimators.\n \n@@ -164,6 +170,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n \n     \"\"\"\n \n+\n     def __init__(self,\n                  n_estimators=100,\n                  max_samples=\"auto\",\n@@ -173,7 +180,9 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n+                 warm_start=False,\n                  verbose=0):\n+\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,7 +196,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\n",
    "variant_5": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..db7cbdc0f 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -111,18 +111,24 @@ class IsolationForest(BaseBagging, OutlierMixin):\n            ``behaviour`` parameter will be deprecated in 0.22 and removed in\n            0.24.\n \n+    random_state : int, RandomState instance or None, optional (default=None)\n+\n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n         If RandomState instance, random_state is the random number generator;\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n \n     Attributes\n-    ----------\n     estimators_ : list of DecisionTreeClassifier\n         The collection of fitted sub-estimators.\n \n@@ -164,6 +170,19 @@ class IsolationForest(BaseBagging, OutlierMixin):\n \n     \"\"\"\n \n+\n+    def __init__(self,\n+                 n_estimators=100,\n+                 max_samples=\"auto\",\n+                 contamination=\"legacy\",\n+                 max_features=1.,\n+                 bootstrap=False,\n+                 n_jobs=None,\n+                 behaviour='old',\n+                 random_state=None,\n+                 verbose=0,\n+                 warm_start=False):\n+\n     def __init__(self,\n                  n_estimators=100,\n                  max_samples=\"auto\",\n@@ -173,7 +192,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,7 +207,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\n",
    "variant_6": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..f6250555b 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -111,18 +111,24 @@ class IsolationForest(BaseBagging, OutlierMixin):\n            ``behaviour`` parameter will be deprecated in 0.22 and removed in\n            0.24.\n \n+    random_state : int, RandomState instance or None, optional (default=None)\n+\n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n         If RandomState instance, random_state is the random number generator;\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n \n     Attributes\n-    ----------\n     estimators_ : list of DecisionTreeClassifier\n         The collection of fitted sub-estimators.\n \n@@ -164,6 +170,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n \n     \"\"\"\n \n+\n     def __init__(self,\n                  n_estimators=100,\n                  max_samples=\"auto\",\n@@ -173,7 +180,9 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n+\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,6 +196,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n+            warm_start=warm_start,\n             verbose=verbose)\n \n         self.behaviour = behaviour\n",
    "variant_7": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..db7cbdc0f 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -111,18 +111,24 @@ class IsolationForest(BaseBagging, OutlierMixin):\n            ``behaviour`` parameter will be deprecated in 0.22 and removed in\n            0.24.\n \n+    random_state : int, RandomState instance or None, optional (default=None)\n+\n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n         If RandomState instance, random_state is the random number generator;\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n \n     Attributes\n-    ----------\n     estimators_ : list of DecisionTreeClassifier\n         The collection of fitted sub-estimators.\n \n@@ -164,6 +170,19 @@ class IsolationForest(BaseBagging, OutlierMixin):\n \n     \"\"\"\n \n+\n+    def __init__(self,\n+                 n_estimators=100,\n+                 max_samples=\"auto\",\n+                 contamination=\"legacy\",\n+                 max_features=1.,\n+                 bootstrap=False,\n+                 n_jobs=None,\n+                 behaviour='old',\n+                 random_state=None,\n+                 verbose=0,\n+                 warm_start=False):\n+\n     def __init__(self,\n                  n_estimators=100,\n                  max_samples=\"auto\",\n@@ -173,7 +192,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,7 +207,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\n",
    "variant_8": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..a17d8a69b 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -111,18 +111,24 @@ class IsolationForest(BaseBagging, OutlierMixin):\n            ``behaviour`` parameter will be deprecated in 0.22 and removed in\n            0.24.\n \n+    random_state : int, RandomState instance or None, optional (default=None)\n+\n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n         If RandomState instance, random_state is the random number generator;\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n \n     Attributes\n-    ----------\n     estimators_ : list of DecisionTreeClassifier\n         The collection of fitted sub-estimators.\n \n@@ -164,6 +170,7 @@ class IsolationForest(BaseBagging, OutlierMixin):\n \n     \"\"\"\n \n+\n     def __init__(self,\n                  n_estimators=100,\n                  max_samples=\"auto\",\n@@ -173,7 +180,9 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n+\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,7 +196,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\n"
  }
}