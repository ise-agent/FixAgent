{
  "zero_patches": {
    "Location 1 (/root/hy/projects_1/scikit-learn/sklearn/utils/multiclass.py lines 343-345)": "\n    if not issparse(y) and y.dtype == object:\n        # Check if this is a nullable pandas dtype converted to object array\n        if hasattr(y, '_pandas_nullable_dtype_info'):\n            # Use the preserved pandas dtype information\n            original_dtype = y._pandas_nullable_dtype_info\n            if hasattr(original_dtype, 'numpy_dtype'):\n                # This is a nullable pandas dtype, skip the object check\n                pass\n            else:\n                # Regular object array with non-string objects\n                if not isinstance(y.flat[0], str):\n                    return \"unknown\"\n        else:\n            # Regular object array check\n            if not isinstance(y.flat[0], str):\n                return \"unknown\"\n",
    "Location 2 (/root/hy/projects_1/scikit-learn/sklearn/utils/multiclass.py lines 282-313)": "\n    xp, is_array_api = get_namespace(y)\n    \n    # Check for nullable pandas dtypes before conversion\n    nullable_pandas_dtype = None\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n        # Handle nullable pandas integer/boolean dtypes\n        if y.dtype.name in ['Int8', 'Int16', 'Int32', 'Int64', 'boolean']:\n            nullable_pandas_dtype = y.dtype.name\n        # Handle nullable pandas string dtype\n        elif str(y.dtype) == 'string':\n            nullable_pandas_dtype = 'string'\n    \n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n                # Store nullable pandas dtype info as array attribute if conversion resulted in object dtype\n                if nullable_pandas_dtype is not None and y.dtype == object:\n                    y = np.asarray(y)  # Ensure it's a numpy array so we can add attributes\n                    y._nullable_pandas_dtype = nullable_pandas_dtype\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n                # Store nullable pandas dtype info even for explicitly object arrays\n                if nullable_pandas_dtype is not None:\n                    y = np.asarray(y)\n                    y._nullable_pandas_dtype = nullable_pandas_dtype\n",
    "Location 3 (/root/hy/projects_1/scikit-learn/sklearn/utils/multiclass.py lines 94-97)": "\ndef check_classification_targets(y):\n    \"\"\"Ensure that target y is of a non-regression type.\n\n    Only the following target types (as defined in type_of_target) are allowed:\n        'binary', 'multiclass', 'multiclass-multioutput',\n        'multilabel-indicator', 'multilabel-sequences'\n\n    Parameters\n    ----------\n    y : array-like\n        Target values.\n    \"\"\"\n    y_type = type_of_target(y, input_name=\"y\")\n    if y_type not in [\n        \"binary\",\n        \"multiclass\",\n        \"multiclass-multioutput\",\n        \"multilabel-indicator\",\n        \"multilabel-sequences\",\n    ]:\n        raise ValueError(\n            \"Unknown label type: %r. Maybe you are trying to fit a \"\n            \"classifier, which expects discrete classes on a \"\n            \"regression target with continuous values.\" % y_type\n        )\n\n\ndef type_of_target(y, input_name=\"\"):\n    \"\"\"Determine the type of data indicated by the target.\n\n    Note that this type is the most specific type that can be inferred.\n    For example:\n\n        * ``binary`` is more specific but compatible with ``multiclass``.\n        * ``multiclass`` is more specific but compatible with\n          ``multiclass-multioutput``.\n        * ``multilabel-indicator`` is more specific but compatible with\n          ``multiclass-multioutput``.\n\n    Parameters\n    ----------\n    y : {array-like, sparse matrix}\n        Target values. If a sparse matrix, it should be in CSR format.\n\n    input_name : str, default=\"\"\n        The data name used to construct the error message.\n\n        .. versionadded:: 1.1.0\n\n    Returns\n    -------\n    target_type : str\n        One of:\n\n        * 'continuous': `y` is an array-like of floats that are not all\n          integers, and is 1d or a column vector.\n        * 'continuous-multioutput': `y` is a 2d array of floats that are\n          not all integers, and both dimensions are of size > 1.\n        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n          vector.\n        * 'multiclass': `y` contains more than two discrete values, is not a\n          sequence of sequences, and is 1d or a column vector.\n        * 'multiclass-multioutput': `y` is a 2d array that contains more\n          than two discrete values, is not a sequence of sequences, and both\n          dimensions are of size > 1.\n        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n          of two dimensions with at least two columns, and at most 2 unique\n          values.\n        * 'unknown': `y` is array-like but none of the above apply.\n\n    Examples\n    --------\n    >>> from sklearn.utils.multiclass import type_of_target\n    >>> import numpy as np\n    >>> type_of_target([0.1, 0.6])\n    'continuous'\n    >>> type_of_target([1, -1, -1, 1])\n    'binary'\n    >>> type_of_target(['a', 'b', 'a'])\n    'binary'\n    >>> type_of_target([1.0, 2.0])\n    'binary'\n    >>> type_of_target([1, 0, 2])\n    'multiclass'\n    >>> type_of_target([1.0, 0.0, 3.0])\n    'multiclass'\n    >>> type_of_target(['a', 'b', 'c'])\n    'multiclass'\n    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n    'multiclass-multioutput'\n    >>> type_of_target([[1, 2]])\n    'multilabel-indicator'\n    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n    'continuous-multioutput'\n    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n    'multilabel-indicator'\n    \"\"\"\n    xp, is_array_api_compliant = get_namespace(y)\n    if is_array_api_compliant:\n        # array_api does not allow for 0-d arrays.\n        y = xp.atleast_1d(xp.asarray(y))\n    else:\n        y = check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], ensure_2d=False)\n    \n    # Store original y for reference if it's a pandas Series with nullable dtype\n    original_y = y\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name') and 'pandas' in str(type(y.dtype)):\n        # This is a nullable pandas dtype, extract the base dtype\n        if hasattr(y.dtype, 'numpy_dtype'):\n            base_dtype = y.dtype.numpy_dtype\n        elif hasattr(y.dtype, 'type'):\n            base_dtype = y.dtype.type\n        else:\n            # For string dtypes and others, check the non-null values\n            non_null_mask = ~y.isna() if hasattr(y, 'isna') else ~np.isnan(y.astype(float, errors='ignore'))\n            if non_null_mask.any():\n                non_null_values = y[non_null_mask]\n                if len(non_null_values) > 0:\n                    first_val = non_null_values.iloc[0] if hasattr(non_null_values, 'iloc') else non_null_values[0]\n                    base_dtype = type(first_val)\n                else:\n                    base_dtype = object\n            else:\n                base_dtype = object\n    else:\n        base_dtype = getattr(y, 'dtype', object)\n\n    # The old sequence of sequences format\n    try:\n        if (\n            not hasattr(y[0], \"__array__\")\n            and isinstance(y[0], Sequence)\n            and not isinstance(y[0], str)\n        ):\n            raise ValueError(\n                \"You appear to be using a legacy multi-label data\"\n                \" representation. Sequence of sequences are no\"\n                \" longer supported; use a binary array or sparse\"\n                \" matrix instead - the MultiLabelBinarizer\"\n                \" transformer can convert to this format.\"\n            )\n    except (IndexError, TypeError):\n        pass\n\n    # Invalid inputs\n    if y.ndim not in (1, 2):\n        # Number of dimension greater than 2: [[[1, 2]]]\n        return \"unknown\"\n\n    # The case of an empty array\n    if y.shape[0] == 0:\n        return \"unknown\"\n\n    # Check if multioutput\n    if y.ndim == 2 and y.shape[1] > 1:\n        suffix = \"-multioutput\"  # [[1, 2], [1, 2]]\n        ndim = 2\n        y = y.reshape(-1)\n    else:\n        suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n        ndim = 1\n        y = y.reshape(-1)\n\n    # check float and contains non-integer float values\n    if base_dtype.kind == \"f\":\n        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n        _assert_all_finite(y)\n        return \"continuous\" + suffix\n\n    # Check for object dtype that might contain nullable pandas data\n    if y.dtype.kind == \"O\":\n        # Handle object arrays that might contain nullable pandas data\n        if hasattr(original_y, 'dtype') and hasattr(original_y.dtype, 'name'):\n            # Check if this is a nullable pandas dtype stored in object array\n            if 'string' in str(original_y.dtype).lower() or 'String' in str(original_y.dtype):\n                # This is pandas string data, treat as categorical\n                pass\n            elif hasattr(original_y.dtype, 'numpy_dtype'):\n                # Use the underlying numpy dtype for classification\n                if original_y.dtype.numpy_dtype.kind == \"f\":\n                    _assert_all_finite(y[~pd.isna(original_y)] if hasattr(original_y, 'isna') else y)\n                    return \"continuous\" + suffix\n        \n        # For regular object arrays, check the actual values\n        if len(y) > 0:\n            # Get non-null values for analysis\n            if hasattr(original_y, 'isna'):\n                non_null_mask = ~original_y.isna()\n                if non_null_mask.any():\n                    sample_values = original_y[non_null_mask]\n                    if len(sample_values) > 0:\n                        first_val = sample_values.iloc[0] if hasattr(sample_values, 'iloc') else sample_values[0]\n                        if isinstance(first_val, (int, float)) and not isinstance(first_val, bool):\n                            try:\n                                # Try to convert to numeric and check if float\n                                numeric_vals = pd.to_numeric(sample_values, errors='coerce')\n                                if not numeric_vals.isna().all():\n                                    if (numeric_vals % 1 != 0).any():\n                                        return \"continuous\" + suffix\n                            except (ValueError, TypeError):\n                                pass\n\n    if (\n        ndim == 2\n        and y.shape[0] == (y == 1).sum()\n        and y.shape[0] == (y == 0).sum() + (y == 1).sum()\n    ):\n        # [0, 1, 1, 1, 0, 0] but not [1, 0, 0, 1, 1, 1, 1, 0]\n        return \"multilabel-indicator\"\n\n    try:\n        unique_values = xp.unique_values(y)\n    except AttributeError:\n        # xp does not implement unique_values\n        unique_values = np.unique(y)\n\n    if len(unique_values) <= 2:\n        return \"binary\" + suffix\n    else:\n        return \"multiclass\" + suffix\n",
    "Location 4 (/root/hy/projects_1/scikit-learn/sklearn/utils/multiclass.py lines 354-367)": "\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    \n    # Handle object arrays that may contain nullable pandas data\n    if y.dtype == object:\n        # For object arrays, we need to check the actual values to determine uniqueness\n        try:\n            # Try to get unique values from the actual data\n            if issparse(y):\n                unique_vals = xp.unique_values(y.data)\n            else:\n                # For dense object arrays, flatten and get unique non-null values\n                flat_y = y.ravel()\n                non_null_mask = flat_y != None\n                if hasattr(flat_y, 'notna'):\n                    non_null_mask = flat_y.notna()\n                elif hasattr(flat_y, '__array__'):\n                    # Handle pandas nullable arrays\n                    try:\n                        non_null_mask = ~np.array([pd.isna(val) for val in flat_y])\n                    except:\n                        non_null_mask = np.array([val is not None for val in flat_y])\n                \n                if np.any(non_null_mask):\n                    unique_vals = xp.unique_values(flat_y[non_null_mask])\n                else:\n                    unique_vals = xp.asarray([])\n        except:\n            # Fallback to original logic if unique value extraction fails\n            unique_vals = xp.unique_values(y)\n    else:\n        unique_vals = xp.unique_values(y)\n    \n    if unique_vals.shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_1/scikit-learn/sklearn/utils/multiclass.py lines 343-345)": [
      "\n    if not issparse(y) and y.dtype == object:\n        # Check if this is a pandas nullable dtype converted to object array\n        if hasattr(y, '_pandas_nullable_dtype'):\n            # Use the stored original pandas dtype for classification\n            original_dtype = y._pandas_nullable_dtype\n            if hasattr(original_dtype, 'numpy_dtype'):\n                # For pandas nullable dtypes, get the underlying numpy dtype\n                underlying_dtype = original_dtype.numpy_dtype\n                if underlying_dtype.kind in 'biufc':\n                    # Numeric pandas nullable dtype - continue with normal classification\n                    pass\n                else:\n                    # Non-numeric pandas nullable dtype\n                    return \"unknown\"\n            else:\n                return \"unknown\"\n        elif not isinstance(y.flat[0], str):\n            # [obj_1] and not [\"label_1\"]\n            return \"unknown\"\n",
      "\n    if not issparse(y) and y.dtype == object:\n        # Check if this is a pandas nullable dtype converted to object array\n        if hasattr(y, '_pandas_nullable_dtype_info'):\n            # Use the stored pandas dtype info to determine the actual type\n            original_dtype = y._pandas_nullable_dtype_info\n            if hasattr(original_dtype, 'numpy_dtype'):\n                # For pandas nullable dtypes, use the underlying numpy dtype\n                effective_dtype = original_dtype.numpy_dtype\n            else:\n                # Fallback to string representation parsing\n                dtype_str = str(original_dtype).lower()\n                if 'int' in dtype_str:\n                    effective_dtype = np.int64\n                elif 'float' in dtype_str:\n                    effective_dtype = np.float64\n                elif 'bool' in dtype_str:\n                    effective_dtype = np.bool_\n                else:\n                    effective_dtype = object\n            \n            # If the effective dtype is not object, skip the object-specific check\n            if effective_dtype != object:\n                pass  # Continue with normal processing\n            else:\n                # Still an object dtype, check the first element\n                if not isinstance(y.flat[0], str):\n                    return \"unknown\"\n        else:\n            # Regular object array, check if first element is not a string\n            if not isinstance(y.flat[0], str):\n                return \"unknown\"\n",
      "\n    if not issparse(y) and y.dtype == object:\n        # Check if this is a pandas nullable dtype that was converted to object array\n        if hasattr(y, '_original_pandas_dtype'):\n            # Use the original pandas dtype for classification\n            original_dtype = y._original_pandas_dtype\n            if hasattr(original_dtype, 'numpy_dtype'):\n                # For pandas nullable dtypes, get the underlying numpy dtype\n                underlying_dtype = original_dtype.numpy_dtype\n                if underlying_dtype.kind in ('i', 'u', 'f'):\n                    # It's a numeric nullable dtype, continue with normal classification\n                    pass\n                elif underlying_dtype.kind in ('U', 'S') or underlying_dtype == object:\n                    # It's a string nullable dtype, continue with normal classification\n                    pass\n                else:\n                    return \"unknown\"\n            else:\n                return \"unknown\"\n        elif not isinstance(y.flat[0], str):\n            # [obj_1] and not [\"label_1\"]\n            return \"unknown\"\n",
      "\n    if not issparse(y) and y.dtype == object:\n        # Check if this is a pandas nullable dtype converted to object array\n        if hasattr(y, '_pandas_nullable_dtype'):\n            # Use the preserved pandas dtype info for classification\n            pandas_dtype = y._pandas_nullable_dtype\n            if hasattr(pandas_dtype, 'numpy_dtype'):\n                # For nullable integer/boolean dtypes, use the underlying numpy dtype\n                effective_dtype = pandas_dtype.numpy_dtype\n            elif str(pandas_dtype).startswith('string'):\n                # For nullable string dtype, treat as string\n                return \"unknown\"\n            else:\n                return \"unknown\"\n        elif len(y) > 0 and not isinstance(y.flat[0], str):\n            # [obj_1] and not [\"label_1\"] - check first non-null element\n            first_elem = None\n            for elem in y.flat:\n                if elem is not None and not (hasattr(elem, 'isna') and elem.isna()):\n                    first_elem = elem\n                    break\n            if first_elem is not None and not isinstance(first_elem, str):\n                return \"unknown\"\n",
      "\n    if not issparse(y) and y.dtype == object:\n        # Check if this is a pandas nullable dtype converted to object array\n        if hasattr(y, '_pandas_nullable_dtype'):\n            # Use the stored original pandas dtype for classification\n            original_dtype = y._pandas_nullable_dtype\n            if hasattr(original_dtype, 'numpy_dtype'):\n                # For pandas nullable dtypes, get the underlying numpy dtype\n                underlying_dtype = original_dtype.numpy_dtype\n                if underlying_dtype.kind in 'biufc':\n                    # Numeric nullable dtype - continue with normal classification\n                    pass\n                else:\n                    return \"unknown\"\n            else:\n                return \"unknown\"\n        elif not isinstance(y.flat[0], str):\n            # [obj_1] and not [\"label_1\"]\n            return \"unknown\"\n",
      "\n    if not issparse(y) and y.dtype == object:\n        # Handle nullable pandas dtypes that get converted to object arrays\n        if hasattr(y, '_pandas_dtype_info'):\n            # Use preserved pandas dtype information\n            original_dtype = y._pandas_dtype_info\n            if hasattr(original_dtype, 'numpy_dtype'):\n                # This is a nullable pandas dtype, check the underlying type\n                underlying_dtype = original_dtype.numpy_dtype\n                if underlying_dtype.kind in ('i', 'u', 'f'):\n                    # Nullable integer or float dtype, continue with normal classification\n                    pass\n                else:\n                    return \"unknown\"\n            else:\n                return \"unknown\"\n        elif len(y) > 0 and not isinstance(y.flat[0], str):\n            # [obj_1] and not [\"label_1\"]\n            return \"unknown\"\n",
      "\n    if not issparse(y) and y.dtype == object:\n        # Check if this is a pandas nullable dtype converted to object array\n        if hasattr(y, '_pandas_nullable_dtype'):\n            # Use the preserved pandas dtype info for classification\n            pandas_dtype = y._pandas_nullable_dtype\n            if hasattr(pandas_dtype, 'numpy_dtype'):\n                # For nullable dtypes, use the underlying numpy dtype\n                effective_dtype = pandas_dtype.numpy_dtype\n            else:\n                # Fallback: infer from non-null values\n                non_null_mask = y != None  # Using != None to handle pandas NA\n                if non_null_mask.any():\n                    non_null_values = y[non_null_mask]\n                    if len(non_null_values) > 0:\n                        first_val = non_null_values[0]\n                        if isinstance(first_val, (int, np.integer)):\n                            effective_dtype = np.dtype('int64')\n                        elif isinstance(first_val, (float, np.floating)):\n                            effective_dtype = np.dtype('float64')\n                        elif isinstance(first_val, str):\n                            effective_dtype = np.dtype('object')\n                        else:\n                            return \"unknown\"\n                    else:\n                        return \"unknown\"\n                else:\n                    return \"unknown\"\n            # Skip the string check for nullable dtypes since we know the underlying type\n        elif not isinstance(y.flat[0], str):\n            # [obj_1] and not [\"label_1\"]\n            return \"unknown\"\n",
      "\n    if not issparse(y) and y.dtype == object:\n        # Check if this is a pandas nullable dtype converted to object array\n        if hasattr(y, '_pandas_nullable_dtype'):\n            # Use the original pandas dtype for classification\n            original_dtype = y._pandas_nullable_dtype\n            if hasattr(original_dtype, 'numpy_dtype'):\n                # For pandas nullable dtypes, use the underlying numpy dtype\n                effective_dtype = original_dtype.numpy_dtype\n            else:\n                effective_dtype = original_dtype\n            # Skip the object dtype check since we know the real type\n        elif not isinstance(y.flat[0], str):\n            # [obj_1] and not [\"label_1\"]\n            return \"unknown\"\n"
    ],
    "Location 2 (/root/hy/projects_1/scikit-learn/sklearn/utils/multiclass.py lines 282-313)": [
      "\n    xp, is_array_api = get_namespace(y)\n    \n    # Store original pandas dtype information for nullable dtypes\n    original_pandas_dtype = None\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n        dtype_name = y.dtype.name\n        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'Float64', 'Float32', 'boolean', 'string']:\n            original_pandas_dtype = dtype_name\n    \n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n                # Preserve pandas dtype information as an attribute\n                if original_pandas_dtype is not None:\n                    y._original_pandas_dtype = original_pandas_dtype\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n                if original_pandas_dtype is not None:\n                    y._original_pandas_dtype = original_pandas_dtype\n",
      "\n    xp, is_array_api = get_namespace(y)\n    \n    # Store original pandas dtype information for nullable dtypes\n    original_pandas_dtype = None\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n        dtype_name = y.dtype.name\n        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'boolean', 'string']:\n            original_pandas_dtype = dtype_name\n    \n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n    \n    # Store the original pandas dtype info as an attribute for later use\n    if original_pandas_dtype is not None:\n        y._original_pandas_dtype = original_pandas_dtype\n",
      "\n    xp, is_array_api = get_namespace(y)\n    \n    # Store original pandas dtype information for nullable dtypes\n    original_pandas_dtype = None\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n        dtype_name = y.dtype.name\n        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'Float64', 'Float32', 'boolean', 'string']:\n            original_pandas_dtype = dtype_name\n    \n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n    \n    # Attach the original pandas dtype information for later use\n    if original_pandas_dtype is not None:\n        y._original_pandas_dtype = original_pandas_dtype\n",
      "\n    xp, is_array_api = get_namespace(y)\n    \n    # Store original pandas dtype information for nullable dtypes\n    original_pandas_dtype = None\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n        dtype_name = y.dtype.name\n        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'boolean', 'string']:\n            original_pandas_dtype = dtype_name\n    \n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n    \n    # Store the original pandas dtype for later use\n    if original_pandas_dtype is not None:\n        y._original_pandas_dtype = original_pandas_dtype\n",
      "\n    xp, is_array_api = get_namespace(y)\n    \n    # Check for nullable pandas dtypes before validation\n    original_pandas_dtype = None\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n        dtype_name = y.dtype.name\n        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'boolean', 'string']:\n            original_pandas_dtype = dtype_name\n    \n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n    \n    # Store the original pandas dtype info for later use\n    if original_pandas_dtype is not None:\n        y._original_pandas_dtype = original_pandas_dtype\n",
      "\n    xp, is_array_api = get_namespace(y)\n    \n    # Store original pandas dtype information for nullable dtypes\n    original_pandas_dtype = None\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n        # Check for nullable pandas dtypes\n        if y.dtype.name in ['Int64', 'Int32', 'Int16', 'Int8', 'Float64', 'Float32', 'boolean', 'string']:\n            original_pandas_dtype = y.dtype.name\n    \n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n                # Preserve pandas dtype information as an attribute\n                if original_pandas_dtype is not None:\n                    y._original_pandas_dtype = original_pandas_dtype\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n                if original_pandas_dtype is not None:\n                    y._original_pandas_dtype = original_pandas_dtype\n",
      "\n    xp, is_array_api = get_namespace(y)\n    \n    # Check for nullable pandas dtypes before validation\n    nullable_pandas_dtype = None\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n        dtype_name = y.dtype.name\n        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'Float64', 'Float32', 'boolean', 'string']:\n            nullable_pandas_dtype = dtype_name\n    \n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n                # Store nullable pandas dtype info as attribute if conversion happened\n                if nullable_pandas_dtype is not None:\n                    y._nullable_pandas_dtype = nullable_pandas_dtype\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n                if nullable_pandas_dtype is not None:\n                    y._nullable_pandas_dtype = nullable_pandas_dtype\n",
      "\n    xp, is_array_api = get_namespace(y)\n    \n    # Store original pandas dtype information for nullable dtypes\n    original_pandas_dtype = None\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n        dtype_name = y.dtype.name\n        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'boolean', 'string']:\n            original_pandas_dtype = dtype_name\n    \n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n    \n    # Store the original pandas dtype info in the array for later use\n    if original_pandas_dtype is not None and hasattr(y, '__dict__'):\n        y.__dict__['_original_pandas_dtype'] = original_pandas_dtype\n    elif original_pandas_dtype is not None:\n        # For arrays that don't support __dict__, use a different approach\n        if not hasattr(y, '_sklearn_dtype_info'):\n            try:\n                y._sklearn_dtype_info = {'original_pandas_dtype': original_pandas_dtype}\n            except AttributeError:\n                # If we can't attach the info, we'll rely on the type detection logic\n                pass\n"
    ],
    "Location 3 (/root/hy/projects_1/scikit-learn/sklearn/utils/multiclass.py lines 94-97)": [
      "\n    if (\n        label_type == \"multilabel-indicator\"\n        and len(\n            set(\n                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], dtype=None).shape[1] for y in ys\n            )\n        )\n        > 1\n    ):\n",
      "\n        set(\n            check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], dtype=None).shape[1] for y in ys\n        )\n",
      "\n        set(\n            check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], dtype=None).shape[1] for y in ys\n        )\n",
      "\n        set(\n            check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], dtype=None).shape[1] for y in ys\n        )\n",
      "\n        > 1\n    ):\n        raise ValueError(\n            \"Multi-label binary indicator input with different numbers of labels\"\n        )\n",
      "\ndef type_of_target(y, input_name=\"\"):\n    \"\"\"Determine the type of data indicated by the target.\n\n    Note that this type is the most specific type that can be inferred.\n    For example:\n\n        * ``binary`` is more specific but compatible with ``multiclass``.\n        * ``multiclass`` is more specific but compatible with ``continuous``.\n        * ``multilabel-indicator`` is more specific but compatible with\n          ``multiclass-multioutput``.\n\n    Parameters\n    ----------\n    y : {array-like, sparse matrix}\n        Target values. If a sparse matrix, it should be in CSR format.\n\n    input_name : str, default=\"\"\n        The data name used to construct the error message.\n\n        .. versionadded:: 1.1.0\n\n    Returns\n    -------\n    target_type : str\n        One of:\n\n        * 'continuous': `y` is an array-like of floats that are not all\n          integers, and is 1d or a column vector.\n        * 'continuous-multioutput': `y` is a 2d array of floats that are\n          not all integers, and both dimensions are of size > 1.\n        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n          vector.\n        * 'multiclass': `y` contains more than two discrete values, is not a\n          sequence of sequences, and is 1d or a column vector.\n        * 'multiclass-multioutput': `y` is a 2d array that contains more\n          than two discrete values, is not a sequence of sequences, and both\n          dimensions are of size > 1.\n        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n          of two dimensions with at least two columns, and at most 2 unique\n          values.\n        * 'unknown': `y` is array-like but none of the above apply.\n\n    Examples\n    --------\n    >>> from sklearn.utils.multiclass import type_of_target\n    >>> import numpy as np\n    >>> type_of_target([0.1, 0.6])\n    'continuous'\n    >>> type_of_target([1, -1, -1, 1])\n    'binary'\n    >>> type_of_target(['a', 'b', 'a'])\n    'binary'\n    >>> type_of_target([1.0, 2.0])\n    'binary'\n    >>> type_of_target([1, 0, 2])\n    'multiclass'\n    >>> type_of_target([1.0, 0.0, 3.0])\n    'multiclass'\n    >>> type_of_target(['a', 'b', 'c'])\n    'multiclass'\n    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n    'multiclass-multioutput'\n    >>> type_of_target([[1, 2]])\n    'multilabel-indicator'\n    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n    'continuous-multioutput'\n    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n    'multilabel-indicator'\n    \"\"\"\n    xp, is_array_api_compliant = get_namespace(y)\n    \n    # Store original pandas dtype information if available\n    original_pandas_dtype = None\n    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n        # Check for nullable pandas dtypes\n        if hasattr(y.dtype, 'na_value') or str(y.dtype).endswith(('Int64', 'Float64', 'boolean', 'string')):\n            original_pandas_dtype = y.dtype\n    \n    if is_array_api_compliant:\n        y = check_array(y, accept_sparse=False, ensure_2d=False, dtype=None, input_name=input_name)\n    else:\n        y = check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], ensure_2d=False, dtype=None, input_name=input_name)\n\n    # The old sequence of sequences format\n    try:\n        if (\n            not hasattr(y[0], \"__array__\")\n            and isinstance(y[0], Sequence)\n            and not isinstance(y[0], str)\n        ):\n            raise ValueError(\n                \"You appear to be using a legacy multi-label data\"\n                \" representation. Sequence of sequences are no\"\n                \" longer supported; use a binary array or sparse\"\n                \" matrix instead - the MultiLabelBinarizer\"\n                \" transformer can convert to this format.\"\n            )\n    except IndexError:\n        pass\n\n    # Invalid inputs\n    if y.ndim not in (1, 2):\n        # Number of dimension greater than 2: [[[1, 2]]]\n        return \"unknown\"\n\n    # Check if multioutput\n    if y.ndim == 2 and y.shape[1] == 0:\n        return \"unknown\"  # [[]]\n    \n    multioutput = y.ndim == 2 and y.shape[1] > 1\n\n    if y.dtype == object:\n        # Handle nullable pandas dtypes stored in object arrays\n        if original_pandas_dtype is not None:\n            # Extract non-null values to determine the actual type\n            if hasattr(y, 'dropna'):\n                non_null_values = y.dropna()\n            else:\n                # For numpy object arrays containing pandas data\n                non_null_mask = pd.notna(y) if 'pd' in globals() else ~pd.isna(y)\n                if non_null_mask.any():\n                    non_null_values = y[non_null_mask]\n                    # Try to infer the actual dtype from non-null values\n                    if len(non_null_values) > 0:\n                        sample_val = non_null_values[0] if hasattr(non_null_values, '__getitem__') else next(iter(non_null_values))\n                        if isinstance(sample_val, (int, np.integer)):\n                            y = y.astype(float)  # Convert to float to handle NaN\n                        elif isinstance(sample_val, (float, np.floating)):\n                            y = y.astype(float)\n                        elif isinstance(sample_val, (bool, np.bool_)):\n                            y = y.astype(float)  # Convert to float to handle NaN\n        else:\n            # Original object dtype handling\n            return \"unknown\"\n\n    # The targets are of a supported type, proceed to identify the exact target type\n    if multioutput:\n        # Ensure y is in a format where we can extract unique values per column\n        if issparse(y):\n            if y.format not in [\"csr\", \"csc\"]:\n                y = y.tocsr()\n        \n        suffix = \"-multioutput\"\n        for column in range(y.shape[1]):\n            if issparse(y):\n                column_y = y[:, column].toarray().ravel()\n            else:\n                column_y = y[:, column]\n            \n            unique_values = xp.unique_values(column_y)\n            \n            if len(unique_values) > 2:\n                return \"multiclass\" + suffix  # [1, 2, 3] or ['a', 'b', 'c']\n            elif len(unique_values) == 2:\n                if not _is_integral_float(unique_values):\n                    return \"continuous\" + suffix  # [1.1, 2.3] or float NaN\n        \n        # All columns are either binary or continuous with only 0s and 1s\n        if suffix == \"-multioutput\":\n            return \"multilabel-indicator\"  # [[1, 0], [0, 1]]\n        else:\n            return \"binary\" + suffix  # [[1, 1], [0, 0]]\n    else:\n        # Single output\n        unique_values = xp.unique_values(y)\n        \n        if len(unique_values) <= 2:\n            return \"binary\"  # [1, 0] or ['a', 'b']\n        elif _is_integral_float(unique_values):\n            return \"multiclass\"  # [1, 2, 3] or [1., 2., 3.]\n        else:\n            return \"continuous\"  # [1.1, 2.3]\n",
      "\n        > 1\n    ):\n        raise ValueError(\n            \"Multi-label binary indicator input with different numbers of labels\"\n        )\n",
      "\ndef _check_partial_fit_first_call(clf, classes=None):\n    \"\"\"Private helper function for factorizing common classes param logic.\n\n    Estimators that implement the ``partial_fit`` API should call this\n    function on the first call to ``partial_fit``.\n    State validation must be performed prior to calling this function.\n\n    Parameters\n    ----------\n    clf : estimator\n        The estimator to which partial_fit is being applied.\n\n    classes : array-like of shape (n_classes,), default=None\n        The unique classes.\n    \"\"\"\n    if getattr(clf, \"classes_\", None) is None and classes is None:\n        raise ValueError(\"classes must be passed on the first call \"\n                         \"to partial_fit.\")\n\n    elif classes is not None:\n        if getattr(clf, \"classes_\", None) is not None:\n            if not np.array_equal(clf.classes_, unique_labels(classes)):\n                raise ValueError(\"`classes=%r` is not the same as on last call \"\n                                 \"to partial_fit, was: %r\" % (classes,\n                                                              clf.classes_))\n\n        else:\n            # This is the first call to partial_fit\n            clf.classes_ = unique_labels(classes)\n    return clf\n\n\ndef class_distribution(y, sample_weight=None):\n    \"\"\"Compute class priors from a target array.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        The labels.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns\n    -------\n    classes : ndarray of shape (n_classes,)\n        The unique classes\n\n    n_classes : int\n        The number of unique classes\n\n    class_prior : ndarray of shape (n_classes,)\n        The class frequencies\n    \"\"\"\n    classes, y_indices = np.unique(y, return_inverse=True)\n    n_classes = classes.shape[0]\n\n    if sample_weight is not None:\n        class_prior = np.bincount(y_indices, weights=sample_weight)\n    else:\n        class_prior = np.bincount(y_indices)\n\n    class_prior = class_prior / class_prior.sum()\n    return classes, n_classes, class_prior\n\n\ndef _ovr_decision_function(predictions, confidences, n_classes):\n    \"\"\"Compute a continuous, tie-breaking OvR decision function from OvO.\n\n    It is important to include a continuous value, not only the predicted\n    class integer, to make computing AUC or calibration meaningful.\n\n    Parameters\n    ----------\n    predictions : array-like of shape (n_samples, n_classifiers)\n        Predicted classes for each binary classifier.\n\n    confidences : array-like of shape (n_samples, n_classifiers)\n        Decision functions or predicted probabilities for positive class\n        for each binary classifier.\n\n    n_classes : int\n        Number of classes. n_classifiers must be\n        ``n_classes * (n_classes - 1 ) / 2``.\n\n    Returns\n    -------\n    decision : array-like of shape (n_samples, n_classes)\n        The continuous OvR decision function.\n    \"\"\"\n    n_samples = predictions.shape[0]\n    votes = np.zeros((n_samples, n_classes))\n    sum_of_confidences = np.zeros((n_samples, n_classes))\n\n    k = 0\n    for i in range(n_classes):\n        for j in range(i + 1, n_classes):\n            sum_of_confidences[:, i] -= confidences[:, k]\n            sum_of_confidences[:, j] += confidences[:, k]\n            votes[predictions[:, k] == i, i] += 1\n            votes[predictions[:, k] == j, j] += 1\n            k += 1\n\n    # Monotonically transform the sum_of_confidences to (-1, 1) and add it to\n    # the votes. The monotonic transformation  is a fast and simple way to\n    # guarantee the consecutive classes are separated by at least 1\n    # (which is needed to ensure convergence in the binary search of\n    # DecisionBoundaryDisplay.from_estimator).\n    max_confidences = np.max(np.abs(sum_of_confidences), axis=1, keepdims=True)\n    max_confidences[max_confidences == 0] = 1\n    return votes + sum_of_confidences / max_confidences\n\n\ndef _check_multi_class(y):\n    \"\"\"Check if ``y`` is in a multilabel format.\n\n    Parameters\n    ----------\n    y : ndarray of shape (n_samples, n_outputs)\n        Target values. This function does not accept sparse matrices.\n\n    Returns\n    -------\n    out : str,\n        Return ``'multilabel-indicator'`` if ``y`` is a label indicator\n        matrix, ``'multiclass'`` if ``y`` is an array of integers,\n        ``'multiclass-multioutput'`` if ``y`` is a 2d array of integers,\n        and ``'binary'`` if ``y`` contains <= 2 unique values.\n    \"\"\"\n    xp, _ = get_namespace(y)\n    \n    if hasattr(y, 'shape') and len(y.shape) > 1 and y.shape[1] > 1:\n        suffix = \"-multioutput\"  # [[1, 2], [1, 2]]\n        ndim = 2\n    else:\n        suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n        ndim = 1\n\n    if y.dtype.kind == 'O':\n        # Handle object dtype arrays that may contain nullable pandas data\n        if hasattr(y, 'dtype') and hasattr(y.dtype, 'name') and 'pandas' in str(type(y.dtype)):\n            # This is likely a converted nullable pandas dtype\n            # Try to get the actual underlying type from non-null values\n            try:\n                non_null_mask = y != np.array(None)\n                if np.any(non_null_mask):\n                    sample_val = y[non_null_mask][0]\n                    if hasattr(sample_val, 'dtype'):\n                        actual_dtype = sample_val.dtype\n                    else:\n                        actual_dtype = type(sample_val)\n                    \n                    # Check if it's numeric\n                    if np.issubdtype(actual_dtype, np.number):\n                        # Count unique values for classification\n                        unique_values = xp.unique_values(y[non_null_mask])\n                        if len(unique_values) <= 2:\n                            return \"binary\" + suffix\n                        else:\n                            return \"multiclass\" + suffix\n            except (IndexError, TypeError, AttributeError):\n                pass\n        \n        # Fallback for other object arrays\n        return \"unknown\"\n\n    # Check if the array only contains 0 and 1.\n    if xp.isdtype(y.dtype, (\"integral\", \"bool\")):\n        unique_values = xp.unique_values(y)\n        if len(unique_values) <= 2:\n            return \"binary\" + suffix  # [0, 1, 1, 0]\n        else:\n            return \"multiclass\" + suffix  # [0, 1, 2, 2, 1]\n\n    if xp.isdtype(y.dtype, \"real floating\"):\n        if ndim == 2:\n            # This is a 2d array of floats, check if it's a label indicator matrix\n            if _is_integral_float(y) and xp.all((y == 0) | (y == 1)):\n                return \"multilabel-indicator\"  # [[1.0, 0.0], [0.0, 1.0]]\n            else:\n                # 2d array of floats that are not 0 and 1\n                return \"continuous\" + suffix  # [[1.1, 2.0], [3.0, 4.0]]\n        else:\n            # 1d array of floats, check if they are integral\n            if _is_integral_float(y):\n                unique_values = xp.unique_values(y)\n                if len(unique_values) <= 2:\n                    return \"binary\" + suffix  # [1.0, 0.0, 0.0, 1.0]\n                else:\n                    return \"multiclass\" + suffix  # [1.0, 2.0, 3.0, 3.0, 2.0]\n            else:\n                return \"continuous\" + suffix  # [1.1, 2.2, 3.1, 4.4, 2.5]\n    \n    return \"unknown\"\n"
    ],
    "Location 4 (/root/hy/projects_1/scikit-learn/sklearn/utils/multiclass.py lines 354-367)": [
      "\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    \n    # For object arrays that might contain nullable pandas data, \n    # use the preserved dtype info if available\n    if hasattr(y, '_sklearn_pandas_dtype_info'):\n        # Use the original pandas dtype for unique value calculation\n        original_dtype = y._sklearn_pandas_dtype_info['dtype']\n        if hasattr(original_dtype, 'numpy_dtype'):\n            # This is a nullable pandas dtype - count unique values properly\n            unique_count = len(xp.unique_values(y))\n        else:\n            unique_count = xp.unique_values(y).shape[0]\n    else:\n        unique_count = xp.unique_values(y).shape[0]\n    \n    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n",
      "\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    \n    # Handle object arrays that may contain nullable pandas data\n    if y.dtype == object:\n        # For object arrays, we need to handle potential nullable pandas data\n        # by examining the actual values rather than relying on dtype alone\n        try:\n            # Try to get unique values, handling potential pandas nullable types\n            unique_vals = xp.unique_values(y)\n            # If we have an object array with actual numeric/categorical data,\n            # the unique_values operation should work\n            unique_count = unique_vals.shape[0]\n        except (TypeError, AttributeError):\n            # If unique_values fails on object array, fall back to numpy\n            import numpy as np\n            unique_count = len(np.unique(y))\n    else:\n        unique_count = xp.unique_values(y).shape[0]\n    \n    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n",
      "\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    \n    # Handle object arrays that may contain nullable pandas data\n    if hasattr(y, 'dtype') and y.dtype == object:\n        # For object arrays, we need to check the actual values\n        # This handles nullable pandas dtypes that get converted to object arrays\n        flat_data = y.ravel() if not issparse(y) else y.data\n        non_null_mask = flat_data != None  # Handle None values from nullable dtypes\n        if xp.any(non_null_mask):\n            unique_vals = xp.unique_values(flat_data[non_null_mask])\n        else:\n            unique_vals = xp.asarray([])  # All null case\n    else:\n        unique_vals = xp.unique_values(y)\n    \n    if unique_vals.shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n",
      "\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    \n    # Handle object arrays that might contain nullable pandas data\n    if y.dtype == object:\n        # For object arrays, we need to look at the actual values to determine uniqueness\n        # This handles cases where nullable pandas dtypes were converted to object arrays\n        try:\n            # Try to get unique values from the actual data\n            if issparse(y):\n                unique_vals = xp.unique_values(y.data)\n            else:\n                # For dense object arrays, flatten and filter out None/NaN values for counting\n                flat_data = y.ravel()\n                non_null_data = [val for val in flat_data if val is not None and not (isinstance(val, float) and xp.isnan(val))]\n                if len(non_null_data) == 0:\n                    unique_vals = xp.asarray([])\n                else:\n                    unique_vals = xp.unique_values(xp.asarray(non_null_data))\n        except (TypeError, ValueError):\n            # Fallback to original logic if unique_values fails on object array\n            unique_vals = xp.unique_values(y)\n    else:\n        unique_vals = xp.unique_values(y)\n    \n    if unique_vals.shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n",
      "\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    \n    # Handle nullable pandas dtypes in object arrays\n    if hasattr(y, 'dtype') and y.dtype == object:\n        # Check if this is a converted nullable pandas dtype\n        sample_val = first_row[0] if len(first_row) > 0 else None\n        if sample_val is not None and hasattr(sample_val, 'dtype'):\n            # This might be a pandas array inside object dtype\n            try:\n                # Try to get unique values from the underlying data\n                if hasattr(y, 'ravel'):\n                    flat_data = y.ravel()\n                    non_null_data = [x for x in flat_data if x is not None and not (hasattr(x, 'isna') and x.isna())]\n                    if non_null_data:\n                        unique_count = len(set(non_null_data))\n                    else:\n                        unique_count = 0\n                else:\n                    unique_count = xp.unique_values(y).shape[0]\n            except:\n                unique_count = xp.unique_values(y).shape[0]\n        else:\n            unique_count = xp.unique_values(y).shape[0]\n    else:\n        unique_count = xp.unique_values(y).shape[0]\n    \n    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n",
      "\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    \n    # Handle nullable pandas dtypes for unique value counting\n    if hasattr(y, 'dtype') and y.dtype == object:\n        # Check if this is a converted nullable pandas dtype\n        sample_vals = y.flat[:min(100, y.size)]  # Sample for efficiency\n        non_null_vals = [v for v in sample_vals if v is not None and not (hasattr(v, '__class__') and 'pandas' in str(v.__class__) and hasattr(v, 'isna') and v.isna())]\n        if non_null_vals:\n            # Use non-null values for unique counting\n            unique_count = len(set(non_null_vals))\n        else:\n            unique_count = xp.unique_values(y).shape[0]\n    else:\n        unique_count = xp.unique_values(y).shape[0]\n    \n    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n",
      "\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    \n    # For object arrays that might contain nullable pandas data, use the original data for unique counting\n    unique_data = y\n    if hasattr(y, 'dtype') and y.dtype == object and hasattr(y, '_original_pandas_dtype'):\n        # Use original pandas series for accurate unique value counting\n        unique_data = y._original_pandas_series\n        unique_count = len(unique_data.dropna().unique())\n    else:\n        unique_count = xp.unique_values(y).shape[0]\n    \n    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n",
      "\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    \n    # Handle object arrays that may contain nullable pandas data\n    if y.dtype == object:\n        # For object arrays, we need to look at the actual values to count unique elements\n        # This handles cases where nullable pandas dtypes are converted to object arrays\n        try:\n            # Extract non-null values for unique counting\n            if issparse(y):\n                data_for_unique = y.data[y.data != None]\n            else:\n                flat_y = y.ravel()\n                data_for_unique = flat_y[flat_y != None]\n            \n            if len(data_for_unique) > 0:\n                unique_count = len(set(data_for_unique))\n            else:\n                unique_count = 0\n        except (TypeError, ValueError):\n            # Fallback to original logic if extraction fails\n            unique_count = xp.unique_values(y).shape[0]\n    else:\n        unique_count = xp.unique_values(y).shape[0]\n    \n    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..4182ee24b 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -91,6 +91,7 @@ def unique_labels(*ys):\n     if (\n         label_type == \"multilabel-indicator\"\n         and len(\n+\n             set(\n                 check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n             )\n@@ -279,6 +280,7 @@ def type_of_target(y, input_name=\"\"):\n     >>> type_of_target(np.array([[0, 1], [1, 1]]))\n     'multilabel-indicator'\n     \"\"\"\n+\n     xp, is_array_api = get_namespace(y)\n     valid = (\n         (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n@@ -298,6 +300,16 @@ def type_of_target(y, input_name=\"\"):\n     if is_multilabel(y):\n         return \"multilabel-indicator\"\n \n+    # Store information about nullable pandas dtypes before conversion\n+    is_nullable_pandas = False\n+    original_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y, 'isna'):\n+        # This is likely a pandas Series\n+        dtype_str = str(y.dtype)\n+        if dtype_str in ['Int64', 'Float64', 'boolean']:\n+            is_nullable_pandas = True\n+            original_pandas_dtype = dtype_str\n+\n     # DeprecationWarning will be replaced by ValueError, see NEP 34\n     # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n     # We therefore catch both deprecation (NumPy < 1.24) warning and\n@@ -312,6 +324,10 @@ def type_of_target(y, input_name=\"\"):\n                 # see NEP 34\n                 y = xp.asarray(y, dtype=object)\n \n+    # Store the nullable pandas information for later use in classification\n+    if is_nullable_pandas:\n+        y._sklearn_nullable_pandas_dtype = original_pandas_dtype\n+\n     # The old sequence of sequences format\n     try:\n         if (\n@@ -340,9 +356,35 @@ def type_of_target(y, input_name=\"\"):\n             return \"binary\"  # []\n         # 2-D empty array: [[]]\n         return \"unknown\"\n+\n+    # Check for nullable pandas dtypes that become object arrays\n     if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n+        # Check if this is a nullable pandas dtype converted to object array\n+        # by examining the non-null values to determine the underlying type\n+        try:\n+            # Get non-null values to check their type\n+            non_null_mask = y != np.array(None)\n+            if np.any(non_null_mask):\n+                non_null_values = y[non_null_mask]\n+                if len(non_null_values) > 0:\n+                    # Check if all non-null values are numeric (int or float)\n+                    first_val = non_null_values[0]\n+                    if isinstance(first_val, (int, float, np.integer, np.floating)):\n+                        # This is likely a nullable pandas numeric dtype\n+                        # Continue with normal classification instead of returning \"unknown\"\n+                        pass\n+                    else:\n+                        # [obj_1] and not [\"label_1\"] - original behavior for true object arrays\n+                        return \"unknown\"\n+                else:\n+                    # All values are null\n+                    return \"unknown\"\n+            else:\n+                # All values are null\n+                return \"unknown\"\n+        except (TypeError, ValueError):\n+            # [obj_1] and not [\"label_1\"] - fallback to original behavior\n+            return \"unknown\"\n \n     # Check if multioutput\n     if y.ndim == 2 and y.shape[1] > 1:\n@@ -351,6 +393,7 @@ def type_of_target(y, input_name=\"\"):\n         suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n \n     # Check float and contains non-integer float values\n+\n     if y.dtype.kind == \"f\":\n         # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n         data = y.data if issparse(y) else y\n@@ -360,7 +403,25 @@ def type_of_target(y, input_name=\"\"):\n \n     # Check multiclass\n     first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n+    \n+    # Handle nullable pandas dtypes that became object arrays\n+    if y.dtype == object and hasattr(y, '_sklearn_nullable_pandas_dtype'):\n+        # For nullable pandas dtypes, we need to handle None values in unique counting\n+        try:\n+            # Get non-null values for unique counting\n+            non_null_mask = y != np.array(None)\n+            if np.any(non_null_mask):\n+                non_null_values = y[non_null_mask]\n+                unique_count = len(np.unique(non_null_values))\n+            else:\n+                unique_count = 0\n+        except (TypeError, ValueError):\n+            # Fallback to regular unique counting\n+            unique_count = xp.unique_values(y).shape[0]\n+    else:\n+        unique_count = xp.unique_values(y).shape[0]\n+    \n+    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n         return \"multiclass\" + suffix\n     else:\n",
    "variant_0": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..60ee89214 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -91,10 +91,227 @@ def unique_labels(*ys):\n     if (\n         label_type == \"multilabel-indicator\"\n         and len(\n-            set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n-            )\n+\n+def check_classification_targets(y):\n+    \"\"\"Ensure that target y is of a non-regression type.\n+\n+    Only the following target types (as defined in type_of_target) are allowed:\n+        'binary', 'multiclass', 'multiclass-multioutput',\n+        'multilabel-indicator', 'multilabel-sequences'\n+\n+    Parameters\n+    ----------\n+    y : array-like\n+        Target values.\n+    \"\"\"\n+    y_type = type_of_target(y, input_name=\"y\")\n+    if y_type not in [\n+        \"binary\",\n+        \"multiclass\",\n+        \"multiclass-multioutput\",\n+        \"multilabel-indicator\",\n+        \"multilabel-sequences\",\n+    ]:\n+        raise ValueError(\n+            \"Unknown label type: %r. Maybe you are trying to fit a \"\n+            \"classifier, which expects discrete classes on a \"\n+            \"regression target with continuous values.\" % y_type\n         )\n+\n+\n+def type_of_target(y, input_name=\"\"):\n+    \"\"\"Determine the type of data indicated by the target.\n+\n+    Note that this type is the most specific type that can be inferred.\n+    For example:\n+\n+        * ``binary`` is more specific but compatible with ``multiclass``.\n+        * ``multiclass`` is more specific but compatible with\n+          ``multiclass-multioutput``.\n+        * ``multilabel-indicator`` is more specific but compatible with\n+          ``multiclass-multioutput``.\n+\n+    Parameters\n+    ----------\n+    y : {array-like, sparse matrix}\n+        Target values. If a sparse matrix, it should be in CSR format.\n+\n+    input_name : str, default=\"\"\n+        The data name used to construct the error message.\n+\n+        .. versionadded:: 1.1.0\n+\n+    Returns\n+    -------\n+    target_type : str\n+        One of:\n+\n+        * 'continuous': `y` is an array-like of floats that are not all\n+          integers, and is 1d or a column vector.\n+        * 'continuous-multioutput': `y` is a 2d array of floats that are\n+          not all integers, and both dimensions are of size > 1.\n+        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n+          vector.\n+        * 'multiclass': `y` contains more than two discrete values, is not a\n+          sequence of sequences, and is 1d or a column vector.\n+        * 'multiclass-multioutput': `y` is a 2d array that contains more\n+          than two discrete values, is not a sequence of sequences, and both\n+          dimensions are of size > 1.\n+        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n+          of two dimensions with at least two columns, and at most 2 unique\n+          values.\n+        * 'unknown': `y` is array-like but none of the above apply.\n+\n+    Examples\n+    --------\n+    >>> from sklearn.utils.multiclass import type_of_target\n+    >>> import numpy as np\n+    >>> type_of_target([0.1, 0.6])\n+    'continuous'\n+    >>> type_of_target([1, -1, -1, 1])\n+    'binary'\n+    >>> type_of_target(['a', 'b', 'a'])\n+    'binary'\n+    >>> type_of_target([1.0, 2.0])\n+    'binary'\n+    >>> type_of_target([1, 0, 2])\n+    'multiclass'\n+    >>> type_of_target([1.0, 0.0, 3.0])\n+    'multiclass'\n+    >>> type_of_target(['a', 'b', 'c'])\n+    'multiclass'\n+    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n+    'multiclass-multioutput'\n+    >>> type_of_target([[1, 2]])\n+    'multilabel-indicator'\n+    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n+    'continuous-multioutput'\n+    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n+    'multilabel-indicator'\n+    \"\"\"\n+    xp, is_array_api_compliant = get_namespace(y)\n+    if is_array_api_compliant:\n+        # array_api does not allow for 0-d arrays.\n+        y = xp.atleast_1d(xp.asarray(y))\n+    else:\n+        y = check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], ensure_2d=False)\n+    \n+    # Store original y for reference if it's a pandas Series with nullable dtype\n+    original_y = y\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name') and 'pandas' in str(type(y.dtype)):\n+        # This is a nullable pandas dtype, extract the base dtype\n+        if hasattr(y.dtype, 'numpy_dtype'):\n+            base_dtype = y.dtype.numpy_dtype\n+        elif hasattr(y.dtype, 'type'):\n+            base_dtype = y.dtype.type\n+        else:\n+            # For string dtypes and others, check the non-null values\n+            non_null_mask = ~y.isna() if hasattr(y, 'isna') else ~np.isnan(y.astype(float, errors='ignore'))\n+            if non_null_mask.any():\n+                non_null_values = y[non_null_mask]\n+                if len(non_null_values) > 0:\n+                    first_val = non_null_values.iloc[0] if hasattr(non_null_values, 'iloc') else non_null_values[0]\n+                    base_dtype = type(first_val)\n+                else:\n+                    base_dtype = object\n+            else:\n+                base_dtype = object\n+    else:\n+        base_dtype = getattr(y, 'dtype', object)\n+\n+    # The old sequence of sequences format\n+    try:\n+        if (\n+            not hasattr(y[0], \"__array__\")\n+            and isinstance(y[0], Sequence)\n+            and not isinstance(y[0], str)\n+        ):\n+            raise ValueError(\n+                \"You appear to be using a legacy multi-label data\"\n+                \" representation. Sequence of sequences are no\"\n+                \" longer supported; use a binary array or sparse\"\n+                \" matrix instead - the MultiLabelBinarizer\"\n+                \" transformer can convert to this format.\"\n+            )\n+    except (IndexError, TypeError):\n+        pass\n+\n+    # Invalid inputs\n+    if y.ndim not in (1, 2):\n+        # Number of dimension greater than 2: [[[1, 2]]]\n+        return \"unknown\"\n+\n+    # The case of an empty array\n+    if y.shape[0] == 0:\n+        return \"unknown\"\n+\n+    # Check if multioutput\n+    if y.ndim == 2 and y.shape[1] > 1:\n+        suffix = \"-multioutput\"  # [[1, 2], [1, 2]]\n+        ndim = 2\n+        y = y.reshape(-1)\n+    else:\n+        suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n+        ndim = 1\n+        y = y.reshape(-1)\n+\n+    # check float and contains non-integer float values\n+    if base_dtype.kind == \"f\":\n+        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n+        _assert_all_finite(y)\n+        return \"continuous\" + suffix\n+\n+    # Check for object dtype that might contain nullable pandas data\n+    if y.dtype.kind == \"O\":\n+        # Handle object arrays that might contain nullable pandas data\n+        if hasattr(original_y, 'dtype') and hasattr(original_y.dtype, 'name'):\n+            # Check if this is a nullable pandas dtype stored in object array\n+            if 'string' in str(original_y.dtype).lower() or 'String' in str(original_y.dtype):\n+                # This is pandas string data, treat as categorical\n+                pass\n+            elif hasattr(original_y.dtype, 'numpy_dtype'):\n+                # Use the underlying numpy dtype for classification\n+                if original_y.dtype.numpy_dtype.kind == \"f\":\n+                    _assert_all_finite(y[~pd.isna(original_y)] if hasattr(original_y, 'isna') else y)\n+                    return \"continuous\" + suffix\n+        \n+        # For regular object arrays, check the actual values\n+        if len(y) > 0:\n+            # Get non-null values for analysis\n+            if hasattr(original_y, 'isna'):\n+                non_null_mask = ~original_y.isna()\n+                if non_null_mask.any():\n+                    sample_values = original_y[non_null_mask]\n+                    if len(sample_values) > 0:\n+                        first_val = sample_values.iloc[0] if hasattr(sample_values, 'iloc') else sample_values[0]\n+                        if isinstance(first_val, (int, float)) and not isinstance(first_val, bool):\n+                            try:\n+                                # Try to convert to numeric and check if float\n+                                numeric_vals = pd.to_numeric(sample_values, errors='coerce')\n+                                if not numeric_vals.isna().all():\n+                                    if (numeric_vals % 1 != 0).any():\n+                                        return \"continuous\" + suffix\n+                            except (ValueError, TypeError):\n+                                pass\n+\n+    if (\n+        ndim == 2\n+        and y.shape[0] == (y == 1).sum()\n+        and y.shape[0] == (y == 0).sum() + (y == 1).sum()\n+    ):\n+        # [0, 1, 1, 1, 0, 0] but not [1, 0, 0, 1, 1, 1, 1, 0]\n+        return \"multilabel-indicator\"\n+\n+    try:\n+        unique_values = xp.unique_values(y)\n+    except AttributeError:\n+        # xp does not implement unique_values\n+        unique_values = np.unique(y)\n+\n+    if len(unique_values) <= 2:\n+        return \"binary\" + suffix\n+    else:\n+        return \"multiclass\" + suffix\n         > 1\n     ):\n         raise ValueError(\n@@ -279,7 +496,19 @@ def type_of_target(y, input_name=\"\"):\n     >>> type_of_target(np.array([[0, 1], [1, 1]]))\n     'multilabel-indicator'\n     \"\"\"\n+\n     xp, is_array_api = get_namespace(y)\n+    \n+    # Check for nullable pandas dtypes before conversion\n+    nullable_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n+        # Handle nullable pandas integer/boolean dtypes\n+        if y.dtype.name in ['Int8', 'Int16', 'Int32', 'Int64', 'boolean']:\n+            nullable_pandas_dtype = y.dtype.name\n+        # Handle nullable pandas string dtype\n+        elif str(y.dtype) == 'string':\n+            nullable_pandas_dtype = 'string'\n+    \n     valid = (\n         (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n         and not isinstance(y, str)\n@@ -307,10 +536,18 @@ def type_of_target(y, input_name=\"\"):\n         if not issparse(y):\n             try:\n                 y = xp.asarray(y)\n+                # Store nullable pandas dtype info as array attribute if conversion resulted in object dtype\n+                if nullable_pandas_dtype is not None and y.dtype == object:\n+                    y = np.asarray(y)  # Ensure it's a numpy array so we can add attributes\n+                    y._nullable_pandas_dtype = nullable_pandas_dtype\n             except (np.VisibleDeprecationWarning, ValueError):\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n                 y = xp.asarray(y, dtype=object)\n+                # Store nullable pandas dtype info even for explicitly object arrays\n+                if nullable_pandas_dtype is not None:\n+                    y = np.asarray(y)\n+                    y._nullable_pandas_dtype = nullable_pandas_dtype\n \n     # The old sequence of sequences format\n     try:\n@@ -340,9 +577,23 @@ def type_of_target(y, input_name=\"\"):\n             return \"binary\"  # []\n         # 2-D empty array: [[]]\n         return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n+\n+    if not issparse(y) and y.dtype == object:\n+        # Check if this is a nullable pandas dtype converted to object array\n+        if hasattr(y, '_pandas_nullable_dtype_info'):\n+            # Use the preserved pandas dtype information\n+            original_dtype = y._pandas_nullable_dtype_info\n+            if hasattr(original_dtype, 'numpy_dtype'):\n+                # This is a nullable pandas dtype, skip the object check\n+                pass\n+            else:\n+                # Regular object array with non-string objects\n+                if not isinstance(y.flat[0], str):\n+                    return \"unknown\"\n+        else:\n+            # Regular object array check\n+            if not isinstance(y.flat[0], str):\n+                return \"unknown\"\n \n     # Check if multioutput\n     if y.ndim == 2 and y.shape[1] > 1:\n@@ -351,16 +602,41 @@ def type_of_target(y, input_name=\"\"):\n         suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n \n     # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n \n     # Check multiclass\n     first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n+    \n+    # Handle object arrays that may contain nullable pandas data\n+    if y.dtype == object:\n+        # For object arrays, we need to check the actual values to determine uniqueness\n+        try:\n+            # Try to get unique values from the actual data\n+            if issparse(y):\n+                unique_vals = xp.unique_values(y.data)\n+            else:\n+                # For dense object arrays, flatten and get unique non-null values\n+                flat_y = y.ravel()\n+                non_null_mask = flat_y != None\n+                if hasattr(flat_y, 'notna'):\n+                    non_null_mask = flat_y.notna()\n+                elif hasattr(flat_y, '__array__'):\n+                    # Handle pandas nullable arrays\n+                    try:\n+                        non_null_mask = ~np.array([pd.isna(val) for val in flat_y])\n+                    except:\n+                        non_null_mask = np.array([val is not None for val in flat_y])\n+                \n+                if np.any(non_null_mask):\n+                    unique_vals = xp.unique_values(flat_y[non_null_mask])\n+                else:\n+                    unique_vals = xp.asarray([])\n+        except:\n+            # Fallback to original logic if unique value extraction fails\n+            unique_vals = xp.unique_values(y)\n+    else:\n+        unique_vals = xp.unique_values(y)\n+    \n+    if unique_vals.shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n         return \"multiclass\" + suffix\n     else:\n",
    "variant_1": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..600ece889 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -88,14 +88,20 @@ def unique_labels(*ys):\n     label_type = ys_types.pop()\n \n     # Check consistency for the indicator format\n+    if (\n+        label_type == \"multilabel-indicator\"\n+        and len(\n+\n     if (\n         label_type == \"multilabel-indicator\"\n         and len(\n             set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n+                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], dtype=None).shape[1] for y in ys\n             )\n         )\n         > 1\n+    ):\n+        > 1\n     ):\n         raise ValueError(\n             \"Multi-label binary indicator input with different numbers of labels\"\n@@ -279,7 +285,16 @@ def type_of_target(y, input_name=\"\"):\n     >>> type_of_target(np.array([[0, 1], [1, 1]]))\n     'multilabel-indicator'\n     \"\"\"\n+\n     xp, is_array_api = get_namespace(y)\n+    \n+    # Store original pandas dtype information for nullable dtypes\n+    original_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n+        dtype_name = y.dtype.name\n+        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'Float64', 'Float32', 'boolean', 'string']:\n+            original_pandas_dtype = dtype_name\n+    \n     valid = (\n         (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n         and not isinstance(y, str)\n@@ -307,10 +322,15 @@ def type_of_target(y, input_name=\"\"):\n         if not issparse(y):\n             try:\n                 y = xp.asarray(y)\n+                # Preserve pandas dtype information as an attribute\n+                if original_pandas_dtype is not None:\n+                    y._original_pandas_dtype = original_pandas_dtype\n             except (np.VisibleDeprecationWarning, ValueError):\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n                 y = xp.asarray(y, dtype=object)\n+                if original_pandas_dtype is not None:\n+                    y._original_pandas_dtype = original_pandas_dtype\n \n     # The old sequence of sequences format\n     try:\n@@ -340,9 +360,26 @@ def type_of_target(y, input_name=\"\"):\n             return \"binary\"  # []\n         # 2-D empty array: [[]]\n         return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n+\n+    if not issparse(y) and y.dtype == object:\n+        # Check if this is a pandas nullable dtype converted to object array\n+        if hasattr(y, '_pandas_nullable_dtype'):\n+            # Use the stored original pandas dtype for classification\n+            original_dtype = y._pandas_nullable_dtype\n+            if hasattr(original_dtype, 'numpy_dtype'):\n+                # For pandas nullable dtypes, get the underlying numpy dtype\n+                underlying_dtype = original_dtype.numpy_dtype\n+                if underlying_dtype.kind in 'biufc':\n+                    # Numeric pandas nullable dtype - continue with normal classification\n+                    pass\n+                else:\n+                    # Non-numeric pandas nullable dtype\n+                    return \"unknown\"\n+            else:\n+                return \"unknown\"\n+        elif not isinstance(y.flat[0], str):\n+            # [obj_1] and not [\"label_1\"]\n+            return \"unknown\"\n \n     # Check if multioutput\n     if y.ndim == 2 and y.shape[1] > 1:\n@@ -351,16 +388,24 @@ def type_of_target(y, input_name=\"\"):\n         suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n \n     # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n \n     # Check multiclass\n     first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n+    \n+    # For object arrays that might contain nullable pandas data, \n+    # use the preserved dtype info if available\n+    if hasattr(y, '_sklearn_pandas_dtype_info'):\n+        # Use the original pandas dtype for unique value calculation\n+        original_dtype = y._sklearn_pandas_dtype_info['dtype']\n+        if hasattr(original_dtype, 'numpy_dtype'):\n+            # This is a nullable pandas dtype - count unique values properly\n+            unique_count = len(xp.unique_values(y))\n+        else:\n+            unique_count = xp.unique_values(y).shape[0]\n+    else:\n+        unique_count = xp.unique_values(y).shape[0]\n+    \n+    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n         return \"multiclass\" + suffix\n     else:\n",
    "variant_2": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..e0902c9e8 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -91,9 +91,9 @@ def unique_labels(*ys):\n     if (\n         label_type == \"multilabel-indicator\"\n         and len(\n-            set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n-            )\n+\n+        set(\n+            check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], dtype=None).shape[1] for y in ys\n         )\n         > 1\n     ):\n@@ -279,7 +279,16 @@ def type_of_target(y, input_name=\"\"):\n     >>> type_of_target(np.array([[0, 1], [1, 1]]))\n     'multilabel-indicator'\n     \"\"\"\n+\n     xp, is_array_api = get_namespace(y)\n+    \n+    # Store original pandas dtype information for nullable dtypes\n+    original_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n+        dtype_name = y.dtype.name\n+        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'boolean', 'string']:\n+            original_pandas_dtype = dtype_name\n+    \n     valid = (\n         (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n         and not isinstance(y, str)\n@@ -311,6 +320,10 @@ def type_of_target(y, input_name=\"\"):\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n                 y = xp.asarray(y, dtype=object)\n+    \n+    # Store the original pandas dtype info as an attribute for later use\n+    if original_pandas_dtype is not None:\n+        y._original_pandas_dtype = original_pandas_dtype\n \n     # The old sequence of sequences format\n     try:\n@@ -340,9 +353,38 @@ def type_of_target(y, input_name=\"\"):\n             return \"binary\"  # []\n         # 2-D empty array: [[]]\n         return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n+\n+    if not issparse(y) and y.dtype == object:\n+        # Check if this is a pandas nullable dtype converted to object array\n+        if hasattr(y, '_pandas_nullable_dtype_info'):\n+            # Use the stored pandas dtype info to determine the actual type\n+            original_dtype = y._pandas_nullable_dtype_info\n+            if hasattr(original_dtype, 'numpy_dtype'):\n+                # For pandas nullable dtypes, use the underlying numpy dtype\n+                effective_dtype = original_dtype.numpy_dtype\n+            else:\n+                # Fallback to string representation parsing\n+                dtype_str = str(original_dtype).lower()\n+                if 'int' in dtype_str:\n+                    effective_dtype = np.int64\n+                elif 'float' in dtype_str:\n+                    effective_dtype = np.float64\n+                elif 'bool' in dtype_str:\n+                    effective_dtype = np.bool_\n+                else:\n+                    effective_dtype = object\n+            \n+            # If the effective dtype is not object, skip the object-specific check\n+            if effective_dtype != object:\n+                pass  # Continue with normal processing\n+            else:\n+                # Still an object dtype, check the first element\n+                if not isinstance(y.flat[0], str):\n+                    return \"unknown\"\n+        else:\n+            # Regular object array, check if first element is not a string\n+            if not isinstance(y.flat[0], str):\n+                return \"unknown\"\n \n     # Check if multioutput\n     if y.ndim == 2 and y.shape[1] > 1:\n@@ -351,16 +393,28 @@ def type_of_target(y, input_name=\"\"):\n         suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n \n     # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n \n     # Check multiclass\n     first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n+    \n+    # Handle object arrays that may contain nullable pandas data\n+    if y.dtype == object:\n+        # For object arrays, we need to handle potential nullable pandas data\n+        # by examining the actual values rather than relying on dtype alone\n+        try:\n+            # Try to get unique values, handling potential pandas nullable types\n+            unique_vals = xp.unique_values(y)\n+            # If we have an object array with actual numeric/categorical data,\n+            # the unique_values operation should work\n+            unique_count = unique_vals.shape[0]\n+        except (TypeError, AttributeError):\n+            # If unique_values fails on object array, fall back to numpy\n+            import numpy as np\n+            unique_count = len(np.unique(y))\n+    else:\n+        unique_count = xp.unique_values(y).shape[0]\n+    \n+    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n         return \"multiclass\" + suffix\n     else:\n",
    "variant_3": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..f684ffa04 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -91,9 +91,9 @@ def unique_labels(*ys):\n     if (\n         label_type == \"multilabel-indicator\"\n         and len(\n-            set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n-            )\n+\n+        set(\n+            check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], dtype=None).shape[1] for y in ys\n         )\n         > 1\n     ):\n@@ -279,7 +279,16 @@ def type_of_target(y, input_name=\"\"):\n     >>> type_of_target(np.array([[0, 1], [1, 1]]))\n     'multilabel-indicator'\n     \"\"\"\n+\n     xp, is_array_api = get_namespace(y)\n+    \n+    # Store original pandas dtype information for nullable dtypes\n+    original_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n+        dtype_name = y.dtype.name\n+        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'Float64', 'Float32', 'boolean', 'string']:\n+            original_pandas_dtype = dtype_name\n+    \n     valid = (\n         (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n         and not isinstance(y, str)\n@@ -311,6 +320,10 @@ def type_of_target(y, input_name=\"\"):\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n                 y = xp.asarray(y, dtype=object)\n+    \n+    # Attach the original pandas dtype information for later use\n+    if original_pandas_dtype is not None:\n+        y._original_pandas_dtype = original_pandas_dtype\n \n     # The old sequence of sequences format\n     try:\n@@ -340,9 +353,28 @@ def type_of_target(y, input_name=\"\"):\n             return \"binary\"  # []\n         # 2-D empty array: [[]]\n         return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n+\n+    if not issparse(y) and y.dtype == object:\n+        # Check if this is a pandas nullable dtype that was converted to object array\n+        if hasattr(y, '_original_pandas_dtype'):\n+            # Use the original pandas dtype for classification\n+            original_dtype = y._original_pandas_dtype\n+            if hasattr(original_dtype, 'numpy_dtype'):\n+                # For pandas nullable dtypes, get the underlying numpy dtype\n+                underlying_dtype = original_dtype.numpy_dtype\n+                if underlying_dtype.kind in ('i', 'u', 'f'):\n+                    # It's a numeric nullable dtype, continue with normal classification\n+                    pass\n+                elif underlying_dtype.kind in ('U', 'S') or underlying_dtype == object:\n+                    # It's a string nullable dtype, continue with normal classification\n+                    pass\n+                else:\n+                    return \"unknown\"\n+            else:\n+                return \"unknown\"\n+        elif not isinstance(y.flat[0], str):\n+            # [obj_1] and not [\"label_1\"]\n+            return \"unknown\"\n \n     # Check if multioutput\n     if y.ndim == 2 and y.shape[1] > 1:\n@@ -351,16 +383,24 @@ def type_of_target(y, input_name=\"\"):\n         suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n \n     # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n \n     # Check multiclass\n     first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n+    \n+    # Handle object arrays that may contain nullable pandas data\n+    if hasattr(y, 'dtype') and y.dtype == object:\n+        # For object arrays, we need to check the actual values\n+        # This handles nullable pandas dtypes that get converted to object arrays\n+        flat_data = y.ravel() if not issparse(y) else y.data\n+        non_null_mask = flat_data != None  # Handle None values from nullable dtypes\n+        if xp.any(non_null_mask):\n+            unique_vals = xp.unique_values(flat_data[non_null_mask])\n+        else:\n+            unique_vals = xp.asarray([])  # All null case\n+    else:\n+        unique_vals = xp.unique_values(y)\n+    \n+    if unique_vals.shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n         return \"multiclass\" + suffix\n     else:\n",
    "variant_4": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..fad7b06cf 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -91,9 +91,9 @@ def unique_labels(*ys):\n     if (\n         label_type == \"multilabel-indicator\"\n         and len(\n-            set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n-            )\n+\n+        set(\n+            check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], dtype=None).shape[1] for y in ys\n         )\n         > 1\n     ):\n@@ -279,7 +279,16 @@ def type_of_target(y, input_name=\"\"):\n     >>> type_of_target(np.array([[0, 1], [1, 1]]))\n     'multilabel-indicator'\n     \"\"\"\n+\n     xp, is_array_api = get_namespace(y)\n+    \n+    # Store original pandas dtype information for nullable dtypes\n+    original_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n+        dtype_name = y.dtype.name\n+        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'boolean', 'string']:\n+            original_pandas_dtype = dtype_name\n+    \n     valid = (\n         (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n         and not isinstance(y, str)\n@@ -311,6 +320,10 @@ def type_of_target(y, input_name=\"\"):\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n                 y = xp.asarray(y, dtype=object)\n+    \n+    # Store the original pandas dtype for later use\n+    if original_pandas_dtype is not None:\n+        y._original_pandas_dtype = original_pandas_dtype\n \n     # The old sequence of sequences format\n     try:\n@@ -340,9 +353,29 @@ def type_of_target(y, input_name=\"\"):\n             return \"binary\"  # []\n         # 2-D empty array: [[]]\n         return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n+\n+    if not issparse(y) and y.dtype == object:\n+        # Check if this is a pandas nullable dtype converted to object array\n+        if hasattr(y, '_pandas_nullable_dtype'):\n+            # Use the preserved pandas dtype info for classification\n+            pandas_dtype = y._pandas_nullable_dtype\n+            if hasattr(pandas_dtype, 'numpy_dtype'):\n+                # For nullable integer/boolean dtypes, use the underlying numpy dtype\n+                effective_dtype = pandas_dtype.numpy_dtype\n+            elif str(pandas_dtype).startswith('string'):\n+                # For nullable string dtype, treat as string\n+                return \"unknown\"\n+            else:\n+                return \"unknown\"\n+        elif len(y) > 0 and not isinstance(y.flat[0], str):\n+            # [obj_1] and not [\"label_1\"] - check first non-null element\n+            first_elem = None\n+            for elem in y.flat:\n+                if elem is not None and not (hasattr(elem, 'isna') and elem.isna()):\n+                    first_elem = elem\n+                    break\n+            if first_elem is not None and not isinstance(first_elem, str):\n+                return \"unknown\"\n \n     # Check if multioutput\n     if y.ndim == 2 and y.shape[1] > 1:\n@@ -351,16 +384,33 @@ def type_of_target(y, input_name=\"\"):\n         suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n \n     # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n \n     # Check multiclass\n     first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n+    \n+    # Handle object arrays that might contain nullable pandas data\n+    if y.dtype == object:\n+        # For object arrays, we need to look at the actual values to determine uniqueness\n+        # This handles cases where nullable pandas dtypes were converted to object arrays\n+        try:\n+            # Try to get unique values from the actual data\n+            if issparse(y):\n+                unique_vals = xp.unique_values(y.data)\n+            else:\n+                # For dense object arrays, flatten and filter out None/NaN values for counting\n+                flat_data = y.ravel()\n+                non_null_data = [val for val in flat_data if val is not None and not (isinstance(val, float) and xp.isnan(val))]\n+                if len(non_null_data) == 0:\n+                    unique_vals = xp.asarray([])\n+                else:\n+                    unique_vals = xp.unique_values(xp.asarray(non_null_data))\n+        except (TypeError, ValueError):\n+            # Fallback to original logic if unique_values fails on object array\n+            unique_vals = xp.unique_values(y)\n+    else:\n+        unique_vals = xp.unique_values(y)\n+    \n+    if unique_vals.shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n         return \"multiclass\" + suffix\n     else:\n",
    "variant_5": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..fbd11c2e3 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -91,9 +91,11 @@ def unique_labels(*ys):\n     if (\n         label_type == \"multilabel-indicator\"\n         and len(\n-            set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n-            )\n+\n+        > 1\n+    ):\n+        raise ValueError(\n+            \"Multi-label binary indicator input with different numbers of labels\"\n         )\n         > 1\n     ):\n@@ -279,7 +281,16 @@ def type_of_target(y, input_name=\"\"):\n     >>> type_of_target(np.array([[0, 1], [1, 1]]))\n     'multilabel-indicator'\n     \"\"\"\n+\n     xp, is_array_api = get_namespace(y)\n+    \n+    # Check for nullable pandas dtypes before validation\n+    original_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n+        dtype_name = y.dtype.name\n+        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'boolean', 'string']:\n+            original_pandas_dtype = dtype_name\n+    \n     valid = (\n         (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n         and not isinstance(y, str)\n@@ -311,6 +322,10 @@ def type_of_target(y, input_name=\"\"):\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n                 y = xp.asarray(y, dtype=object)\n+    \n+    # Store the original pandas dtype info for later use\n+    if original_pandas_dtype is not None:\n+        y._original_pandas_dtype = original_pandas_dtype\n \n     # The old sequence of sequences format\n     try:\n@@ -340,9 +355,25 @@ def type_of_target(y, input_name=\"\"):\n             return \"binary\"  # []\n         # 2-D empty array: [[]]\n         return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n+\n+    if not issparse(y) and y.dtype == object:\n+        # Check if this is a pandas nullable dtype converted to object array\n+        if hasattr(y, '_pandas_nullable_dtype'):\n+            # Use the stored original pandas dtype for classification\n+            original_dtype = y._pandas_nullable_dtype\n+            if hasattr(original_dtype, 'numpy_dtype'):\n+                # For pandas nullable dtypes, get the underlying numpy dtype\n+                underlying_dtype = original_dtype.numpy_dtype\n+                if underlying_dtype.kind in 'biufc':\n+                    # Numeric nullable dtype - continue with normal classification\n+                    pass\n+                else:\n+                    return \"unknown\"\n+            else:\n+                return \"unknown\"\n+        elif not isinstance(y.flat[0], str):\n+            # [obj_1] and not [\"label_1\"]\n+            return \"unknown\"\n \n     # Check if multioutput\n     if y.ndim == 2 and y.shape[1] > 1:\n@@ -351,16 +382,35 @@ def type_of_target(y, input_name=\"\"):\n         suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n \n     # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n \n     # Check multiclass\n     first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n+    \n+    # Handle nullable pandas dtypes in object arrays\n+    if hasattr(y, 'dtype') and y.dtype == object:\n+        # Check if this is a converted nullable pandas dtype\n+        sample_val = first_row[0] if len(first_row) > 0 else None\n+        if sample_val is not None and hasattr(sample_val, 'dtype'):\n+            # This might be a pandas array inside object dtype\n+            try:\n+                # Try to get unique values from the underlying data\n+                if hasattr(y, 'ravel'):\n+                    flat_data = y.ravel()\n+                    non_null_data = [x for x in flat_data if x is not None and not (hasattr(x, 'isna') and x.isna())]\n+                    if non_null_data:\n+                        unique_count = len(set(non_null_data))\n+                    else:\n+                        unique_count = 0\n+                else:\n+                    unique_count = xp.unique_values(y).shape[0]\n+            except:\n+                unique_count = xp.unique_values(y).shape[0]\n+        else:\n+            unique_count = xp.unique_values(y).shape[0]\n+    else:\n+        unique_count = xp.unique_values(y).shape[0]\n+    \n+    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n         return \"multiclass\" + suffix\n     else:\n",
    "variant_6": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..11ca6a4e0 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -91,10 +91,179 @@ def unique_labels(*ys):\n     if (\n         label_type == \"multilabel-indicator\"\n         and len(\n-            set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n+\n+def type_of_target(y, input_name=\"\"):\n+    \"\"\"Determine the type of data indicated by the target.\n+\n+    Note that this type is the most specific type that can be inferred.\n+    For example:\n+\n+        * ``binary`` is more specific but compatible with ``multiclass``.\n+        * ``multiclass`` is more specific but compatible with ``continuous``.\n+        * ``multilabel-indicator`` is more specific but compatible with\n+          ``multiclass-multioutput``.\n+\n+    Parameters\n+    ----------\n+    y : {array-like, sparse matrix}\n+        Target values. If a sparse matrix, it should be in CSR format.\n+\n+    input_name : str, default=\"\"\n+        The data name used to construct the error message.\n+\n+        .. versionadded:: 1.1.0\n+\n+    Returns\n+    -------\n+    target_type : str\n+        One of:\n+\n+        * 'continuous': `y` is an array-like of floats that are not all\n+          integers, and is 1d or a column vector.\n+        * 'continuous-multioutput': `y` is a 2d array of floats that are\n+          not all integers, and both dimensions are of size > 1.\n+        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n+          vector.\n+        * 'multiclass': `y` contains more than two discrete values, is not a\n+          sequence of sequences, and is 1d or a column vector.\n+        * 'multiclass-multioutput': `y` is a 2d array that contains more\n+          than two discrete values, is not a sequence of sequences, and both\n+          dimensions are of size > 1.\n+        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n+          of two dimensions with at least two columns, and at most 2 unique\n+          values.\n+        * 'unknown': `y` is array-like but none of the above apply.\n+\n+    Examples\n+    --------\n+    >>> from sklearn.utils.multiclass import type_of_target\n+    >>> import numpy as np\n+    >>> type_of_target([0.1, 0.6])\n+    'continuous'\n+    >>> type_of_target([1, -1, -1, 1])\n+    'binary'\n+    >>> type_of_target(['a', 'b', 'a'])\n+    'binary'\n+    >>> type_of_target([1.0, 2.0])\n+    'binary'\n+    >>> type_of_target([1, 0, 2])\n+    'multiclass'\n+    >>> type_of_target([1.0, 0.0, 3.0])\n+    'multiclass'\n+    >>> type_of_target(['a', 'b', 'c'])\n+    'multiclass'\n+    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n+    'multiclass-multioutput'\n+    >>> type_of_target([[1, 2]])\n+    'multilabel-indicator'\n+    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n+    'continuous-multioutput'\n+    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n+    'multilabel-indicator'\n+    \"\"\"\n+    xp, is_array_api_compliant = get_namespace(y)\n+    \n+    # Store original pandas dtype information if available\n+    original_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n+        # Check for nullable pandas dtypes\n+        if hasattr(y.dtype, 'na_value') or str(y.dtype).endswith(('Int64', 'Float64', 'boolean', 'string')):\n+            original_pandas_dtype = y.dtype\n+    \n+    if is_array_api_compliant:\n+        y = check_array(y, accept_sparse=False, ensure_2d=False, dtype=None, input_name=input_name)\n+    else:\n+        y = check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"], ensure_2d=False, dtype=None, input_name=input_name)\n+\n+    # The old sequence of sequences format\n+    try:\n+        if (\n+            not hasattr(y[0], \"__array__\")\n+            and isinstance(y[0], Sequence)\n+            and not isinstance(y[0], str)\n+        ):\n+            raise ValueError(\n+                \"You appear to be using a legacy multi-label data\"\n+                \" representation. Sequence of sequences are no\"\n+                \" longer supported; use a binary array or sparse\"\n+                \" matrix instead - the MultiLabelBinarizer\"\n+                \" transformer can convert to this format.\"\n             )\n-        )\n+    except IndexError:\n+        pass\n+\n+    # Invalid inputs\n+    if y.ndim not in (1, 2):\n+        # Number of dimension greater than 2: [[[1, 2]]]\n+        return \"unknown\"\n+\n+    # Check if multioutput\n+    if y.ndim == 2 and y.shape[1] == 0:\n+        return \"unknown\"  # [[]]\n+    \n+    multioutput = y.ndim == 2 and y.shape[1] > 1\n+\n+    if y.dtype == object:\n+        # Handle nullable pandas dtypes stored in object arrays\n+        if original_pandas_dtype is not None:\n+            # Extract non-null values to determine the actual type\n+            if hasattr(y, 'dropna'):\n+                non_null_values = y.dropna()\n+            else:\n+                # For numpy object arrays containing pandas data\n+                non_null_mask = pd.notna(y) if 'pd' in globals() else ~pd.isna(y)\n+                if non_null_mask.any():\n+                    non_null_values = y[non_null_mask]\n+                    # Try to infer the actual dtype from non-null values\n+                    if len(non_null_values) > 0:\n+                        sample_val = non_null_values[0] if hasattr(non_null_values, '__getitem__') else next(iter(non_null_values))\n+                        if isinstance(sample_val, (int, np.integer)):\n+                            y = y.astype(float)  # Convert to float to handle NaN\n+                        elif isinstance(sample_val, (float, np.floating)):\n+                            y = y.astype(float)\n+                        elif isinstance(sample_val, (bool, np.bool_)):\n+                            y = y.astype(float)  # Convert to float to handle NaN\n+        else:\n+            # Original object dtype handling\n+            return \"unknown\"\n+\n+    # The targets are of a supported type, proceed to identify the exact target type\n+    if multioutput:\n+        # Ensure y is in a format where we can extract unique values per column\n+        if issparse(y):\n+            if y.format not in [\"csr\", \"csc\"]:\n+                y = y.tocsr()\n+        \n+        suffix = \"-multioutput\"\n+        for column in range(y.shape[1]):\n+            if issparse(y):\n+                column_y = y[:, column].toarray().ravel()\n+            else:\n+                column_y = y[:, column]\n+            \n+            unique_values = xp.unique_values(column_y)\n+            \n+            if len(unique_values) > 2:\n+                return \"multiclass\" + suffix  # [1, 2, 3] or ['a', 'b', 'c']\n+            elif len(unique_values) == 2:\n+                if not _is_integral_float(unique_values):\n+                    return \"continuous\" + suffix  # [1.1, 2.3] or float NaN\n+        \n+        # All columns are either binary or continuous with only 0s and 1s\n+        if suffix == \"-multioutput\":\n+            return \"multilabel-indicator\"  # [[1, 0], [0, 1]]\n+        else:\n+            return \"binary\" + suffix  # [[1, 1], [0, 0]]\n+    else:\n+        # Single output\n+        unique_values = xp.unique_values(y)\n+        \n+        if len(unique_values) <= 2:\n+            return \"binary\"  # [1, 0] or ['a', 'b']\n+        elif _is_integral_float(unique_values):\n+            return \"multiclass\"  # [1, 2, 3] or [1., 2., 3.]\n+        else:\n+            return \"continuous\"  # [1.1, 2.3]\n         > 1\n     ):\n         raise ValueError(\n@@ -279,7 +448,16 @@ def type_of_target(y, input_name=\"\"):\n     >>> type_of_target(np.array([[0, 1], [1, 1]]))\n     'multilabel-indicator'\n     \"\"\"\n+\n     xp, is_array_api = get_namespace(y)\n+    \n+    # Store original pandas dtype information for nullable dtypes\n+    original_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n+        # Check for nullable pandas dtypes\n+        if y.dtype.name in ['Int64', 'Int32', 'Int16', 'Int8', 'Float64', 'Float32', 'boolean', 'string']:\n+            original_pandas_dtype = y.dtype.name\n+    \n     valid = (\n         (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n         and not isinstance(y, str)\n@@ -307,10 +485,15 @@ def type_of_target(y, input_name=\"\"):\n         if not issparse(y):\n             try:\n                 y = xp.asarray(y)\n+                # Preserve pandas dtype information as an attribute\n+                if original_pandas_dtype is not None:\n+                    y._original_pandas_dtype = original_pandas_dtype\n             except (np.VisibleDeprecationWarning, ValueError):\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n                 y = xp.asarray(y, dtype=object)\n+                if original_pandas_dtype is not None:\n+                    y._original_pandas_dtype = original_pandas_dtype\n \n     # The old sequence of sequences format\n     try:\n@@ -340,9 +523,25 @@ def type_of_target(y, input_name=\"\"):\n             return \"binary\"  # []\n         # 2-D empty array: [[]]\n         return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n+\n+    if not issparse(y) and y.dtype == object:\n+        # Handle nullable pandas dtypes that get converted to object arrays\n+        if hasattr(y, '_pandas_dtype_info'):\n+            # Use preserved pandas dtype information\n+            original_dtype = y._pandas_dtype_info\n+            if hasattr(original_dtype, 'numpy_dtype'):\n+                # This is a nullable pandas dtype, check the underlying type\n+                underlying_dtype = original_dtype.numpy_dtype\n+                if underlying_dtype.kind in ('i', 'u', 'f'):\n+                    # Nullable integer or float dtype, continue with normal classification\n+                    pass\n+                else:\n+                    return \"unknown\"\n+            else:\n+                return \"unknown\"\n+        elif len(y) > 0 and not isinstance(y.flat[0], str):\n+            # [obj_1] and not [\"label_1\"]\n+            return \"unknown\"\n \n     # Check if multioutput\n     if y.ndim == 2 and y.shape[1] > 1:\n@@ -351,16 +550,24 @@ def type_of_target(y, input_name=\"\"):\n         suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n \n     # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n \n     # Check multiclass\n     first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n+    \n+    # Handle nullable pandas dtypes for unique value counting\n+    if hasattr(y, 'dtype') and y.dtype == object:\n+        # Check if this is a converted nullable pandas dtype\n+        sample_vals = y.flat[:min(100, y.size)]  # Sample for efficiency\n+        non_null_vals = [v for v in sample_vals if v is not None and not (hasattr(v, '__class__') and 'pandas' in str(v.__class__) and hasattr(v, 'isna') and v.isna())]\n+        if non_null_vals:\n+            # Use non-null values for unique counting\n+            unique_count = len(set(non_null_vals))\n+        else:\n+            unique_count = xp.unique_values(y).shape[0]\n+    else:\n+        unique_count = xp.unique_values(y).shape[0]\n+    \n+    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n         return \"multiclass\" + suffix\n     else:\n",
    "variant_7": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..3195b21f2 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -91,9 +91,11 @@ def unique_labels(*ys):\n     if (\n         label_type == \"multilabel-indicator\"\n         and len(\n-            set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n-            )\n+\n+        > 1\n+    ):\n+        raise ValueError(\n+            \"Multi-label binary indicator input with different numbers of labels\"\n         )\n         > 1\n     ):\n@@ -279,7 +281,16 @@ def type_of_target(y, input_name=\"\"):\n     >>> type_of_target(np.array([[0, 1], [1, 1]]))\n     'multilabel-indicator'\n     \"\"\"\n+\n     xp, is_array_api = get_namespace(y)\n+    \n+    # Check for nullable pandas dtypes before validation\n+    nullable_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n+        dtype_name = y.dtype.name\n+        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'Float64', 'Float32', 'boolean', 'string']:\n+            nullable_pandas_dtype = dtype_name\n+    \n     valid = (\n         (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n         and not isinstance(y, str)\n@@ -307,10 +318,15 @@ def type_of_target(y, input_name=\"\"):\n         if not issparse(y):\n             try:\n                 y = xp.asarray(y)\n+                # Store nullable pandas dtype info as attribute if conversion happened\n+                if nullable_pandas_dtype is not None:\n+                    y._nullable_pandas_dtype = nullable_pandas_dtype\n             except (np.VisibleDeprecationWarning, ValueError):\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n                 y = xp.asarray(y, dtype=object)\n+                if nullable_pandas_dtype is not None:\n+                    y._nullable_pandas_dtype = nullable_pandas_dtype\n \n     # The old sequence of sequences format\n     try:\n@@ -340,9 +356,38 @@ def type_of_target(y, input_name=\"\"):\n             return \"binary\"  # []\n         # 2-D empty array: [[]]\n         return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n+\n+    if not issparse(y) and y.dtype == object:\n+        # Check if this is a pandas nullable dtype converted to object array\n+        if hasattr(y, '_pandas_nullable_dtype'):\n+            # Use the preserved pandas dtype info for classification\n+            pandas_dtype = y._pandas_nullable_dtype\n+            if hasattr(pandas_dtype, 'numpy_dtype'):\n+                # For nullable dtypes, use the underlying numpy dtype\n+                effective_dtype = pandas_dtype.numpy_dtype\n+            else:\n+                # Fallback: infer from non-null values\n+                non_null_mask = y != None  # Using != None to handle pandas NA\n+                if non_null_mask.any():\n+                    non_null_values = y[non_null_mask]\n+                    if len(non_null_values) > 0:\n+                        first_val = non_null_values[0]\n+                        if isinstance(first_val, (int, np.integer)):\n+                            effective_dtype = np.dtype('int64')\n+                        elif isinstance(first_val, (float, np.floating)):\n+                            effective_dtype = np.dtype('float64')\n+                        elif isinstance(first_val, str):\n+                            effective_dtype = np.dtype('object')\n+                        else:\n+                            return \"unknown\"\n+                    else:\n+                        return \"unknown\"\n+                else:\n+                    return \"unknown\"\n+            # Skip the string check for nullable dtypes since we know the underlying type\n+        elif not isinstance(y.flat[0], str):\n+            # [obj_1] and not [\"label_1\"]\n+            return \"unknown\"\n \n     # Check if multioutput\n     if y.ndim == 2 and y.shape[1] > 1:\n@@ -351,16 +396,20 @@ def type_of_target(y, input_name=\"\"):\n         suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n \n     # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n \n     # Check multiclass\n     first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n+    \n+    # For object arrays that might contain nullable pandas data, use the original data for unique counting\n+    unique_data = y\n+    if hasattr(y, 'dtype') and y.dtype == object and hasattr(y, '_original_pandas_dtype'):\n+        # Use original pandas series for accurate unique value counting\n+        unique_data = y._original_pandas_series\n+        unique_count = len(unique_data.dropna().unique())\n+    else:\n+        unique_count = xp.unique_values(y).shape[0]\n+    \n+    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n         return \"multiclass\" + suffix\n     else:\n",
    "variant_8": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..5155b00e1 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -91,10 +91,201 @@ def unique_labels(*ys):\n     if (\n         label_type == \"multilabel-indicator\"\n         and len(\n-            set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n-            )\n-        )\n+\n+def _check_partial_fit_first_call(clf, classes=None):\n+    \"\"\"Private helper function for factorizing common classes param logic.\n+\n+    Estimators that implement the ``partial_fit`` API should call this\n+    function on the first call to ``partial_fit``.\n+    State validation must be performed prior to calling this function.\n+\n+    Parameters\n+    ----------\n+    clf : estimator\n+        The estimator to which partial_fit is being applied.\n+\n+    classes : array-like of shape (n_classes,), default=None\n+        The unique classes.\n+    \"\"\"\n+    if getattr(clf, \"classes_\", None) is None and classes is None:\n+        raise ValueError(\"classes must be passed on the first call \"\n+                         \"to partial_fit.\")\n+\n+    elif classes is not None:\n+        if getattr(clf, \"classes_\", None) is not None:\n+            if not np.array_equal(clf.classes_, unique_labels(classes)):\n+                raise ValueError(\"`classes=%r` is not the same as on last call \"\n+                                 \"to partial_fit, was: %r\" % (classes,\n+                                                              clf.classes_))\n+\n+        else:\n+            # This is the first call to partial_fit\n+            clf.classes_ = unique_labels(classes)\n+    return clf\n+\n+\n+def class_distribution(y, sample_weight=None):\n+    \"\"\"Compute class priors from a target array.\n+\n+    Parameters\n+    ----------\n+    y : array-like of shape (n_samples,)\n+        The labels.\n+\n+    sample_weight : array-like of shape (n_samples,), default=None\n+        Sample weights.\n+\n+    Returns\n+    -------\n+    classes : ndarray of shape (n_classes,)\n+        The unique classes\n+\n+    n_classes : int\n+        The number of unique classes\n+\n+    class_prior : ndarray of shape (n_classes,)\n+        The class frequencies\n+    \"\"\"\n+    classes, y_indices = np.unique(y, return_inverse=True)\n+    n_classes = classes.shape[0]\n+\n+    if sample_weight is not None:\n+        class_prior = np.bincount(y_indices, weights=sample_weight)\n+    else:\n+        class_prior = np.bincount(y_indices)\n+\n+    class_prior = class_prior / class_prior.sum()\n+    return classes, n_classes, class_prior\n+\n+\n+def _ovr_decision_function(predictions, confidences, n_classes):\n+    \"\"\"Compute a continuous, tie-breaking OvR decision function from OvO.\n+\n+    It is important to include a continuous value, not only the predicted\n+    class integer, to make computing AUC or calibration meaningful.\n+\n+    Parameters\n+    ----------\n+    predictions : array-like of shape (n_samples, n_classifiers)\n+        Predicted classes for each binary classifier.\n+\n+    confidences : array-like of shape (n_samples, n_classifiers)\n+        Decision functions or predicted probabilities for positive class\n+        for each binary classifier.\n+\n+    n_classes : int\n+        Number of classes. n_classifiers must be\n+        ``n_classes * (n_classes - 1 ) / 2``.\n+\n+    Returns\n+    -------\n+    decision : array-like of shape (n_samples, n_classes)\n+        The continuous OvR decision function.\n+    \"\"\"\n+    n_samples = predictions.shape[0]\n+    votes = np.zeros((n_samples, n_classes))\n+    sum_of_confidences = np.zeros((n_samples, n_classes))\n+\n+    k = 0\n+    for i in range(n_classes):\n+        for j in range(i + 1, n_classes):\n+            sum_of_confidences[:, i] -= confidences[:, k]\n+            sum_of_confidences[:, j] += confidences[:, k]\n+            votes[predictions[:, k] == i, i] += 1\n+            votes[predictions[:, k] == j, j] += 1\n+            k += 1\n+\n+    # Monotonically transform the sum_of_confidences to (-1, 1) and add it to\n+    # the votes. The monotonic transformation  is a fast and simple way to\n+    # guarantee the consecutive classes are separated by at least 1\n+    # (which is needed to ensure convergence in the binary search of\n+    # DecisionBoundaryDisplay.from_estimator).\n+    max_confidences = np.max(np.abs(sum_of_confidences), axis=1, keepdims=True)\n+    max_confidences[max_confidences == 0] = 1\n+    return votes + sum_of_confidences / max_confidences\n+\n+\n+def _check_multi_class(y):\n+    \"\"\"Check if ``y`` is in a multilabel format.\n+\n+    Parameters\n+    ----------\n+    y : ndarray of shape (n_samples, n_outputs)\n+        Target values. This function does not accept sparse matrices.\n+\n+    Returns\n+    -------\n+    out : str,\n+        Return ``'multilabel-indicator'`` if ``y`` is a label indicator\n+        matrix, ``'multiclass'`` if ``y`` is an array of integers,\n+        ``'multiclass-multioutput'`` if ``y`` is a 2d array of integers,\n+        and ``'binary'`` if ``y`` contains <= 2 unique values.\n+    \"\"\"\n+    xp, _ = get_namespace(y)\n+    \n+    if hasattr(y, 'shape') and len(y.shape) > 1 and y.shape[1] > 1:\n+        suffix = \"-multioutput\"  # [[1, 2], [1, 2]]\n+        ndim = 2\n+    else:\n+        suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n+        ndim = 1\n+\n+    if y.dtype.kind == 'O':\n+        # Handle object dtype arrays that may contain nullable pandas data\n+        if hasattr(y, 'dtype') and hasattr(y.dtype, 'name') and 'pandas' in str(type(y.dtype)):\n+            # This is likely a converted nullable pandas dtype\n+            # Try to get the actual underlying type from non-null values\n+            try:\n+                non_null_mask = y != np.array(None)\n+                if np.any(non_null_mask):\n+                    sample_val = y[non_null_mask][0]\n+                    if hasattr(sample_val, 'dtype'):\n+                        actual_dtype = sample_val.dtype\n+                    else:\n+                        actual_dtype = type(sample_val)\n+                    \n+                    # Check if it's numeric\n+                    if np.issubdtype(actual_dtype, np.number):\n+                        # Count unique values for classification\n+                        unique_values = xp.unique_values(y[non_null_mask])\n+                        if len(unique_values) <= 2:\n+                            return \"binary\" + suffix\n+                        else:\n+                            return \"multiclass\" + suffix\n+            except (IndexError, TypeError, AttributeError):\n+                pass\n+        \n+        # Fallback for other object arrays\n+        return \"unknown\"\n+\n+    # Check if the array only contains 0 and 1.\n+    if xp.isdtype(y.dtype, (\"integral\", \"bool\")):\n+        unique_values = xp.unique_values(y)\n+        if len(unique_values) <= 2:\n+            return \"binary\" + suffix  # [0, 1, 1, 0]\n+        else:\n+            return \"multiclass\" + suffix  # [0, 1, 2, 2, 1]\n+\n+    if xp.isdtype(y.dtype, \"real floating\"):\n+        if ndim == 2:\n+            # This is a 2d array of floats, check if it's a label indicator matrix\n+            if _is_integral_float(y) and xp.all((y == 0) | (y == 1)):\n+                return \"multilabel-indicator\"  # [[1.0, 0.0], [0.0, 1.0]]\n+            else:\n+                # 2d array of floats that are not 0 and 1\n+                return \"continuous\" + suffix  # [[1.1, 2.0], [3.0, 4.0]]\n+        else:\n+            # 1d array of floats, check if they are integral\n+            if _is_integral_float(y):\n+                unique_values = xp.unique_values(y)\n+                if len(unique_values) <= 2:\n+                    return \"binary\" + suffix  # [1.0, 0.0, 0.0, 1.0]\n+                else:\n+                    return \"multiclass\" + suffix  # [1.0, 2.0, 3.0, 3.0, 2.0]\n+            else:\n+                return \"continuous\" + suffix  # [1.1, 2.2, 3.1, 4.4, 2.5]\n+    \n+    return \"unknown\"\n         > 1\n     ):\n         raise ValueError(\n@@ -279,7 +470,16 @@ def type_of_target(y, input_name=\"\"):\n     >>> type_of_target(np.array([[0, 1], [1, 1]]))\n     'multilabel-indicator'\n     \"\"\"\n+\n     xp, is_array_api = get_namespace(y)\n+    \n+    # Store original pandas dtype information for nullable dtypes\n+    original_pandas_dtype = None\n+    if hasattr(y, 'dtype') and hasattr(y.dtype, 'name'):\n+        dtype_name = y.dtype.name\n+        if dtype_name in ['Int64', 'Int32', 'Int16', 'Int8', 'boolean', 'string']:\n+            original_pandas_dtype = dtype_name\n+    \n     valid = (\n         (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n         and not isinstance(y, str)\n@@ -311,6 +511,18 @@ def type_of_target(y, input_name=\"\"):\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n                 y = xp.asarray(y, dtype=object)\n+    \n+    # Store the original pandas dtype info in the array for later use\n+    if original_pandas_dtype is not None and hasattr(y, '__dict__'):\n+        y.__dict__['_original_pandas_dtype'] = original_pandas_dtype\n+    elif original_pandas_dtype is not None:\n+        # For arrays that don't support __dict__, use a different approach\n+        if not hasattr(y, '_sklearn_dtype_info'):\n+            try:\n+                y._sklearn_dtype_info = {'original_pandas_dtype': original_pandas_dtype}\n+            except AttributeError:\n+                # If we can't attach the info, we'll rely on the type detection logic\n+                pass\n \n     # The old sequence of sequences format\n     try:\n@@ -340,9 +552,21 @@ def type_of_target(y, input_name=\"\"):\n             return \"binary\"  # []\n         # 2-D empty array: [[]]\n         return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n+\n+    if not issparse(y) and y.dtype == object:\n+        # Check if this is a pandas nullable dtype converted to object array\n+        if hasattr(y, '_pandas_nullable_dtype'):\n+            # Use the original pandas dtype for classification\n+            original_dtype = y._pandas_nullable_dtype\n+            if hasattr(original_dtype, 'numpy_dtype'):\n+                # For pandas nullable dtypes, use the underlying numpy dtype\n+                effective_dtype = original_dtype.numpy_dtype\n+            else:\n+                effective_dtype = original_dtype\n+            # Skip the object dtype check since we know the real type\n+        elif not isinstance(y.flat[0], str):\n+            # [obj_1] and not [\"label_1\"]\n+            return \"unknown\"\n \n     # Check if multioutput\n     if y.ndim == 2 and y.shape[1] > 1:\n@@ -351,16 +575,33 @@ def type_of_target(y, input_name=\"\"):\n         suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n \n     # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n \n     # Check multiclass\n     first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n+    \n+    # Handle object arrays that may contain nullable pandas data\n+    if y.dtype == object:\n+        # For object arrays, we need to look at the actual values to count unique elements\n+        # This handles cases where nullable pandas dtypes are converted to object arrays\n+        try:\n+            # Extract non-null values for unique counting\n+            if issparse(y):\n+                data_for_unique = y.data[y.data != None]\n+            else:\n+                flat_y = y.ravel()\n+                data_for_unique = flat_y[flat_y != None]\n+            \n+            if len(data_for_unique) > 0:\n+                unique_count = len(set(data_for_unique))\n+            else:\n+                unique_count = 0\n+        except (TypeError, ValueError):\n+            # Fallback to original logic if extraction fails\n+            unique_count = xp.unique_values(y).shape[0]\n+    else:\n+        unique_count = xp.unique_values(y).shape[0]\n+    \n+    if unique_count > 2 or (y.ndim == 2 and len(first_row) > 1):\n         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n         return \"multiclass\" + suffix\n     else:\n"
  }
}