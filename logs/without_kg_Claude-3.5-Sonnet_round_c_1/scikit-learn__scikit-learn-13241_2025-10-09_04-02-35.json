{
  "zero_patches": {
    "Location 1 (/root/hy/projects_1/scikit-learn/sklearn/decomposition/kernel_pca.py lines 200-216)": "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n        \n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # find the element with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # make that element positive\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
    "Location 2 (/root/hy/projects_1/scikit-learn/sklearn/decomposition/kernel_pca.py lines 276-279)": "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the first non-zero element\n            non_zero_idx = np.nonzero(self.alphas_[:, i])[0]\n            if len(non_zero_idx) > 0:\n                first_non_zero = self.alphas_[non_zero_idx[0], i]\n                if first_non_zero < 0:\n                    self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with zero eigenvalues\n        self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n        self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
    "Location 3 (/root/hy/projects_1/scikit-learn/sklearn/decomposition/kernel_pca.py lines 296-299)": "\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and self.n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            # initialize with [-1,1] as in ARPACK\n            random_state = check_random_state(self.random_state)\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                               which=\"LA\",\n                                               tol=self.tol,\n                                               maxiter=self.max_iter,\n                                               v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the element with the largest absolute value\n            max_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If it's negative, flip the entire eigenvector\n            if self.alphas_[max_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_1/scikit-learn/sklearn/decomposition/kernel_pca.py lines 200-216)": [
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # find the index of the element with maximum absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # if this element is negative, flip the sign of the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n        \n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # find the element with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # if it's negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n        \n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # find the index of the element with maximum absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # if this element is negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n        \n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # find the index of the element with maximum absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # if this element is negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n        \n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the element with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If that element is negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n        \n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # find the element with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # flip the sign if the largest element is negative\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n        \n        # enforce consistent eigenvector orientation\n        for i in range(self.alphas_.shape[1]):\n            # find the index of the element with maximum absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # flip the eigenvector if the max absolute element is negative\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n        \n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # find the index of the element with largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # ensure this element is positive\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n"
    ],
    "Location 2 (/root/hy/projects_1/scikit-learn/sklearn/decomposition/kernel_pca.py lines 276-279)": [
      "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the first non-zero element and make it positive\n            eigenvector = self.alphas_[:, i]\n            first_nonzero_idx = np.argmax(np.abs(eigenvector))\n            if eigenvector[first_nonzero_idx] < 0:\n                self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the first non-zero element and make it positive\n            eigenvector = self.alphas_[:, i]\n            first_nonzero_idx = np.argmax(np.abs(eigenvector))\n            if eigenvector[first_nonzero_idx] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        # Sort eigenvectors by eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the first non-zero element and make it positive\n            eigenvector = self.alphas_[:, i]\n            first_nonzero_idx = np.nonzero(eigenvector)[0]\n            if len(first_nonzero_idx) > 0 and eigenvector[first_nonzero_idx[0]] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        # Make the element with largest absolute value positive for each eigenvector\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        # Sort eigenvectors by eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the first non-zero element and make it positive\n            eigenvector = self.alphas_[:, i]\n            first_nonzero_idx = np.where(np.abs(eigenvector) > 1e-12)[0]\n            if len(first_nonzero_idx) > 0:\n                if eigenvector[first_nonzero_idx[0]] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.n_components is not None:\n            self.alphas_ = self.alphas_[:, :self.n_components]\n            self.lambdas_ = self.lambdas_[:self.n_components]\n",
      "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the first non-zero element and make it positive\n            eigenvector = self.alphas_[:, i]\n            first_nonzero_idx = np.argmax(np.abs(eigenvector))\n            if eigenvector[first_nonzero_idx] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        # Sort eigenvectors and eigenvalues by decreasing eigenvalues\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the index of the element with largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # Ensure the element with largest absolute value is positive\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        # Sort eigenvectors by eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the index of the element with largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If that element is negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n"
    ],
    "Location 3 (/root/hy/projects_1/scikit-learn/sklearn/decomposition/kernel_pca.py lines 296-299)": [
      "\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and self.n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            # initialize with [-1,1] as in ARPACK\n            random_state = check_random_state(self.random_state)\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                               which=\"LA\",\n                                               tol=self.tol,\n                                               maxiter=self.max_iter,\n                                               v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n        \n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the element with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If this element is negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if remove_zero_eig:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and self.n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            # initialize with [-1,1] as in ARPACK\n            random_state = check_random_state(self.random_state)\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                               which=\"LA\",\n                                               tol=self.tol,\n                                               maxiter=self.max_iter,\n                                               v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Enforce consistent sign convention for eigenvectors\n        # Make the element with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        # Sort eigenvectors and eigenvalues by eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        # Make the element with largest absolute value positive for each eigenvector\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # Flip eigenvectors' sign to enforce deterministic output\n        max_abs_cols = np.argmax(np.abs(self.alphas_), axis=0)\n        signs = np.sign(self.alphas_[max_abs_cols, range(self.alphas_.shape[1])])\n        self.alphas_ *= signs\n\n        if self.fit_inverse_transform:\n            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))\n            self.dual_coef_ = np.dot(self.alphas_, sqrt_lambdas)\n\n        return self\n",
      "\n        # Sort eigenvectors by eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the first non-zero element and make it positive\n            eigenvector = self.alphas_[:, i]\n            first_nonzero_idx = np.argmax(np.abs(eigenvector))\n            if eigenvector[first_nonzero_idx] < 0:\n                self.alphas_[:, i] = -eigenvector\n",
      "\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and self.n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(K, eigvals=(\n                K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            # initialize with [-1,1] as in ARPACK\n            random_state = check_random_state(self.random_state)\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                               which=\"LA\",\n                                               tol=self.tol,\n                                               maxiter=self.max_iter,\n                                               v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n        \n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the element with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If this element is negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] = -self.alphas_[:, i]\n",
      "\n        # Flip eigenvectors' sign to enforce deterministic output\n        max_abs_rows = np.argmax(np.abs(self.alphas_), axis=0)\n        signs = np.sign(self.alphas_[max_abs_rows, range(self.alphas_.shape[1])])\n        self.alphas_ *= signs\n\n        if self.fit_inverse_transform:\n            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))\n            self.dual_coef_ = np.dot(self.alphas_, sqrt_lambdas)\n\n        return self.alphas_ * np.sqrt(self.lambdas_)\n",
      "\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and self.n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            # arpack solver only supports eigenvalues in ascending order\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                               which=\"LA\",\n                                               tol=self.tol,\n                                               maxiter=self.max_iter)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Enforce consistent sign convention for eigenvectors\n        # Make the first non-zero element of each eigenvector positive\n        for i in range(self.alphas_.shape[1]):\n            # Find first non-zero element\n            nonzero_idx = np.flatnonzero(self.alphas_[:, i])\n            if len(nonzero_idx) > 0:\n                first_nonzero = nonzero_idx[0]\n                if self.alphas_[first_nonzero, i] < 0:\n                    self.alphas_[:, i] *= -1\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..e21c26376 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -214,6 +215,13 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n+        \n+        # Ensure consistent eigenvector signs to avoid sign inconsistency\n+        # Make the element with the largest absolute value positive for each eigenvector\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n@@ -273,10 +281,10 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         self.fit(X, **params)\n \n         X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n-\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -293,6 +301,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "variant_0": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..043022ecd 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -214,6 +215,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n+        \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # find the element with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # make that element positive\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n@@ -273,10 +282,24 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the first non-zero element\n+            non_zero_idx = np.nonzero(self.alphas_[:, i])[0]\n+            if len(non_zero_idx) > 0:\n+                first_non_zero = self.alphas_[non_zero_idx[0], i]\n+                if first_non_zero < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # Remove eigenvectors with zero eigenvalues\n+        self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+        self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -293,10 +316,40 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        if self.eigen_solver == 'auto':\n+            if K.shape[0] > 200 and self.n_components < 10:\n+                eigen_solver = 'arpack'\n+            else:\n+                eigen_solver = 'dense'\n+        else:\n+            eigen_solver = self.eigen_solver\n+\n+        if eigen_solver == 'dense':\n+            self.lambdas_, self.alphas_ = linalg.eigh(\n+                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n+        elif eigen_solver == 'arpack':\n+            # initialize with [-1,1] as in ARPACK\n+            random_state = check_random_state(self.random_state)\n+            v0 = random_state.uniform(-1, 1, K.shape[0])\n+            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n+                                               which=\"LA\",\n+                                               tol=self.tol,\n+                                               maxiter=self.max_iter,\n+                                               v0=v0)\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the element with the largest absolute value\n+            max_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If it's negative, flip the entire eigenvector\n+            if self.alphas_[max_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_1": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..44e7aa94a 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # find the index of the element with maximum absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # if this element is negative, flip the sign of the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -273,10 +282,24 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the first non-zero element and make it positive\n+            eigenvector = self.alphas_[:, i]\n+            first_nonzero_idx = np.argmax(np.abs(eigenvector))\n+            if eigenvector[first_nonzero_idx] < 0:\n+                self.alphas_[:, i] *= -1\n \n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -293,10 +316,45 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        if self.eigen_solver == 'auto':\n+            if K.shape[0] > 200 and self.n_components < 10:\n+                eigen_solver = 'arpack'\n+            else:\n+                eigen_solver = 'dense'\n+        else:\n+            eigen_solver = self.eigen_solver\n+\n+        if eigen_solver == 'dense':\n+            self.lambdas_, self.alphas_ = linalg.eigh(\n+                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n+        elif eigen_solver == 'arpack':\n+            # initialize with [-1,1] as in ARPACK\n+            random_state = check_random_state(self.random_state)\n+            v0 = random_state.uniform(-1, 1, K.shape[0])\n+            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n+                                               which=\"LA\",\n+                                               tol=self.tol,\n+                                               maxiter=self.max_iter,\n+                                               v0=v0)\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+        \n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the element with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If this element is negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if remove_zero_eig:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_2": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..962617b21 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -214,6 +215,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n+        \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # find the element with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # if it's negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n@@ -273,10 +282,19 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the first non-zero element and make it positive\n+            eigenvector = self.alphas_[:, i]\n+            first_nonzero_idx = np.argmax(np.abs(eigenvector))\n+            if eigenvector[first_nonzero_idx] < 0:\n+                self.alphas_[:, i] *= -1\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -293,10 +311,39 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        if self.eigen_solver == 'auto':\n+            if K.shape[0] > 200 and self.n_components < 10:\n+                eigen_solver = 'arpack'\n+            else:\n+                eigen_solver = 'dense'\n+        else:\n+            eigen_solver = self.eigen_solver\n+\n+        if eigen_solver == 'dense':\n+            self.lambdas_, self.alphas_ = linalg.eigh(\n+                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n+        elif eigen_solver == 'arpack':\n+            # initialize with [-1,1] as in ARPACK\n+            random_state = check_random_state(self.random_state)\n+            v0 = random_state.uniform(-1, 1, K.shape[0])\n+            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n+                                               which=\"LA\",\n+                                               tol=self.tol,\n+                                               maxiter=self.max_iter,\n+                                               v0=v0)\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Enforce consistent sign convention for eigenvectors\n+        # Make the element with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_3": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..5e75d27c3 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -214,6 +215,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n+        \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # find the index of the element with maximum absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # if this element is negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n@@ -273,10 +282,19 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors by eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the first non-zero element and make it positive\n+            eigenvector = self.alphas_[:, i]\n+            first_nonzero_idx = np.nonzero(eigenvector)[0]\n+            if len(first_nonzero_idx) > 0 and eigenvector[first_nonzero_idx[0]] < 0:\n+                self.alphas_[:, i] *= -1\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -293,10 +311,23 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Sort eigenvectors and eigenvalues by eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Standardize eigenvector signs for consistency\n+        # Make the element with largest absolute value positive for each eigenvector\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_4": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..091430189 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -214,6 +215,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n+        \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # find the index of the element with maximum absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # if this element is negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n@@ -273,10 +282,18 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n+        # Standardize eigenvector signs for consistency\n+        # Make the element with largest absolute value positive for each eigenvector\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -293,10 +310,17 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Flip eigenvectors' sign to enforce deterministic output\n+        max_abs_cols = np.argmax(np.abs(self.alphas_), axis=0)\n+        signs = np.sign(self.alphas_[max_abs_cols, range(self.alphas_.shape[1])])\n+        self.alphas_ *= signs\n+\n+        if self.fit_inverse_transform:\n+            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))\n+            self.dual_coef_ = np.dot(self.alphas_, sqrt_lambdas)\n+\n+        return self\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_5": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..b26ab23c2 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -214,6 +215,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n+        \n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the element with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If that element is negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n@@ -273,10 +282,25 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors by eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the first non-zero element and make it positive\n+            eigenvector = self.alphas_[:, i]\n+            first_nonzero_idx = np.where(np.abs(eigenvector) > 1e-12)[0]\n+            if len(first_nonzero_idx) > 0:\n+                if eigenvector[first_nonzero_idx[0]] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, :self.n_components]\n+            self.lambdas_ = self.lambdas_[:self.n_components]\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -293,10 +317,19 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Sort eigenvectors by eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the first non-zero element and make it positive\n+            eigenvector = self.alphas_[:, i]\n+            first_nonzero_idx = np.argmax(np.abs(eigenvector))\n+            if eigenvector[first_nonzero_idx] < 0:\n+                self.alphas_[:, i] = -eigenvector\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_6": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..7f0fcd592 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -214,6 +215,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n+        \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # find the element with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # flip the sign if the largest element is negative\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n@@ -273,10 +282,19 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the first non-zero element and make it positive\n+            eigenvector = self.alphas_[:, i]\n+            first_nonzero_idx = np.argmax(np.abs(eigenvector))\n+            if eigenvector[first_nonzero_idx] < 0:\n+                self.alphas_[:, i] *= -1\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -293,10 +311,40 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        if self.eigen_solver == 'auto':\n+            if K.shape[0] > 200 and self.n_components < 10:\n+                eigen_solver = 'arpack'\n+            else:\n+                eigen_solver = 'dense'\n+        else:\n+            eigen_solver = self.eigen_solver\n+\n+        if eigen_solver == 'dense':\n+            self.lambdas_, self.alphas_ = linalg.eigh(K, eigvals=(\n+                K.shape[0] - n_components, K.shape[0] - 1))\n+        elif eigen_solver == 'arpack':\n+            # initialize with [-1,1] as in ARPACK\n+            random_state = check_random_state(self.random_state)\n+            v0 = random_state.uniform(-1, 1, K.shape[0])\n+            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n+                                               which=\"LA\",\n+                                               tol=self.tol,\n+                                               maxiter=self.max_iter,\n+                                               v0=v0)\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+        \n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the element with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If this element is negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] = -self.alphas_[:, i]\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_7": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..94e2777b2 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -214,6 +215,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n+        \n+        # enforce consistent eigenvector orientation\n+        for i in range(self.alphas_.shape[1]):\n+            # find the index of the element with maximum absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # flip the eigenvector if the max absolute element is negative\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n@@ -273,10 +282,19 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues by decreasing eigenvalues\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the index of the element with largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # Ensure the element with largest absolute value is positive\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -293,10 +311,17 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Flip eigenvectors' sign to enforce deterministic output\n+        max_abs_rows = np.argmax(np.abs(self.alphas_), axis=0)\n+        signs = np.sign(self.alphas_[max_abs_rows, range(self.alphas_.shape[1])])\n+        self.alphas_ *= signs\n+\n+        if self.fit_inverse_transform:\n+            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))\n+            self.dual_coef_ = np.dot(self.alphas_, sqrt_lambdas)\n+\n+        return self.alphas_ * np.sqrt(self.lambdas_)\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_8": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..58d54c029 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -214,6 +215,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n+        \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # find the index of the element with largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # ensure this element is positive\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n@@ -273,10 +282,19 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors by eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the index of the element with largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If that element is negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -293,10 +311,39 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        if self.eigen_solver == 'auto':\n+            if K.shape[0] > 200 and self.n_components < 10:\n+                eigen_solver = 'arpack'\n+            else:\n+                eigen_solver = 'dense'\n+        else:\n+            eigen_solver = self.eigen_solver\n+\n+        if eigen_solver == 'dense':\n+            self.lambdas_, self.alphas_ = linalg.eigh(\n+                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n+        elif eigen_solver == 'arpack':\n+            # arpack solver only supports eigenvalues in ascending order\n+            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n+                                               which=\"LA\",\n+                                               tol=self.tol,\n+                                               maxiter=self.max_iter)\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Enforce consistent sign convention for eigenvectors\n+        # Make the first non-zero element of each eigenvector positive\n+        for i in range(self.alphas_.shape[1]):\n+            # Find first non-zero element\n+            nonzero_idx = np.flatnonzero(self.alphas_[:, i])\n+            if len(nonzero_idx) > 0:\n+                first_nonzero = nonzero_idx[0]\n+                if self.alphas_[first_nonzero, i] < 0:\n+                    self.alphas_[:, i] *= -1\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n"
  }
}