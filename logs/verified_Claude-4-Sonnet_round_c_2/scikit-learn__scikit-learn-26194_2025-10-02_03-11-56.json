{
  "zero_patches": {
    "Location 1 (/root/hy/projects/scikit-learn/sklearn/metrics/_ranking.py lines 1082-1086)": "\n    # Add an extra threshold position\n    # to make sure that the curve starts at (0, 0)\n    tps = np.r_[0, tps]\n    fps = np.r_[0, fps]\n    \n    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n    # then we're likely dealing with probability estimates and should cap at 1\n    max_threshold = thresholds[0] + 1\n    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n        max_threshold = 1.0\n    thresholds = np.r_[max_threshold, thresholds]\n",
    "Location 2 (/root/hy/projects/scikit-learn/sklearn/metrics/tests/test_ranking.py lines 421-421)": "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n",
    "Location 3 (/root/hy/projects/scikit-learn/sklearn/metrics/tests/test_ranking.py lines 427-427)": "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n",
    "Location 4 (/root/hy/projects/scikit-learn/sklearn/metrics/tests/test_ranking.py lines 428-428)": "\n\ndef test_roc_curve_with_probablity_estimates():\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)\n    _, _, thresholds = roc_curve(y_true, y_score)\n    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects/scikit-learn/sklearn/metrics/_ranking.py lines 1082-1086)": [
      "\n    # Add an extra threshold position\n    # to make sure that the curve starts at (0, 0)\n    tps = np.r_[0, tps]\n    fps = np.r_[0, fps]\n    \n    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n    # then we're likely dealing with probability estimates and should cap at 1\n    max_threshold = thresholds[0] + 1\n    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n        max_threshold = 1.0\n    thresholds = np.r_[max_threshold, thresholds]\n",
      "\n    # Add an extra threshold position\n    # to make sure that the curve starts at (0, 0)\n    tps = np.r_[0, tps]\n    fps = np.r_[0, fps]\n    \n    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n    # then we're likely dealing with probability estimates and should cap at 1\n    max_threshold = thresholds[0] + 1\n    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n        max_threshold = 1.0\n    thresholds = np.r_[max_threshold, thresholds]\n",
      "\n    # Add an extra threshold position\n    # to make sure that the curve starts at (0, 0)\n    tps = np.r_[0, tps]\n    fps = np.r_[0, fps]\n    \n    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n    # then we're likely dealing with probability estimates and should cap at 1\n    max_threshold = thresholds[0] + 1\n    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n        max_threshold = 1.0\n    thresholds = np.r_[max_threshold, thresholds]\n",
      "\n    # Add an extra threshold position\n    # to make sure that the curve starts at (0, 0)\n    tps = np.r_[0, tps]\n    fps = np.r_[0, fps]\n    \n    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n    # then we're likely dealing with probability estimates and should cap at 1\n    max_threshold = thresholds[0] + 1\n    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n        max_threshold = 1.0\n    thresholds = np.r_[max_threshold, thresholds]\n",
      "\n    # Add an extra threshold position\n    # to make sure that the curve starts at (0, 0)\n    tps = np.r_[0, tps]\n    fps = np.r_[0, fps]\n    \n    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n    # then we're likely dealing with probability estimates and should cap at 1\n    max_threshold = thresholds[0] + 1\n    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n        max_threshold = 1.0\n    thresholds = np.r_[max_threshold, thresholds]\n",
      "\n    # Add an extra threshold position\n    # to make sure that the curve starts at (0, 0)\n    tps = np.r_[0, tps]\n    fps = np.r_[0, fps]\n    \n    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n    # then we're likely dealing with probability estimates and should cap at 1\n    max_threshold = thresholds[0] + 1\n    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n        max_threshold = 1.0\n    thresholds = np.r_[max_threshold, thresholds]\n",
      "\n    # Add an extra threshold position\n    # to make sure that the curve starts at (0, 0)\n    tps = np.r_[0, tps]\n    fps = np.r_[0, fps]\n    \n    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n    # then we're likely dealing with probability estimates and should cap at 1\n    max_threshold = thresholds[0] + 1\n    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n        max_threshold = 1.0\n    thresholds = np.r_[max_threshold, thresholds]\n",
      "\n    # Add an extra threshold position\n    # to make sure that the curve starts at (0, 0)\n    tps = np.r_[0, tps]\n    fps = np.r_[0, fps]\n    \n    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n    # then we're likely dealing with probability estimates and should cap at 1\n    max_threshold = thresholds[0] + 1\n    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n        max_threshold = 1.0\n    thresholds = np.r_[max_threshold, thresholds]\n"
    ],
    "Location 2 (/root/hy/projects/scikit-learn/sklearn/metrics/tests/test_ranking.py lines 421-421)": [
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n"
    ],
    "Location 3 (/root/hy/projects/scikit-learn/sklearn/metrics/tests/test_ranking.py lines 427-427)": [
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n",
      "\n    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n"
    ],
    "Location 4 (/root/hy/projects/scikit-learn/sklearn/metrics/tests/test_ranking.py lines 428-428)": [
      "\n\ndef test_roc_curve_with_probablity_estimates():\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)\n    _, _, thresholds = roc_curve(y_true, y_score)\n    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n",
      "\n\ndef test_roc_curve_with_probablity_estimates():\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)\n    _, _, thresholds = roc_curve(y_true, y_score)\n    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n",
      "\n\ndef test_roc_curve_with_probablity_estimates():\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)\n    _, _, thresholds = roc_curve(y_true, y_score)\n    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n",
      "\n\ndef test_roc_curve_with_probablity_estimates():\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)\n    _, _, thresholds = roc_curve(y_true, y_score)\n    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n",
      "\n\ndef test_roc_curve_with_probablity_estimates():\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)\n    _, _, thresholds = roc_curve(y_true, y_score)\n    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n",
      "\n\ndef test_roc_curve_with_probablity_estimates():\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)\n    _, _, thresholds = roc_curve(y_true, y_score)\n    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n",
      "\n\ndef test_roc_curve_with_probablity_estimates():\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)\n    _, _, thresholds = roc_curve(y_true, y_score)\n    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n",
      "\n\ndef test_roc_curve_with_probablity_estimates():\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)\n    _, _, thresholds = roc_curve(y_true, y_score)\n    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..50c3a2a95 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1079,11 +1079,18 @@ def roc_curve(\n         tps = tps[optimal_idxs]\n         thresholds = thresholds[optimal_idxs]\n \n+\n     # Add an extra threshold position\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n+    # then we're likely dealing with probability estimates and should cap at 1\n+    max_threshold = thresholds[0] + 1\n+    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n+        max_threshold = 1.0\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..2f3905bde 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,13 +418,23 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+def test_roc_curve_with_probablity_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n+\n \n \n def test_roc_curve_fpr_tpr_increasing():\n",
    "variant_0": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..50c3a2a95 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1079,11 +1079,18 @@ def roc_curve(\n         tps = tps[optimal_idxs]\n         thresholds = thresholds[optimal_idxs]\n \n+\n     # Add an extra threshold position\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n+    # then we're likely dealing with probability estimates and should cap at 1\n+    max_threshold = thresholds[0] + 1\n+    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n+        max_threshold = 1.0\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..504314d38 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,14 +418,23 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+\n+def test_roc_curve_with_probablity_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n \n def test_roc_curve_fpr_tpr_increasing():\n     # Ensure that fpr and tpr returned by roc_curve are increasing.\n",
    "variant_1": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..50c3a2a95 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1079,11 +1079,18 @@ def roc_curve(\n         tps = tps[optimal_idxs]\n         thresholds = thresholds[optimal_idxs]\n \n+\n     # Add an extra threshold position\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n+    # then we're likely dealing with probability estimates and should cap at 1\n+    max_threshold = thresholds[0] + 1\n+    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n+        max_threshold = 1.0\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..504314d38 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,14 +418,23 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+\n+def test_roc_curve_with_probablity_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n \n def test_roc_curve_fpr_tpr_increasing():\n     # Ensure that fpr and tpr returned by roc_curve are increasing.\n",
    "variant_2": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..50c3a2a95 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1079,11 +1079,18 @@ def roc_curve(\n         tps = tps[optimal_idxs]\n         thresholds = thresholds[optimal_idxs]\n \n+\n     # Add an extra threshold position\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n+    # then we're likely dealing with probability estimates and should cap at 1\n+    max_threshold = thresholds[0] + 1\n+    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n+        max_threshold = 1.0\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..504314d38 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,14 +418,23 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+\n+def test_roc_curve_with_probablity_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n \n def test_roc_curve_fpr_tpr_increasing():\n     # Ensure that fpr and tpr returned by roc_curve are increasing.\n",
    "variant_3": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..50c3a2a95 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1079,11 +1079,18 @@ def roc_curve(\n         tps = tps[optimal_idxs]\n         thresholds = thresholds[optimal_idxs]\n \n+\n     # Add an extra threshold position\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n+    # then we're likely dealing with probability estimates and should cap at 1\n+    max_threshold = thresholds[0] + 1\n+    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n+        max_threshold = 1.0\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..504314d38 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,14 +418,23 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+\n+def test_roc_curve_with_probablity_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n \n def test_roc_curve_fpr_tpr_increasing():\n     # Ensure that fpr and tpr returned by roc_curve are increasing.\n",
    "variant_4": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..50c3a2a95 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1079,11 +1079,18 @@ def roc_curve(\n         tps = tps[optimal_idxs]\n         thresholds = thresholds[optimal_idxs]\n \n+\n     # Add an extra threshold position\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n+    # then we're likely dealing with probability estimates and should cap at 1\n+    max_threshold = thresholds[0] + 1\n+    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n+        max_threshold = 1.0\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..504314d38 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,14 +418,23 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+\n+def test_roc_curve_with_probablity_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n \n def test_roc_curve_fpr_tpr_increasing():\n     # Ensure that fpr and tpr returned by roc_curve are increasing.\n",
    "variant_5": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..50c3a2a95 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1079,11 +1079,18 @@ def roc_curve(\n         tps = tps[optimal_idxs]\n         thresholds = thresholds[optimal_idxs]\n \n+\n     # Add an extra threshold position\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n+    # then we're likely dealing with probability estimates and should cap at 1\n+    max_threshold = thresholds[0] + 1\n+    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n+        max_threshold = 1.0\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..504314d38 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,14 +418,23 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+\n+def test_roc_curve_with_probablity_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n \n def test_roc_curve_fpr_tpr_increasing():\n     # Ensure that fpr and tpr returned by roc_curve are increasing.\n",
    "variant_6": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..50c3a2a95 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1079,11 +1079,18 @@ def roc_curve(\n         tps = tps[optimal_idxs]\n         thresholds = thresholds[optimal_idxs]\n \n+\n     # Add an extra threshold position\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n+    # then we're likely dealing with probability estimates and should cap at 1\n+    max_threshold = thresholds[0] + 1\n+    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n+        max_threshold = 1.0\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..504314d38 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,14 +418,23 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+\n+def test_roc_curve_with_probablity_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n \n def test_roc_curve_fpr_tpr_increasing():\n     # Ensure that fpr and tpr returned by roc_curve are increasing.\n",
    "variant_7": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..50c3a2a95 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1079,11 +1079,18 @@ def roc_curve(\n         tps = tps[optimal_idxs]\n         thresholds = thresholds[optimal_idxs]\n \n+\n     # Add an extra threshold position\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n+    # then we're likely dealing with probability estimates and should cap at 1\n+    max_threshold = thresholds[0] + 1\n+    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n+        max_threshold = 1.0\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..504314d38 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,14 +418,23 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+\n+def test_roc_curve_with_probablity_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n \n def test_roc_curve_fpr_tpr_increasing():\n     # Ensure that fpr and tpr returned by roc_curve are increasing.\n",
    "variant_8": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..50c3a2a95 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1079,11 +1079,18 @@ def roc_curve(\n         tps = tps[optimal_idxs]\n         thresholds = thresholds[optimal_idxs]\n \n+\n     # Add an extra threshold position\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # Handle probability estimates: if max threshold + 1 > 1 and all scores are in [0,1],\n+    # then we're likely dealing with probability estimates and should cap at 1\n+    max_threshold = thresholds[0] + 1\n+    if max_threshold > 1.0 and np.all((y_score >= 0) & (y_score <= 1)):\n+        max_threshold = 1.0\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..504314d38 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,14 +418,23 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+\n+\n+def test_roc_curve_with_probablity_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n \n def test_roc_curve_fpr_tpr_increasing():\n     # Ensure that fpr and tpr returned by roc_curve are increasing.\n"
  }
}