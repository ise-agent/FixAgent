{
  "zero_patches": {
    "Location 1 (/root/hy/projects/scikit-learn/sklearn/linear_model/least_angle.py lines 1482-1482)": "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : bool, optional\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, uses the value set in the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n        # otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n            \n        X, y = check_X_y(X, y, y_numeric=True)\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter if self.max_iter is not None else X.shape[1]\n\n        Gram = self._get_gram()\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X,\n            copy_Gram=True, alpha_min=0.0, method='lasso',\n            verbose=max(0, self.verbose - 1), max_iter=max_iter,\n            eps=self.eps, return_path=True, return_n_iter=True,\n            positive=self.positive)\n\n        residues = interpolate.interp1d(alphas_[::-1], self.n_iter_)\n\n        rss = np.sum((y[np.newaxis, :] - np.dot(X, coef_path_)) ** 2, axis=1)\n        K = np.sum(np.abs(coef_path_) > np.finfo(coef_path_.dtype).eps, axis=0)\n\n        if self.criterion == 'aic':\n            K += 1  # add one to account for variance estimate\n            self.criterion_ = rss + 2 * residues(alphas_) * K\n        elif self.criterion == 'bic':\n            s = np.sum((y[np.newaxis, :] - np.dot(X, coef_path_)) ** 2, axis=1)\n            K += 1  # add one to account for variance estimate\n            self.criterion_ = s + residues(alphas_) * K * np.log(y.size)\n\n        n_best = np.argmin(self.criterion_)\n        self.alpha_ = alphas_[n_best]\n        self.coef_ = coef_path_[:, n_best]\n        self._set_intercept(Xmean, ymean, Xstd)\n        return self\n",
    "Location 2 (/root/hy/projects/scikit-learn/sklearn/linear_model/least_angle.py lines 1503-1504)": "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``True``, X will be copied; else, it may be overwritten.\n            If ``None``, uses the copy_X value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n\n        n_samples = X.shape[0]\n",
    "Location 3 (/root/hy/projects/scikit-learn/sklearn/linear_model/least_angle.py lines 1509-1510)": "\ndef fit(self, X, y, copy_X=None):\n    \"\"\"Fit the model using X, y as training data.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Training data.\n    y : array-like, shape (n_samples,)\n        Target values. Will be cast to X's dtype if necessary\n\n    copy_X : boolean, optional, default None\n        If ``True``, X will be copied; else, it may be overwritten.\n        If None, uses the copy_X value from the constructor.\n\n    Returns\n    -------\n    self : object\n        returns an instance of self.\n    \"\"\"\n    X, y = check_X_y(X, y, y_numeric=True)\n\n    # Resolve copy_X parameter: use fit parameter if explicitly provided, \n    # otherwise use constructor value\n    if copy_X is None:\n        copy_X = self.copy_X\n\n    X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n        X, y, self.fit_intercept, self.normalize, copy_X)\n    max_iter = self.max_iter\n\n    Gram = self.precompute\n\n    alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n        X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n        method='lasso', verbose=self.verbose, max_iter=max_iter,\n        eps=self.eps, return_n_iter=True, positive=self.positive)\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects/scikit-learn/sklearn/linear_model/least_angle.py lines 1482-1482)": [
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : bool, optional\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, use the value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n        # otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n            \n        X, y = check_X_y(X, y, y_numeric=True)\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter if self.max_iter is not None else 500\n\n        Gram = self._get_gram()\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True,\n            alpha_min=0.0, method='lasso', verbose=self.verbose,\n            max_iter=max_iter, eps=self.eps, return_n_iter=True,\n            positive=self.positive)\n\n        n_samples = X.shape[0]\n\n        if self.criterion == 'aic':\n            K = 2  # AIC\n        elif self.criterion == 'bic':\n            K = log(n_samples)  # BIC\n        else:\n            raise ValueError('criterion should be either bic or aic')\n\n        R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals\n        mean_squared_error = np.mean(R ** 2, axis=0)\n\n        df = np.zeros(coef_path_.shape[1], dtype=np.int)  # degrees of freedom\n        for k, coef in enumerate(coef_path_.T):\n            mask = np.abs(coef) > np.finfo(coef.dtype).eps\n            if not np.any(mask):\n                continue\n            # get the number of degrees of freedom equal to:\n            # Xc = X[:, mask]\n            # Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs\n            df[k] = np.sum(mask)\n\n        self.alphas_ = alphas_\n        with np.errstate(divide='ignore'):\n            self.criterion_ = n_samples * np.log(mean_squared_error) + K * df\n        n_best = np.argmin(self.criterion_)\n        self.alpha_ = alphas_[n_best]\n        self.coef_ = coef_path_[:, n_best]\n        self._set_intercept(Xmean, ymean, Xstd)\n        return self\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : bool, optional\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, uses the value set in the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n        # otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n            \n        X, y = check_X_y(X, y, y_numeric=True)\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n\n        if self.fit_intercept:\n            max_iter = self.max_iter\n        else:\n            max_iter = min(self.max_iter, X.shape[1])\n\n        self.alphas_, self.coef_path_, self.dual_gap_ = lars_path(\n            X, y, Gram=self.precompute, copy_X=copy_X,\n            copy_Gram=True, alpha_min=0.0, method='lasso', verbose=max(0,\n            self.verbose-1), max_iter=max_iter, eps=self.eps,\n            return_path=self.fit_path, positive=self.positive)\n\n        self.coef_ = self.coef_path_[:, -1]\n\n        # Compute Information criteria\n        K = X.shape[0]  # n_samples\n        Ks = X.shape[1]  # n_features\n        if self.fit_intercept:\n            Ks += 1\n\n        self.criterion_ = []\n        for k, alpha in enumerate(self.alphas_):\n            # compute the number of variables in the model\n            mask = np.abs(self.coef_path_[:, k]) > np.finfo(self.coef_).eps\n            # XXX: Here we need to be smarter\n            if not np.any(mask):\n                # all coefficients are zeros, degrees of freedom is zero\n                # and loglikelihood is the one of the null model (mean)\n                loglikelihood = - K / 2 * (np.log(2 * np.pi) +\n                                           np.log(np.var(y)))\n                ddof = 0\n            else:\n                # compute the prediction\n                y_hat = np.dot(X[:, mask], self.coef_path_[mask, k])\n                if self.fit_intercept:\n                    y_hat += ymean\n                residues = y_hat - y\n                rss = np.dot(residues, residues)\n                sigma2 = rss / K\n\n                loglikelihood = - K / 2 * (np.log(2 * np.pi) +\n                                           np.log(sigma2)) - rss / (2 * sigma2)\n                ddof = mask.sum()\n                if self.fit_intercept:\n                    ddof += 1\n\n            if self.criterion == 'aic':\n                self.criterion_.append(-2 * loglikelihood + 2 * ddof)\n            elif self.criterion == 'bic':\n                self.criterion_.append(-2 * loglikelihood +\n                                       np.log(K) * ddof)\n\n        n_best = np.argmin(self.criterion_)\n        self.alpha_ = self.alphas_[n_best]\n        self.coef_ = self.coef_path_[:, n_best]\n        self._set_intercept(Xmean, ymean, Xstd)\n        return self\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : bool, optional\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, uses the copy_X value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n        # otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n            \n        X, y = check_X_y(X, y, y_numeric=True)\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter if self.max_iter is not None else X.shape[1]\n\n        Gram = self._get_gram()\n\n        alphas_, active_, coef_path_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X,\n            copy_Gram=True, alpha_min=0.0, method='lasso',\n            max_iter=max_iter, eps=self.eps, verbose=self.verbose)\n\n        n_samples = X.shape[0]\n\n        if self.criterion == 'aic':\n            K = 2  # AIC\n        elif self.criterion == 'bic':\n            K = log(n_samples)  # BIC\n        else:\n            raise ValueError('criterion should be either bic or aic')\n\n        R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals\n        mean_squared_error = np.mean(R ** 2, axis=0)\n\n        df = np.zeros(coef_path_.shape[1], dtype=np.intp)  # degrees of freedom\n        for k, coef in enumerate(coef_path_.T):\n            mask = np.abs(coef) > np.finfo(coef.dtype).eps\n            if not np.any(mask):\n                continue\n            # get the number of degrees of freedom equal to:\n            # Xc = X[:, mask]\n            # Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs\n            df[k] = np.sum(mask)\n\n        self.alphas_ = alphas_\n        with np.errstate(divide='ignore'):\n            self.criterion_ = n_samples * np.log(mean_squared_error) + K * df\n        n_best = np.argmin(self.criterion_)\n        self.alpha_ = alphas_[n_best]\n        self.coef_ = coef_path_[:, n_best]\n        self._set_intercept(Xmean, ymean, Xstd)\n        return self\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : bool, optional\n            If provided, this overrides the copy_X provided at\n            construction time.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n        # otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n            \n        X, y = check_X_y(X, y, y_numeric=True)\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter if self.max_iter is not None else len(X)\n\n        Gram = self._get_gram()\n\n        _, _, coef_path_, _ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X,\n            copy_Gram=True, alpha_min=0.0, method='lasso', verbose=max(0, self.verbose - 1),\n            max_iter=max_iter, eps=self.eps, return_n_iter=False)\n\n        residues = interpolate.interp1d(coef_path_.T[1:].T,\n                                        y[:, np.newaxis] - np.dot(X, coef_path_.T[1:].T))\n\n        residues = residues(coef_path_[1:].T).T\n        with np.errstate(divide='ignore', invalid='ignore'):\n            rss = np.sum(residues ** 2, axis=0)\n            edof = np.zeros((coef_path_.shape[-1],), dtype=np.int)\n            for k, coef in enumerate(coef_path_.T):\n                mask = np.abs(coef) > np.finfo(coef.dtype).eps\n                if not np.any(mask):\n                    continue\n                # get the number of degrees of freedom equal to:\n                # Xacti * Xacti^T - Xacti * Xacti^T / (Xacti^T * Xacti + n_samples)\n                # where Xacti is the non-zero rows of X\n                edof[k] = np.sum(mask)\n\n        n_samples = len(y)\n        if self.criterion == 'aic':\n            K = 2  # AIC\n        elif self.criterion == 'bic':\n            K = log(n_samples)  # BIC\n        else:\n            raise ValueError('criterion should be either bic or aic')\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            information_criterion = n_samples * np.log(rss / n_samples) + K * edof\n        information_criterion[~np.isfinite(information_criterion)] = np.inf\n\n        best = np.argmin(information_criterion)\n        best_coef = coef_path_[..., best]\n        self.alpha_ = coef_path_[0, best]\n        self.coef_ = best_coef[1:]\n        self._set_intercept(Xmean, ymean, Xstd)\n        self.criterion_ = information_criterion\n        return self\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : bool, optional\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None (default), uses the copy_X value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n        # otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter if self.max_iter is not None else len(X)\n\n        Gram = self._get_gram()\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X,\n            copy_Gram=True, alpha_min=0.0, method='lasso',\n            max_iter=max_iter, eps=self.eps, verbose=self.verbose)\n\n        residues = interpolate.interp1d(alphas_[::-1], coef_path_.T[::-1])\n\n        # Compute the different criteria\n        K = len(coef_path_.T)\n        n_samples = len(y)\n\n        if self.criterion == 'aic':\n            K *= 2  # AIC\n        elif self.criterion == 'bic':\n            K *= log(n_samples)  # BIC\n\n        # This is a rewrite of the expression\n        # ``residues(alpha) @ residues(alpha)`` such that only one\n        # matrix multiplication is performed.\n        def f(alpha):\n            residues_alpha = residues(alpha)\n            return (residues_alpha * residues_alpha).sum(axis=1) + K\n\n        alphas_aic_bic = np.asarray([f(alpha) for alpha in alphas_])\n        i_best = np.argmin(alphas_aic_bic)\n        alpha_ = alphas_[i_best]\n        coef_ = coef_path_[:, i_best]\n\n        self._set_intercept(Xmean, ymean, Xstd)\n        self.alpha_ = alpha_\n        self.coef_ = coef_\n        return self\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : bool, optional\n            If provided, this overrides the copy_X value\n            passed to the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n        # otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter if self.max_iter is not None else X.shape[1]\n\n        Gram = self._get_gram()\n\n        alphas_, active_, coef_path_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X,\n            copy_Gram=True, alpha_min=0.0, method='lasso',\n            verbose=max(0, self.verbose - 1), max_iter=max_iter,\n            eps=self.eps, return_path=True, return_n_iter=False,\n            positive=self.positive)\n\n        n_samples = X.shape[0]\n\n        if self.criterion == 'aic':\n            K = 2  # AIC\n        elif self.criterion == 'bic':\n            K = log(n_samples)  # BIC\n        else:\n            raise ValueError('criterion should be either bic or aic')\n\n        R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals\n        mean_square_error = np.mean(R ** 2, axis=0)\n\n        df = np.zeros(coef_path_.shape[1], dtype=np.int)  # degrees of freedom\n        for k, alpha in enumerate(alphas_):\n            # add one if not at machine precision\n            df[k] = np.sum(np.abs(coef_path_[:, k]) > np.finfo(coef_path_.dtype).eps)\n\n        self.alphas_ = alphas_\n        with np.errstate(divide='ignore'):\n            self.criterion_ = n_samples * np.log(mean_square_error) + K * df\n        n_best = np.argmin(self.criterion_)\n\n        self.alpha_ = alphas_[n_best]\n        self.coef_ = coef_path_[:, n_best]\n        self._set_intercept(Xmean, ymean, Xstd)\n        return self\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : bool, optional\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, uses the copy_X value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n            \n        X, y = check_X_y(X, y, y_numeric=True)\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter if self.max_iter is not None else X.shape[1]\n\n        Gram = self._get_gram()\n\n        alphas_, active_, coef_path_, n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X,\n            copy_Gram=True, alpha_min=0.0, method='lasso',\n            verbose=self.verbose, max_iter=max_iter, eps=self.eps,\n            return_path=True, return_n_iter=True, positive=self.positive)\n\n        n_features = X.shape[1]\n\n        if self.criterion == 'aic':\n            K = 2  # AIC\n        elif self.criterion == 'bic':\n            K = log(X.shape[0])  # BIC\n        else:\n            raise ValueError('criterion should be either bic or aic')\n\n        R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals\n        mean_squared_error = np.mean(R ** 2, axis=0)\n\n        df = np.zeros(coef_path_.shape[1], dtype=np.intp)  # degrees of freedom\n        for k, coef in enumerate(coef_path_.T):\n            mask = np.abs(coef) > np.finfo(coef.dtype).eps\n            if not np.any(mask):\n                continue\n            # get the number of degrees of freedom equal to:\n            # Xc = X[:, mask]\n            # Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs\n            df[k] = np.sum(mask)\n\n        self.n_iter_ = n_iter_\n        with np.errstate(divide='ignore'):\n            self.criterion_ = n_features * np.log(mean_squared_error) + K * df\n        n_best = np.argmin(self.criterion_)\n        self.alpha_ = alphas_[n_best]\n        self.coef_ = coef_path_[:, n_best]\n        self._set_intercept(Xmean, ymean, Xstd)\n        return self\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : bool, optional\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, uses the copy_X value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n            \n        X, y = check_X_y(X, y, y_numeric=True)\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n\n        max_iter = self.max_iter if self.max_iter is not None else X.shape[1]\n\n        Gram = self._get_gram()\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X,\n            copy_Gram=True, alpha_min=0.0, method='lasso',\n            verbose=max(0, self.verbose - 1), max_iter=max_iter,\n            eps=self.eps, return_path=True, return_n_iter=True,\n            positive=self.positive)\n\n        residues = interpolate.interp1d(alphas_[::-1], self.mse_path_[::-1])\n\n        best = np.argmin(self.criterion_)\n        self.alpha_ = alphas_[best]\n        self.coef_ = coef_path_[:, best]\n        self._set_intercept(Xmean, ymean, Xstd)\n        return self\n"
    ],
    "Location 2 (/root/hy/projects/scikit-learn/sklearn/linear_model/least_angle.py lines 1503-1504)": [
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``None``, uses the copy_X value from the constructor.\n            If ``True``, X will be copied; else, it may be overwritten.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n\n        n_samples = X.shape[0]\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values.\n\n        copy_X : boolean, optional, default None\n            If ``None``, use the copy_X value from the constructor.\n            If ``True``, X will be copied; else, it may be overwritten.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``None``, uses the copy_X value from the constructor.\n            If ``True``, X will be copied; else, it may be overwritten.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n\n        n_samples = X.shape[0]\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``None``, uses the copy_X value from the constructor.\n            If ``True``, X will be copied; else, it may be overwritten.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n\n        n_samples = X.shape[0]\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``None``, uses the copy_X value from the constructor.\n            If ``True``, X will be copied; else, it may be overwritten.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n\n        n_samples = X.shape[0]\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``None``, uses the copy_X value from the constructor.\n            If ``True``, X will be copied; else, it may be overwritten.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n\n        n_samples = X.shape[0]\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``None``, uses the copy_X value from the constructor.\n            If ``True``, X will be copied; else, it may be overwritten.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n\n        n_samples = X.shape[0]\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values.\n\n        copy_X : boolean, optional, default None\n            If ``None``, use the copy_X value from the constructor.\n            If ``True``, X will be copied; else, it may be overwritten.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n\n        n_samples = X.shape[0]\n"
    ],
    "Location 3 (/root/hy/projects/scikit-learn/sklearn/linear_model/least_angle.py lines 1509-1510)": [
      "\ndef fit(self, X, y, copy_X=None):\n    \"\"\"Fit the model using X, y as training data.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        training data.\n\n    y : array-like, shape (n_samples,)\n        target values. Will be cast to X's dtype if necessary\n\n    copy_X : boolean, optional, default None\n        If ``True``, X will be copied; else, it may be overwritten.\n        If None, uses the copy_X value from the constructor.\n\n    Returns\n    -------\n    self : object\n        returns an instance of self.\n    \"\"\"\n    X, y = check_X_y(X, y, y_numeric=True)\n\n    # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n    if copy_X is None:\n        copy_X = self.copy_X\n\n    X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n        X, y, self.fit_intercept, self.normalize, copy_X)\n    max_iter = self.max_iter\n\n    Gram = self.precompute\n\n    alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n        X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n        method='lasso', verbose=self.verbose, max_iter=max_iter,\n        eps=self.eps, return_n_iter=True, positive=self.positive)\n",
      "\ndef fit(self, X, y, copy_X=None):\n    \"\"\"Fit the model using X, y as training data.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        training data.\n\n    y : array-like, shape (n_samples,)\n        target values. Will be cast to X's dtype if necessary\n\n    copy_X : boolean, optional, default None\n        If ``True``, X will be copied; else, it may be overwritten.\n        If None, uses the copy_X value from the constructor.\n\n    Returns\n    -------\n    self : object\n        returns an instance of self.\n    \"\"\"\n    X, y = check_X_y(X, y, y_numeric=True)\n\n    # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n    if copy_X is None:\n        copy_X = self.copy_X\n\n    X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n        X, y, self.fit_intercept, self.normalize, copy_X)\n    max_iter = self.max_iter\n\n    Gram = self.precompute\n\n    alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n        X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n        method='lasso', verbose=self.verbose, max_iter=max_iter,\n        eps=self.eps, return_n_iter=True, positive=self.positive)\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, uses the copy_X value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n",
      "\ndef fit(self, X, y, copy_X=None):\n    \"\"\"Fit the model using X, y as training data.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        training data.\n\n    y : array-like, shape (n_samples,)\n        target values. Will be cast to X's dtype if necessary\n\n    copy_X : boolean, optional, default None\n        If ``True``, X will be copied; else, it may be overwritten.\n        If None, uses the copy_X value from the constructor.\n\n    Returns\n    -------\n    self : object\n        returns an instance of self.\n    \"\"\"\n    X, y = check_X_y(X, y, y_numeric=True)\n\n    # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n    if copy_X is None:\n        copy_X = self.copy_X\n\n    X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n        X, y, self.fit_intercept, self.normalize, copy_X)\n    max_iter = self.max_iter\n\n    Gram = self.precompute\n\n    alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n        X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n        method='lasso', verbose=self.verbose, max_iter=max_iter,\n        eps=self.eps, return_n_iter=True, positive=self.positive)\n\n    n_samples = X.shape[0]\n\n    if self.criterion == 'aic':\n        K = 2  # AIC\n    elif self.criterion == 'bic':\n        K = log(n_samples)  # BIC\n    else:\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, uses the copy_X value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n        # otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, uses the copy_X value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, uses the copy_X value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n",
      "\n    def fit(self, X, y, copy_X=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default None\n            If ``True``, X will be copied; else, it may be overwritten.\n            If None, uses the copy_X value from the constructor.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n        if copy_X is None:\n            copy_X = self.copy_X\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..6ac707e97 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,8 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+\n+    def fit(self, X, y, copy_X=None):\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1500,12 +1501,18 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+        \n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n \n+\n         alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n             X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n             method='lasso', verbose=self.verbose, max_iter=max_iter,\n",
    "variant_0": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..62559fb3e 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,65 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : bool, optional\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, uses the value set in the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n+        # otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+            \n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter if self.max_iter is not None else X.shape[1]\n+\n+        Gram = self._get_gram()\n+\n+        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X,\n+            copy_Gram=True, alpha_min=0.0, method='lasso',\n+            verbose=max(0, self.verbose - 1), max_iter=max_iter,\n+            eps=self.eps, return_path=True, return_n_iter=True,\n+            positive=self.positive)\n+\n+        residues = interpolate.interp1d(alphas_[::-1], self.n_iter_)\n+\n+        rss = np.sum((y[np.newaxis, :] - np.dot(X, coef_path_)) ** 2, axis=1)\n+        K = np.sum(np.abs(coef_path_) > np.finfo(coef_path_.dtype).eps, axis=0)\n+\n+        if self.criterion == 'aic':\n+            K += 1  # add one to account for variance estimate\n+            self.criterion_ = rss + 2 * residues(alphas_) * K\n+        elif self.criterion == 'bic':\n+            s = np.sum((y[np.newaxis, :] - np.dot(X, coef_path_)) ** 2, axis=1)\n+            K += 1  # add one to account for variance estimate\n+            self.criterion_ = s + residues(alphas_) * K * np.log(y.size)\n+\n+        n_best = np.argmin(self.criterion_)\n+        self.alpha_ = alphas_[n_best]\n+        self.coef_ = coef_path_[:, n_best]\n+        self._set_intercept(Xmean, ymean, Xstd)\n+        return self\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1500,8 +1558,35 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If ``None``, uses the copy_X value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n@@ -1511,6 +1596,51 @@ class LassoLarsIC(LassoLars):\n             method='lasso', verbose=self.verbose, max_iter=max_iter,\n             eps=self.eps, return_n_iter=True, positive=self.positive)\n \n+        n_samples = X.shape[0]\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+\n+def fit(self, X, y, copy_X=None):\n+    \"\"\"Fit the model using X, y as training data.\n+\n+    Parameters\n+    ----------\n+    X : array-like, shape (n_samples, n_features)\n+        Training data.\n+    y : array-like, shape (n_samples,)\n+        Target values. Will be cast to X's dtype if necessary\n+\n+    copy_X : boolean, optional, default None\n+        If ``True``, X will be copied; else, it may be overwritten.\n+        If None, uses the copy_X value from the constructor.\n+\n+    Returns\n+    -------\n+    self : object\n+        returns an instance of self.\n+    \"\"\"\n+    X, y = check_X_y(X, y, y_numeric=True)\n+\n+    # Resolve copy_X parameter: use fit parameter if explicitly provided, \n+    # otherwise use constructor value\n+    if copy_X is None:\n+        copy_X = self.copy_X\n+\n+    X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+        X, y, self.fit_intercept, self.normalize, copy_X)\n+    max_iter = self.max_iter\n+\n+    Gram = self.precompute\n+\n+    alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+        X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n+        method='lasso', verbose=self.verbose, max_iter=max_iter,\n+        eps=self.eps, return_n_iter=True, positive=self.positive)\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+\n         n_samples = X.shape[0]\n \n         if self.criterion == 'aic':\n",
    "variant_1": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..877cc2f89 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,76 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : bool, optional\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, use the value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n+        # otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+            \n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter if self.max_iter is not None else 500\n+\n+        Gram = self._get_gram()\n+\n+        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True,\n+            alpha_min=0.0, method='lasso', verbose=self.verbose,\n+            max_iter=max_iter, eps=self.eps, return_n_iter=True,\n+            positive=self.positive)\n+\n+        n_samples = X.shape[0]\n+\n+        if self.criterion == 'aic':\n+            K = 2  # AIC\n+        elif self.criterion == 'bic':\n+            K = log(n_samples)  # BIC\n+        else:\n+            raise ValueError('criterion should be either bic or aic')\n+\n+        R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals\n+        mean_squared_error = np.mean(R ** 2, axis=0)\n+\n+        df = np.zeros(coef_path_.shape[1], dtype=np.int)  # degrees of freedom\n+        for k, coef in enumerate(coef_path_.T):\n+            mask = np.abs(coef) > np.finfo(coef.dtype).eps\n+            if not np.any(mask):\n+                continue\n+            # get the number of degrees of freedom equal to:\n+            # Xc = X[:, mask]\n+            # Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs\n+            df[k] = np.sum(mask)\n+\n+        self.alphas_ = alphas_\n+        with np.errstate(divide='ignore'):\n+            self.criterion_ = n_samples * np.log(mean_squared_error) + K * df\n+        n_best = np.argmin(self.criterion_)\n+        self.alpha_ = alphas_[n_best]\n+        self.coef_ = coef_path_[:, n_best]\n+        self._set_intercept(Xmean, ymean, Xstd)\n+        return self\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1500,8 +1569,35 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``None``, uses the copy_X value from the constructor.\n+            If ``True``, X will be copied; else, it may be overwritten.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n@@ -1511,6 +1607,51 @@ class LassoLarsIC(LassoLars):\n             method='lasso', verbose=self.verbose, max_iter=max_iter,\n             eps=self.eps, return_n_iter=True, positive=self.positive)\n \n+        n_samples = X.shape[0]\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+\n+def fit(self, X, y, copy_X=None):\n+    \"\"\"Fit the model using X, y as training data.\n+\n+    Parameters\n+    ----------\n+    X : array-like, shape (n_samples, n_features)\n+        training data.\n+\n+    y : array-like, shape (n_samples,)\n+        target values. Will be cast to X's dtype if necessary\n+\n+    copy_X : boolean, optional, default None\n+        If ``True``, X will be copied; else, it may be overwritten.\n+        If None, uses the copy_X value from the constructor.\n+\n+    Returns\n+    -------\n+    self : object\n+        returns an instance of self.\n+    \"\"\"\n+    X, y = check_X_y(X, y, y_numeric=True)\n+\n+    # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+    if copy_X is None:\n+        copy_X = self.copy_X\n+\n+    X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+        X, y, self.fit_intercept, self.normalize, copy_X)\n+    max_iter = self.max_iter\n+\n+    Gram = self.precompute\n+\n+    alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+        X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n+        method='lasso', verbose=self.verbose, max_iter=max_iter,\n+        eps=self.eps, return_n_iter=True, positive=self.positive)\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+\n         n_samples = X.shape[0]\n \n         if self.criterion == 'aic':\n",
    "variant_2": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..e7b74fd39 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,93 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : bool, optional\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, uses the value set in the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n+        # otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+            \n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+\n+        if self.fit_intercept:\n+            max_iter = self.max_iter\n+        else:\n+            max_iter = min(self.max_iter, X.shape[1])\n+\n+        self.alphas_, self.coef_path_, self.dual_gap_ = lars_path(\n+            X, y, Gram=self.precompute, copy_X=copy_X,\n+            copy_Gram=True, alpha_min=0.0, method='lasso', verbose=max(0,\n+            self.verbose-1), max_iter=max_iter, eps=self.eps,\n+            return_path=self.fit_path, positive=self.positive)\n+\n+        self.coef_ = self.coef_path_[:, -1]\n+\n+        # Compute Information criteria\n+        K = X.shape[0]  # n_samples\n+        Ks = X.shape[1]  # n_features\n+        if self.fit_intercept:\n+            Ks += 1\n+\n+        self.criterion_ = []\n+        for k, alpha in enumerate(self.alphas_):\n+            # compute the number of variables in the model\n+            mask = np.abs(self.coef_path_[:, k]) > np.finfo(self.coef_).eps\n+            # XXX: Here we need to be smarter\n+            if not np.any(mask):\n+                # all coefficients are zeros, degrees of freedom is zero\n+                # and loglikelihood is the one of the null model (mean)\n+                loglikelihood = - K / 2 * (np.log(2 * np.pi) +\n+                                           np.log(np.var(y)))\n+                ddof = 0\n+            else:\n+                # compute the prediction\n+                y_hat = np.dot(X[:, mask], self.coef_path_[mask, k])\n+                if self.fit_intercept:\n+                    y_hat += ymean\n+                residues = y_hat - y\n+                rss = np.dot(residues, residues)\n+                sigma2 = rss / K\n+\n+                loglikelihood = - K / 2 * (np.log(2 * np.pi) +\n+                                           np.log(sigma2)) - rss / (2 * sigma2)\n+                ddof = mask.sum()\n+                if self.fit_intercept:\n+                    ddof += 1\n+\n+            if self.criterion == 'aic':\n+                self.criterion_.append(-2 * loglikelihood + 2 * ddof)\n+            elif self.criterion == 'bic':\n+                self.criterion_.append(-2 * loglikelihood +\n+                                       np.log(K) * ddof)\n+\n+        n_best = np.argmin(self.criterion_)\n+        self.alpha_ = self.alphas_[n_best]\n+        self.coef_ = self.coef_path_[:, n_best]\n+        self._set_intercept(Xmean, ymean, Xstd)\n+        return self\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1500,8 +1586,35 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values.\n+\n+        copy_X : boolean, optional, default None\n+            If ``None``, use the copy_X value from the constructor.\n+            If ``True``, X will be copied; else, it may be overwritten.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n@@ -1510,6 +1623,49 @@ class LassoLarsIC(LassoLars):\n             X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n             method='lasso', verbose=self.verbose, max_iter=max_iter,\n             eps=self.eps, return_n_iter=True, positive=self.positive)\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+\n+def fit(self, X, y, copy_X=None):\n+    \"\"\"Fit the model using X, y as training data.\n+\n+    Parameters\n+    ----------\n+    X : array-like, shape (n_samples, n_features)\n+        training data.\n+\n+    y : array-like, shape (n_samples,)\n+        target values. Will be cast to X's dtype if necessary\n+\n+    copy_X : boolean, optional, default None\n+        If ``True``, X will be copied; else, it may be overwritten.\n+        If None, uses the copy_X value from the constructor.\n+\n+    Returns\n+    -------\n+    self : object\n+        returns an instance of self.\n+    \"\"\"\n+    X, y = check_X_y(X, y, y_numeric=True)\n+\n+    # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+    if copy_X is None:\n+        copy_X = self.copy_X\n+\n+    X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+        X, y, self.fit_intercept, self.normalize, copy_X)\n+    max_iter = self.max_iter\n+\n+    Gram = self.precompute\n+\n+    alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+        X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n+        method='lasso', verbose=self.verbose, max_iter=max_iter,\n+        eps=self.eps, return_n_iter=True, positive=self.positive)\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n \n         n_samples = X.shape[0]\n \n",
    "variant_3": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..e00cf719d 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,75 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : bool, optional\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, uses the copy_X value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n+        # otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+            \n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter if self.max_iter is not None else X.shape[1]\n+\n+        Gram = self._get_gram()\n+\n+        alphas_, active_, coef_path_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X,\n+            copy_Gram=True, alpha_min=0.0, method='lasso',\n+            max_iter=max_iter, eps=self.eps, verbose=self.verbose)\n+\n+        n_samples = X.shape[0]\n+\n+        if self.criterion == 'aic':\n+            K = 2  # AIC\n+        elif self.criterion == 'bic':\n+            K = log(n_samples)  # BIC\n+        else:\n+            raise ValueError('criterion should be either bic or aic')\n+\n+        R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals\n+        mean_squared_error = np.mean(R ** 2, axis=0)\n+\n+        df = np.zeros(coef_path_.shape[1], dtype=np.intp)  # degrees of freedom\n+        for k, coef in enumerate(coef_path_.T):\n+            mask = np.abs(coef) > np.finfo(coef.dtype).eps\n+            if not np.any(mask):\n+                continue\n+            # get the number of degrees of freedom equal to:\n+            # Xc = X[:, mask]\n+            # Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs\n+            df[k] = np.sum(mask)\n+\n+        self.alphas_ = alphas_\n+        with np.errstate(divide='ignore'):\n+            self.criterion_ = n_samples * np.log(mean_squared_error) + K * df\n+        n_best = np.argmin(self.criterion_)\n+        self.alpha_ = alphas_[n_best]\n+        self.coef_ = coef_path_[:, n_best]\n+        self._set_intercept(Xmean, ymean, Xstd)\n+        return self\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1500,8 +1568,35 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``None``, uses the copy_X value from the constructor.\n+            If ``True``, X will be copied; else, it may be overwritten.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n@@ -1511,6 +1606,51 @@ class LassoLarsIC(LassoLars):\n             method='lasso', verbose=self.verbose, max_iter=max_iter,\n             eps=self.eps, return_n_iter=True, positive=self.positive)\n \n+        n_samples = X.shape[0]\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, uses the copy_X value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+\n         n_samples = X.shape[0]\n \n         if self.criterion == 'aic':\n",
    "variant_4": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..571a8e08c 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,80 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : bool, optional\n+            If provided, this overrides the copy_X provided at\n+            construction time.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n+        # otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+            \n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter if self.max_iter is not None else len(X)\n+\n+        Gram = self._get_gram()\n+\n+        _, _, coef_path_, _ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X,\n+            copy_Gram=True, alpha_min=0.0, method='lasso', verbose=max(0, self.verbose - 1),\n+            max_iter=max_iter, eps=self.eps, return_n_iter=False)\n+\n+        residues = interpolate.interp1d(coef_path_.T[1:].T,\n+                                        y[:, np.newaxis] - np.dot(X, coef_path_.T[1:].T))\n+\n+        residues = residues(coef_path_[1:].T).T\n+        with np.errstate(divide='ignore', invalid='ignore'):\n+            rss = np.sum(residues ** 2, axis=0)\n+            edof = np.zeros((coef_path_.shape[-1],), dtype=np.int)\n+            for k, coef in enumerate(coef_path_.T):\n+                mask = np.abs(coef) > np.finfo(coef.dtype).eps\n+                if not np.any(mask):\n+                    continue\n+                # get the number of degrees of freedom equal to:\n+                # Xacti * Xacti^T - Xacti * Xacti^T / (Xacti^T * Xacti + n_samples)\n+                # where Xacti is the non-zero rows of X\n+                edof[k] = np.sum(mask)\n+\n+        n_samples = len(y)\n+        if self.criterion == 'aic':\n+            K = 2  # AIC\n+        elif self.criterion == 'bic':\n+            K = log(n_samples)  # BIC\n+        else:\n+            raise ValueError('criterion should be either bic or aic')\n+\n+        with np.errstate(divide='ignore', invalid='ignore'):\n+            information_criterion = n_samples * np.log(rss / n_samples) + K * edof\n+        information_criterion[~np.isfinite(information_criterion)] = np.inf\n+\n+        best = np.argmin(information_criterion)\n+        best_coef = coef_path_[..., best]\n+        self.alpha_ = coef_path_[0, best]\n+        self.coef_ = best_coef[1:]\n+        self._set_intercept(Xmean, ymean, Xstd)\n+        self.criterion_ = information_criterion\n+        return self\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1500,8 +1573,35 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``None``, uses the copy_X value from the constructor.\n+            If ``True``, X will be copied; else, it may be overwritten.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n@@ -1511,6 +1611,59 @@ class LassoLarsIC(LassoLars):\n             method='lasso', verbose=self.verbose, max_iter=max_iter,\n             eps=self.eps, return_n_iter=True, positive=self.positive)\n \n+        n_samples = X.shape[0]\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+\n+def fit(self, X, y, copy_X=None):\n+    \"\"\"Fit the model using X, y as training data.\n+\n+    Parameters\n+    ----------\n+    X : array-like, shape (n_samples, n_features)\n+        training data.\n+\n+    y : array-like, shape (n_samples,)\n+        target values. Will be cast to X's dtype if necessary\n+\n+    copy_X : boolean, optional, default None\n+        If ``True``, X will be copied; else, it may be overwritten.\n+        If None, uses the copy_X value from the constructor.\n+\n+    Returns\n+    -------\n+    self : object\n+        returns an instance of self.\n+    \"\"\"\n+    X, y = check_X_y(X, y, y_numeric=True)\n+\n+    # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+    if copy_X is None:\n+        copy_X = self.copy_X\n+\n+    X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+        X, y, self.fit_intercept, self.normalize, copy_X)\n+    max_iter = self.max_iter\n+\n+    Gram = self.precompute\n+\n+    alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+        X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n+        method='lasso', verbose=self.verbose, max_iter=max_iter,\n+        eps=self.eps, return_n_iter=True, positive=self.positive)\n+\n+    n_samples = X.shape[0]\n+\n+    if self.criterion == 'aic':\n+        K = 2  # AIC\n+    elif self.criterion == 'bic':\n+        K = log(n_samples)  # BIC\n+    else:\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+\n         n_samples = X.shape[0]\n \n         if self.criterion == 'aic':\n",
    "variant_5": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..618b8f5db 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,72 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : bool, optional\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None (default), uses the copy_X value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n+        # otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter if self.max_iter is not None else len(X)\n+\n+        Gram = self._get_gram()\n+\n+        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X,\n+            copy_Gram=True, alpha_min=0.0, method='lasso',\n+            max_iter=max_iter, eps=self.eps, verbose=self.verbose)\n+\n+        residues = interpolate.interp1d(alphas_[::-1], coef_path_.T[::-1])\n+\n+        # Compute the different criteria\n+        K = len(coef_path_.T)\n+        n_samples = len(y)\n+\n+        if self.criterion == 'aic':\n+            K *= 2  # AIC\n+        elif self.criterion == 'bic':\n+            K *= log(n_samples)  # BIC\n+\n+        # This is a rewrite of the expression\n+        # ``residues(alpha) @ residues(alpha)`` such that only one\n+        # matrix multiplication is performed.\n+        def f(alpha):\n+            residues_alpha = residues(alpha)\n+            return (residues_alpha * residues_alpha).sum(axis=1) + K\n+\n+        alphas_aic_bic = np.asarray([f(alpha) for alpha in alphas_])\n+        i_best = np.argmin(alphas_aic_bic)\n+        alpha_ = alphas_[i_best]\n+        coef_ = coef_path_[:, i_best]\n+\n+        self._set_intercept(Xmean, ymean, Xstd)\n+        self.alpha_ = alpha_\n+        self.coef_ = coef_\n+        return self\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1500,8 +1565,35 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``None``, uses the copy_X value from the constructor.\n+            If ``True``, X will be copied; else, it may be overwritten.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n@@ -1511,6 +1603,52 @@ class LassoLarsIC(LassoLars):\n             method='lasso', verbose=self.verbose, max_iter=max_iter,\n             eps=self.eps, return_n_iter=True, positive=self.positive)\n \n+        n_samples = X.shape[0]\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, uses the copy_X value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n+        # otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+\n         n_samples = X.shape[0]\n \n         if self.criterion == 'aic':\n",
    "variant_6": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..9599ca280 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,73 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : bool, optional\n+            If provided, this overrides the copy_X value\n+            passed to the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, \n+        # otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter if self.max_iter is not None else X.shape[1]\n+\n+        Gram = self._get_gram()\n+\n+        alphas_, active_, coef_path_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X,\n+            copy_Gram=True, alpha_min=0.0, method='lasso',\n+            verbose=max(0, self.verbose - 1), max_iter=max_iter,\n+            eps=self.eps, return_path=True, return_n_iter=False,\n+            positive=self.positive)\n+\n+        n_samples = X.shape[0]\n+\n+        if self.criterion == 'aic':\n+            K = 2  # AIC\n+        elif self.criterion == 'bic':\n+            K = log(n_samples)  # BIC\n+        else:\n+            raise ValueError('criterion should be either bic or aic')\n+\n+        R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals\n+        mean_square_error = np.mean(R ** 2, axis=0)\n+\n+        df = np.zeros(coef_path_.shape[1], dtype=np.int)  # degrees of freedom\n+        for k, alpha in enumerate(alphas_):\n+            # add one if not at machine precision\n+            df[k] = np.sum(np.abs(coef_path_[:, k]) > np.finfo(coef_path_.dtype).eps)\n+\n+        self.alphas_ = alphas_\n+        with np.errstate(divide='ignore'):\n+            self.criterion_ = n_samples * np.log(mean_square_error) + K * df\n+        n_best = np.argmin(self.criterion_)\n+\n+        self.alpha_ = alphas_[n_best]\n+        self.coef_ = coef_path_[:, n_best]\n+        self._set_intercept(Xmean, ymean, Xstd)\n+        return self\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1500,8 +1566,35 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``None``, uses the copy_X value from the constructor.\n+            If ``True``, X will be copied; else, it may be overwritten.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n@@ -1511,6 +1604,51 @@ class LassoLarsIC(LassoLars):\n             method='lasso', verbose=self.verbose, max_iter=max_iter,\n             eps=self.eps, return_n_iter=True, positive=self.positive)\n \n+        n_samples = X.shape[0]\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, uses the copy_X value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+\n         n_samples = X.shape[0]\n \n         if self.criterion == 'aic':\n",
    "variant_7": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..727704fbb 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,75 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : bool, optional\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, uses the copy_X value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+            \n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter if self.max_iter is not None else X.shape[1]\n+\n+        Gram = self._get_gram()\n+\n+        alphas_, active_, coef_path_, n_iter_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X,\n+            copy_Gram=True, alpha_min=0.0, method='lasso',\n+            verbose=self.verbose, max_iter=max_iter, eps=self.eps,\n+            return_path=True, return_n_iter=True, positive=self.positive)\n+\n+        n_features = X.shape[1]\n+\n+        if self.criterion == 'aic':\n+            K = 2  # AIC\n+        elif self.criterion == 'bic':\n+            K = log(X.shape[0])  # BIC\n+        else:\n+            raise ValueError('criterion should be either bic or aic')\n+\n+        R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals\n+        mean_squared_error = np.mean(R ** 2, axis=0)\n+\n+        df = np.zeros(coef_path_.shape[1], dtype=np.intp)  # degrees of freedom\n+        for k, coef in enumerate(coef_path_.T):\n+            mask = np.abs(coef) > np.finfo(coef.dtype).eps\n+            if not np.any(mask):\n+                continue\n+            # get the number of degrees of freedom equal to:\n+            # Xc = X[:, mask]\n+            # Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs\n+            df[k] = np.sum(mask)\n+\n+        self.n_iter_ = n_iter_\n+        with np.errstate(divide='ignore'):\n+            self.criterion_ = n_features * np.log(mean_squared_error) + K * df\n+        n_best = np.argmin(self.criterion_)\n+        self.alpha_ = alphas_[n_best]\n+        self.coef_ = coef_path_[:, n_best]\n+        self._set_intercept(Xmean, ymean, Xstd)\n+        return self\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1500,8 +1568,78 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``None``, uses the copy_X value from the constructor.\n+            If ``True``, X will be copied; else, it may be overwritten.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+\n+        n_samples = X.shape[0]\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, uses the copy_X value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n@@ -1510,6 +1648,8 @@ class LassoLarsIC(LassoLars):\n             X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n             method='lasso', verbose=self.verbose, max_iter=max_iter,\n             eps=self.eps, return_n_iter=True, positive=self.positive)\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n \n         n_samples = X.shape[0]\n \n",
    "variant_8": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..d897f71b1 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,54 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : bool, optional\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, uses the copy_X value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+            \n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+\n+        max_iter = self.max_iter if self.max_iter is not None else X.shape[1]\n+\n+        Gram = self._get_gram()\n+\n+        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X,\n+            copy_Gram=True, alpha_min=0.0, method='lasso',\n+            verbose=max(0, self.verbose - 1), max_iter=max_iter,\n+            eps=self.eps, return_path=True, return_n_iter=True,\n+            positive=self.positive)\n+\n+        residues = interpolate.interp1d(alphas_[::-1], self.mse_path_[::-1])\n+\n+        best = np.argmin(self.criterion_)\n+        self.alpha_ = alphas_[best]\n+        self.coef_ = coef_path_[:, best]\n+        self._set_intercept(Xmean, ymean, Xstd)\n+        return self\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1500,8 +1547,78 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values.\n+\n+        copy_X : boolean, optional, default None\n+            If ``None``, use the copy_X value from the constructor.\n+            If ``True``, X will be copied; else, it may be overwritten.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n+            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n+\n+        n_samples = X.shape[0]\n+        max_iter = self.max_iter\n+\n+        Gram = self.precompute\n+\n+\n+    def fit(self, X, y, copy_X=None):\n+        \"\"\"Fit the model using X, y as training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            training data.\n+\n+        y : array-like, shape (n_samples,)\n+            target values. Will be cast to X's dtype if necessary\n+\n+        copy_X : boolean, optional, default None\n+            If ``True``, X will be copied; else, it may be overwritten.\n+            If None, uses the copy_X value from the constructor.\n+\n+        Returns\n+        -------\n+        self : object\n+            returns an instance of self.\n+        \"\"\"\n+        X, y = check_X_y(X, y, y_numeric=True)\n+\n+        # Resolve copy_X parameter: use fit parameter if explicitly provided, otherwise use constructor value\n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n+        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n@@ -1510,6 +1627,8 @@ class LassoLarsIC(LassoLars):\n             X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n             method='lasso', verbose=self.verbose, max_iter=max_iter,\n             eps=self.eps, return_n_iter=True, positive=self.positive)\n+            method='lasso', verbose=self.verbose, max_iter=max_iter,\n+            eps=self.eps, return_n_iter=True, positive=self.positive)\n \n         n_samples = X.shape[0]\n \n"
  }
}