{
  "zero_patches": {
    "Location 1 (/root/hy/projects_2/xarray/xarray/core/alignment.py lines 283-292)": "\nfor obj in objects:\n    for dim in obj.dims:\n        if dim not in exclude:\n            coord = obj.coords[dim]\n            # Preserve dask arrays by avoiding operations that trigger computation\n            all_coords[dim].append(coord)\n            try:\n                index = obj.indexes[dim]\n            except KeyError:\n                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n            else:\n                all_indexes[dim].append(index)\n",
    "Location 2 (/root/hy/projects_2/xarray/xarray/core/alignment.py lines 322-327)": "\n            if (\n                any(\n                    not matching_indexes[0].equals(other)\n                    for other in matching_indexes[1:]\n                )\n                or dim in unlabeled_dim_sizes\n            ):\n                if join == \"exact\":\n                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                index = joiner(matching_indexes)\n                # Check if any coordinates are dask arrays before coercing to avoid triggering computation\n                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n                if not has_dask_coords:\n                    # make sure str coords are not cast to object\n                    index = maybe_coerce_to_str(index, all_coords[dim])\n                joined_indexes[dim] = index\n            else:\n                index = all_coords[dim][0]\n",
    "Location 3 (/root/hy/projects_2/xarray/xarray/core/alignment.py lines 357-359)": "\n            new_obj = obj.reindex(\n                copy=copy, \n                fill_value=fill_value, \n                indexers=valid_indexers,\n                method=None\n            )\n",
    "Location 4 (/root/hy/projects_2/xarray/xarray/core/merge.py lines 908-940)": "\ndef dataset_update_method(\n    dataset: \"Dataset\", other: \"CoercibleMapping\"\n) -> _MergeResult:\n    \"\"\"Guts of the Dataset.update method.\n\n    This drops a duplicated coordinates from `other` if `other` is not an\n    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n    GH2180).\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [\n                    c\n                    for c in value.coords\n                    if c not in value.dims and c in dataset.coords\n                ]\n                if coord_names:\n                    # Preserve dask arrays by creating a new DataArray without triggering computation\n                    dropped_value = value.drop_vars(coord_names)\n                    # Ensure chunking is preserved if original value was chunked\n                    if hasattr(value.data, 'chunks') and not hasattr(dropped_value.data, 'chunks'):\n                        dropped_value = dropped_value.chunk(value.chunks)\n                    other[key] = dropped_value\n\n    # use ds.coords and not ds.indexes, else str coords are cast to object\n    indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n    return merge_core(\n        [dataset, other],\n        priority_arg=1,\n        indexes=indexes,\n        combine_attrs=\"override\",\n    )\n",
    "Location 5 (/root/hy/projects_2/xarray/xarray/core/dataset.py lines 3883-3910)": "\ndef update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n    \"\"\"Update this dataset's variables with those from another dataset.\n\n    Parameters\n    ----------\n    other : Dataset or mapping\n        Variables with which to update this dataset. One of:\n\n        - Dataset\n        - mapping {var name: DataArray}\n        - mapping {var name: Variable}\n        - mapping {var name: (dimension name, array-like)}\n        - mapping {var name: (tuple of dimension names, array-like)}\n\n\n    Returns\n    -------\n    updated : Dataset\n        Updated dataset.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions would have inconsistent sizes in the updated\n        dataset.\n    \"\"\"\n    # Check if any variables in the update contain dask arrays\n    preserve_chunks = {}\n    if hasattr(other, 'items'):\n        for name, var in other.items():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    elif hasattr(other, 'variables'):\n        for name, var in other.variables.items():\n            if hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    \n    merge_result = dataset_update_method(self, other)\n    result = self._replace(inplace=True, **merge_result._asdict())\n    \n    # Ensure dask arrays remain chunked after update\n    if preserve_chunks:\n        for name, chunks in preserve_chunks.items():\n            if name in result.variables and not hasattr(result[name].data, 'chunks'):\n                # Restore chunking if it was lost\n                result = result.chunk({name: chunks})\n    \n    return result\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_2/xarray/xarray/core/alignment.py lines 283-292)": [
      "\nfor obj in objects:\n    for dim in obj.dims:\n        if dim not in exclude:\n            coord = obj.coords[dim]\n            # Preserve dask arrays by avoiding operations that trigger computation\n            all_coords[dim].append(coord)\n            try:\n                index = obj.indexes[dim]\n            except KeyError:\n                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n            else:\n                all_indexes[dim].append(index)\n",
      "\n    for obj in objects:\n        for dim in obj.dims:\n            if dim not in exclude:\n                coord = obj.coords[dim]\n                # Preserve dask arrays by avoiding operations that trigger computation\n                all_coords[dim].append(coord)\n                try:\n                    index = obj.indexes[dim]\n                except KeyError:\n                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n                else:\n                    all_indexes[dim].append(index)\n",
      "\nfor obj in objects:\n    for dim in obj.dims:\n        if dim not in exclude:\n            coord = obj.coords[dim]\n            # Preserve dask arrays by avoiding operations that trigger computation\n            all_coords[dim].append(coord)\n            try:\n                index = obj.indexes[dim]\n            except KeyError:\n                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n            else:\n                all_indexes[dim].append(index)\n",
      "\n    for obj in objects:\n        for dim in obj.dims:\n            if dim not in exclude:\n                coord = obj.coords[dim]\n                # Preserve dask arrays by avoiding operations that trigger computation\n                all_coords[dim].append(coord)\n                try:\n                    index = obj.indexes[dim]\n                except KeyError:\n                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n                else:\n                    all_indexes[dim].append(index)\n",
      "\nfor obj in objects:\n    for dim in obj.dims:\n        if dim not in exclude:\n            coord = obj.coords[dim]\n            # Preserve dask arrays by avoiding operations that trigger computation\n            all_coords[dim].append(coord)\n            try:\n                index = obj.indexes[dim]\n            except KeyError:\n                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n            else:\n                all_indexes[dim].append(index)\n",
      "\nfor obj in objects:\n    for dim in obj.dims:\n        if dim not in exclude:\n            coord = obj.coords[dim]\n            # Preserve dask arrays by avoiding operations that trigger computation\n            all_coords[dim].append(coord)\n            try:\n                index = obj.indexes[dim]\n            except KeyError:\n                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n            else:\n                all_indexes[dim].append(index)\n",
      "\nfor obj in objects:\n    for dim in obj.dims:\n        if dim not in exclude:\n            coord = obj.coords[dim]\n            # Preserve dask arrays by avoiding operations that trigger computation\n            all_coords[dim].append(coord)\n            try:\n                index = obj.indexes[dim]\n            except KeyError:\n                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n            else:\n                all_indexes[dim].append(index)\n",
      "\nfor obj in objects:\n    for dim in obj.dims:\n        if dim not in exclude:\n            coord = obj.coords[dim]\n            # Preserve dask arrays by avoiding operations that trigger computation\n            all_coords[dim].append(coord)\n            try:\n                index = obj.indexes[dim]\n            except KeyError:\n                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n            else:\n                all_indexes[dim].append(index)\n"
    ],
    "Location 2 (/root/hy/projects_2/xarray/xarray/core/alignment.py lines 322-327)": [
      "\n            else:\n                if (\n                    any(\n                        not matching_indexes[0].equals(other)\n                        for other in matching_indexes[1:]\n                    )\n                    or dim in unlabeled_dim_sizes\n                ):\n                    if join == \"exact\":\n                        raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                    index = joiner(matching_indexes)\n                    # Check if any coordinates are dask arrays before coercing\n                    has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n                    if not has_dask_coords:\n                        index = maybe_coerce_to_str(index, all_coords[dim])\n                    joined_indexes[dim] = index\n                else:\n                    index = all_coords[dim][0]\n",
      "\n                index = joiner(matching_indexes)\n                # make sure str coords are not cast to object\n                # Check if any coordinates are dask arrays before coercing\n                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n                if not has_dask_coords:\n                    index = maybe_coerce_to_str(index, all_coords[dim])\n                joined_indexes[dim] = index\n            else:\n                # For dask arrays, avoid operations that might trigger computation\n                coord = all_coords[dim][0]\n                if hasattr(coord.data, 'chunks'):\n                    index = coord\n                else:\n                    index = all_coords[dim][0]\n",
      "\n                index = joiner(matching_indexes)\n                # make sure str coords are not cast to object\n                # Check if any coordinates are dask arrays before coercing\n                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n                if not has_dask_coords:\n                    index = maybe_coerce_to_str(index, all_coords[dim])\n                joined_indexes[dim] = index\n            else:\n                # For dask arrays, avoid operations that might trigger computation\n                coord = all_coords[dim][0]\n                if hasattr(coord.data, 'chunks'):\n                    index = coord\n                else:\n                    index = all_coords[dim][0]\n",
      "\n        else:\n            if (\n                any(\n                    not matching_indexes[0].equals(other)\n                    for other in matching_indexes[1:]\n                )\n                or dim in unlabeled_dim_sizes\n            ):\n                if join == \"exact\":\n                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                index = joiner(matching_indexes)\n                # Check if any coordinates are dask arrays before coercing\n                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n                if not has_dask_coords:\n                    # make sure str coords are not cast to object\n                    index = maybe_coerce_to_str(index, all_coords[dim])\n                joined_indexes[dim] = index\n            else:\n                coord = all_coords[dim][0]\n                if hasattr(coord.data, 'chunks'):\n                    index = coord.to_index()  # Use dask-safe index creation\n                else:\n                    index = all_coords[dim][0]\n",
      "\n                if join == \"exact\":\n                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                index = joiner(matching_indexes)\n                # Check if any coordinates are dask arrays before coercing\n                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n                if not has_dask_coords:\n                    # make sure str coords are not cast to object\n                    index = maybe_coerce_to_str(index, all_coords[dim])\n                joined_indexes[dim] = index\n            else:\n                index = all_coords[dim][0]\n",
      "\n                if join == \"exact\":\n                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                index = joiner(matching_indexes)\n                # Check if any coordinates are dask arrays before coercing\n                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n                if not has_dask_coords:\n                    # make sure str coords are not cast to object\n                    index = maybe_coerce_to_str(index, all_coords[dim])\n                joined_indexes[dim] = index\n            else:\n                index = all_coords[dim][0]\n",
      "\n        else:\n            if (\n                any(\n                    not matching_indexes[0].equals(other)\n                    for other in matching_indexes[1:]\n                )\n                or dim in unlabeled_dim_sizes\n            ):\n                if join == \"exact\":\n                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                index = joiner(matching_indexes)\n                # Check if any coordinates are dask arrays before coercing\n                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n                if not has_dask_coords:\n                    # make sure str coords are not cast to object\n                    index = maybe_coerce_to_str(index, all_coords[dim])\n                joined_indexes[dim] = index\n            else:\n                coord = all_coords[dim][0]\n                if hasattr(coord.data, 'chunks'):\n                    index = coord.to_index()  # Use dask-safe index creation\n                else:\n                    index = all_coords[dim][0]\n",
      "\n                if join == \"exact\":\n                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                index = joiner(matching_indexes)\n                # Check if any coordinates are dask arrays before coercing\n                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n                if not has_dask_coords:\n                    # make sure str coords are not cast to object\n                    index = maybe_coerce_to_str(index, all_coords[dim])\n                joined_indexes[dim] = index\n            else:\n                index = all_coords[dim][0]\n"
    ],
    "Location 3 (/root/hy/projects_2/xarray/xarray/core/alignment.py lines 357-359)": [
      "\n            new_obj = obj.reindex(\n                copy=copy, fill_value=fill_value, indexers=valid_indexers,\n                method=None  # Preserve dask arrays by avoiding interpolation\n            )\n",
      "\n            new_obj = obj.reindex(\n                copy=copy, fill_value=fill_value, indexers=valid_indexers\n            )\n",
      "\n            # Check if reindexing involves dask arrays and preserve chunking\n            has_dask_data = any(\n                hasattr(getattr(obj.variables.get(k), 'data', None), 'chunks')\n                for k in valid_indexers.keys()\n            )\n            \n            new_obj = obj.reindex(\n                copy=copy, fill_value=fill_value, indexers=valid_indexers\n            )\n            \n            # Restore dask chunking if it was lost during reindexing\n            if has_dask_data:\n                for var_name, var in new_obj.variables.items():\n                    orig_var = obj.variables.get(var_name)\n                    if (orig_var is not None and \n                        hasattr(orig_var.data, 'chunks') and \n                        not hasattr(var.data, 'chunks')):\n                        new_obj[var_name] = new_obj[var_name].chunk(orig_var.chunks)\n",
      "\n            new_obj = obj.reindex(\n                copy=copy, fill_value=fill_value, indexers=valid_indexers,\n                method=None\n            )\n",
      "\n            new_obj = obj.reindex(\n                copy=copy, fill_value=fill_value, indexers=valid_indexers,\n                method=None  # Preserve dask arrays by avoiding interpolation\n            )\n",
      "\n            new_obj = obj.reindex(\n                copy=copy, fill_value=fill_value, indexers=valid_indexers\n            )\n",
      "\n            new_obj = obj.reindex(\n                copy=copy, \n                fill_value=fill_value, \n                indexers=valid_indexers,\n                method=None\n            )\n",
      "\n            new_obj = obj.reindex(\n                copy=copy, \n                fill_value=fill_value, \n                indexers=valid_indexers,\n                method=None\n            )\n"
    ],
    "Location 4 (/root/hy/projects_2/xarray/xarray/core/merge.py lines 908-940)": [
      "\ndef dataset_update_method(\n    dataset: \"Dataset\", other: \"CoercibleMapping\"\n) -> _MergeResult:\n    \"\"\"Guts of the Dataset.update method.\n\n    This drops a duplicated coordinates from `other` if `other` is not an\n    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n    GH2180).\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [\n                    c\n                    for c in value.coords\n                    if c not in value.dims and c in dataset.coords\n                ]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n\n    # use ds.coords and not ds.indexes, else str coords are cast to object\n    indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n    \n    # Check if any variables contain dask arrays to preserve chunking\n    has_dask_data = False\n    if isinstance(other, dict):\n        for var in other.values():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                has_dask_data = True\n                break\n    elif hasattr(other, 'data_vars'):\n        for var in other.data_vars.values():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                has_dask_data = True\n                break\n    \n    return merge_core(\n        [dataset, other],\n        priority_arg=1,\n        indexes=indexes,\n        combine_attrs=\"override\",\n    )\n",
      "\ndef dataset_update_method(\n    dataset: \"Dataset\", other: \"CoercibleMapping\"\n) -> _MergeResult:\n    \"\"\"Guts of the Dataset.update method.\n\n    This drops a duplicated coordinates from `other` if `other` is not an\n    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n    GH2180).\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [\n                    c\n                    for c in value.coords\n                    if c not in value.dims and c in dataset.coords\n                ]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n\n    # use ds.coords and not ds.indexes, else str coords are cast to object\n    indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n    \n    # Check if we're dealing with dask arrays and preserve chunking\n    has_dask_data = False\n    if hasattr(other, 'items'):\n        for var in other.values():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                has_dask_data = True\n                break\n    \n    return merge_core(\n        [dataset, other],\n        priority_arg=1,\n        indexes=indexes,\n        combine_attrs=\"override\",\n    )\n",
      "\ndef dataset_update_method(\n    dataset: \"Dataset\", other: \"CoercibleMapping\"\n) -> _MergeResult:\n    \"\"\"Guts of the Dataset.update method.\n\n    This drops a duplicated coordinates from `other` if `other` is not an\n    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n    GH2180).\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [\n                    c\n                    for c in value.coords\n                    if c not in value.dims and c in dataset.coords\n                ]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n\n    # use ds.coords and not ds.indexes, else str coords are cast to object\n    indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n    \n    # Check if we're dealing with dask arrays to preserve chunking\n    has_dask_data = False\n    if hasattr(other, 'items'):\n        for var in other.values():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                has_dask_data = True\n                break\n    \n    return merge_core(\n        [dataset, other],\n        priority_arg=1,\n        indexes=indexes,\n        combine_attrs=\"override\",\n    )\n",
      "\ndef dataset_update_method(\n    dataset: \"Dataset\", other: \"CoercibleMapping\"\n) -> _MergeResult:\n    \"\"\"Guts of the Dataset.update method.\n\n    This drops a duplicated coordinates from `other` if `other` is not an\n    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n    GH2180).\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [\n                    c\n                    for c in value.coords\n                    if c not in value.dims and c in dataset.coords\n                ]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n\n    # use ds.coords and not ds.indexes, else str coords are cast to object\n    indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n    \n    # Check if we're dealing with dask arrays to preserve chunking\n    has_dask_data = False\n    if isinstance(other, dict):\n        has_dask_data = any(\n            hasattr(getattr(var, 'data', None), 'chunks')\n            for var in other.values()\n            if hasattr(var, 'data')\n        )\n    elif hasattr(other, 'data_vars'):\n        has_dask_data = any(\n            hasattr(var.data, 'chunks')\n            for var in other.data_vars.values()\n        )\n    \n    return merge_core(\n        [dataset, other],\n        priority_arg=1,\n        indexes=indexes,\n        combine_attrs=\"override\",\n    )\n",
      "\ndef dataset_update_method(\n    dataset: \"Dataset\", other: \"CoercibleMapping\"\n) -> _MergeResult:\n    \"\"\"Guts of the Dataset.update method.\n\n    This drops a duplicated coordinates from `other` if `other` is not an\n    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n    GH2180).\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [\n                    c\n                    for c in value.coords\n                    if c not in value.dims and c in dataset.coords\n                ]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n\n    # use ds.coords and not ds.indexes, else str coords are cast to object\n    indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n    \n    # Check if we're dealing with dask arrays and preserve their chunking\n    has_dask_data = False\n    if hasattr(other, 'items'):\n        for var in other.values():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                has_dask_data = True\n                break\n    \n    return merge_core(\n        [dataset, other],\n        priority_arg=1,\n        indexes=indexes,\n        combine_attrs=\"override\",\n    )\n",
      "\ndef dataset_update_method(\n    dataset: \"Dataset\", other: \"CoercibleMapping\"\n) -> _MergeResult:\n    \"\"\"Guts of the Dataset.update method.\n\n    This drops a duplicated coordinates from `other` if `other` is not an\n    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n    GH2180).\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [\n                    c\n                    for c in value.coords\n                    if c not in value.dims and c in dataset.coords\n                ]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n\n    # use ds.coords and not ds.indexes, else str coords are cast to object\n    indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n    \n    # Check if we're dealing with dask arrays to preserve chunking\n    has_dask_data = False\n    if isinstance(other, dict):\n        for var in other.values():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                has_dask_data = True\n                break\n    elif hasattr(other, 'data_vars'):\n        for var in other.data_vars.values():\n            if hasattr(var.data, 'chunks'):\n                has_dask_data = True\n                break\n    \n    return merge_core(\n        [dataset, other],\n        priority_arg=1,\n        indexes=indexes,\n        combine_attrs=\"override\",\n    )\n",
      "\ndef dataset_update_method(\n    dataset: \"Dataset\", other: \"CoercibleMapping\"\n) -> _MergeResult:\n    \"\"\"Guts of the Dataset.update method.\n\n    This drops a duplicated coordinates from `other` if `other` is not an\n    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n    GH2180).\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [\n                    c\n                    for c in value.coords\n                    if c not in value.dims and c in dataset.coords\n                ]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n\n    # use ds.coords and not ds.indexes, else str coords are cast to object\n    indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n    \n    # Check if we have dask arrays to preserve during merge\n    has_dask_data = False\n    if isinstance(other, dict):\n        for var in other.values():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                has_dask_data = True\n                break\n    elif hasattr(other, 'data_vars'):\n        for var in other.data_vars.values():\n            if hasattr(var.data, 'chunks'):\n                has_dask_data = True\n                break\n    \n    return merge_core(\n        [dataset, other],\n        priority_arg=1,\n        indexes=indexes,\n        combine_attrs=\"override\",\n    )\n",
      "\ndef dataset_update_method(\n    dataset: \"Dataset\", other: \"CoercibleMapping\"\n) -> _MergeResult:\n    \"\"\"Guts of the Dataset.update method.\n\n    This drops a duplicated coordinates from `other` if `other` is not an\n    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n    GH2180).\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [\n                    c\n                    for c in value.coords\n                    if c not in value.dims and c in dataset.coords\n                ]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n\n    # use ds.coords and not ds.indexes, else str coords are cast to object\n    indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n    \n    # Check if any variables contain dask arrays to preserve chunking\n    has_dask_data = False\n    if hasattr(other, 'values'):\n        has_dask_data = any(\n            hasattr(getattr(var, 'data', None), 'chunks')\n            for var in other.values()\n            if hasattr(var, 'data')\n        )\n    elif isinstance(other, dict):\n        has_dask_data = any(\n            hasattr(getattr(var, 'data', None), 'chunks')\n            for var in other.values()\n            if hasattr(var, 'data')\n        )\n    \n    return merge_core(\n        [dataset, other],\n        priority_arg=1,\n        indexes=indexes,\n        combine_attrs=\"override\",\n    )\n"
    ],
    "Location 5 (/root/hy/projects_2/xarray/xarray/core/dataset.py lines 3883-3910)": [
      "\ndef update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n    \"\"\"Update this dataset's variables with those from another dataset.\n\n    Parameters\n    ----------\n    other : Dataset or mapping\n        Variables with which to update this dataset. One of:\n\n        - Dataset\n        - mapping {var name: DataArray}\n        - mapping {var name: Variable}\n        - mapping {var name: (dimension name, array-like)}\n        - mapping {var name: (tuple of dimension names, array-like)}\n\n\n    Returns\n    -------\n    updated : Dataset\n        Updated dataset.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions would have inconsistent sizes in the updated\n        dataset.\n    \"\"\"\n    # Check if any variables in the update contain dask arrays\n    preserve_chunks = {}\n    if hasattr(other, 'items'):\n        for name, var in other.items():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    \n    merge_result = dataset_update_method(self, other)\n    result = self._replace(inplace=True, **merge_result._asdict())\n    \n    # Ensure dask arrays remain chunked after update\n    if preserve_chunks:\n        for name, chunks in preserve_chunks.items():\n            if name in result.variables and not hasattr(result.variables[name].data, 'chunks'):\n                # Restore chunking if it was lost during the merge process\n                result = result.chunk({name: chunks})\n    \n    return result\n",
      "\ndef update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n    \"\"\"Update this dataset's variables with those from another dataset.\n\n    Parameters\n    ----------\n    other : Dataset or mapping\n        Variables with which to update this dataset. One of:\n\n        - Dataset\n        - mapping {var name: DataArray}\n        - mapping {var name: Variable}\n        - mapping {var name: (dimension name, array-like)}\n        - mapping {var name: (tuple of dimension names, array-like)}\n\n\n    Returns\n    -------\n    updated : Dataset\n        Updated dataset.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions would have inconsistent sizes in the updated\n        dataset.\n    \"\"\"\n    # Check if any variables in the update contain dask arrays\n    preserve_chunks = {}\n    if hasattr(other, 'items'):\n        for name, var in other.items():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    \n    merge_result = dataset_update_method(self, other)\n    result = self._replace(inplace=True, **merge_result._asdict())\n    \n    # Ensure dask arrays remain chunked after update\n    if preserve_chunks:\n        for name, chunks in preserve_chunks.items():\n            if name in result.variables and not hasattr(result.variables[name].data, 'chunks'):\n                # Restore chunking if it was lost during merge\n                result = result.chunk({name: chunks})\n    \n    return result\n",
      "\ndef update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n    \"\"\"Update this dataset's variables with those from another dataset.\n\n    Parameters\n    ----------\n    other : Dataset or mapping\n        Variables with which to update this dataset. One of:\n\n        - Dataset\n        - mapping {var name: DataArray}\n        - mapping {var name: Variable}\n        - mapping {var name: (dimension name, array-like)}\n        - mapping {var name: (tuple of dimension names, array-like)}\n\n\n    Returns\n    -------\n    updated : Dataset\n        Updated dataset.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions would have inconsistent sizes in the updated\n        dataset.\n    \"\"\"\n    # Check if any variables in the update contain dask arrays\n    preserve_chunks = {}\n    if hasattr(other, 'values'):\n        for name, var in other.items():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    \n    merge_result = dataset_update_method(self, other)\n    result = self._replace(inplace=True, **merge_result._asdict())\n    \n    # Ensure dask arrays remain chunked after update\n    for name, chunks in preserve_chunks.items():\n        if name in result.variables:\n            var = result.variables[name]\n            if not hasattr(var.data, 'chunks'):\n                # Restore chunking if it was lost during merge\n                result = result.chunk({name: chunks})\n    \n    return result\n",
      "\ndef update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n    \"\"\"Update this dataset's variables with those from another dataset.\n\n    Parameters\n    ----------\n    other : Dataset or mapping\n        Variables with which to update this dataset. One of:\n\n        - Dataset\n        - mapping {var name: DataArray}\n        - mapping {var name: Variable}\n        - mapping {var name: (dimension name, array-like)}\n        - mapping {var name: (tuple of dimension names, array-like)}\n\n\n    Returns\n    -------\n    updated : Dataset\n        Updated dataset.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions would have inconsistent sizes in the updated\n        dataset.\n    \"\"\"\n    # Check if any variables in the update contain dask arrays\n    preserve_chunks = {}\n    if hasattr(other, 'items'):\n        for name, var in other.items():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    elif hasattr(other, 'variables'):\n        for name, var in other.variables.items():\n            if hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    \n    merge_result = dataset_update_method(self, other)\n    result = self._replace(inplace=True, **merge_result._asdict())\n    \n    # Ensure dask arrays remain chunked after update\n    if preserve_chunks:\n        for name, chunks in preserve_chunks.items():\n            if name in result.variables:\n                var = result.variables[name]\n                if not hasattr(var.data, 'chunks'):\n                    # Restore chunking if it was lost during merge\n                    result = result.assign({name: result[name].chunk(chunks)})\n    \n    return result\n",
      "\ndef update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n    \"\"\"Update this dataset's variables with those from another dataset.\n\n    Parameters\n    ----------\n    other : Dataset or mapping\n        Variables with which to update this dataset. One of:\n\n        - Dataset\n        - mapping {var name: DataArray}\n        - mapping {var name: Variable}\n        - mapping {var name: (dimension name, array-like)}\n        - mapping {var name: (tuple of dimension names, array-like)}\n\n\n    Returns\n    -------\n    updated : Dataset\n        Updated dataset.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions would have inconsistent sizes in the updated\n        dataset.\n    \"\"\"\n    # Check if any variables in the update contain dask arrays\n    preserve_chunks = {}\n    if hasattr(other, 'items'):\n        for name, var in other.items():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    elif hasattr(other, 'variables'):\n        for name, var in other.variables.items():\n            if hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    \n    merge_result = dataset_update_method(self, other)\n    result = self._replace(inplace=True, **merge_result._asdict())\n    \n    # Ensure dask arrays remain chunked after update\n    if preserve_chunks:\n        for name, chunks in preserve_chunks.items():\n            if name in result.variables and not hasattr(result.variables[name].data, 'chunks'):\n                # Restore chunking if it was lost during merge\n                result._variables[name] = result._variables[name].chunk(chunks)\n    \n    return result\n",
      "\ndef update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n    \"\"\"Update this dataset's variables with those from another dataset.\n\n    Parameters\n    ----------\n    other : Dataset or mapping\n        Variables with which to update this dataset. One of:\n\n        - Dataset\n        - mapping {var name: DataArray}\n        - mapping {var name: Variable}\n        - mapping {var name: (dimension name, array-like)}\n        - mapping {var name: (tuple of dimension names, array-like)}\n\n\n    Returns\n    -------\n    updated : Dataset\n        Updated dataset.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions would have inconsistent sizes in the updated\n        dataset.\n    \"\"\"\n    # Check if any variables in the update contain dask arrays\n    preserve_chunks = {}\n    if hasattr(other, 'items'):\n        for name, var in other.items():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    \n    merge_result = dataset_update_method(self, other)\n    result = self._replace(inplace=True, **merge_result._asdict())\n    \n    # Ensure dask arrays remain chunked after update\n    for name, chunks in preserve_chunks.items():\n        if name in result.variables:\n            var = result.variables[name]\n            if not hasattr(var.data, 'chunks'):\n                # Restore chunking if it was lost during merge\n                result = result.assign({name: result[name].chunk(chunks)})\n    \n    return result\n",
      "\ndef update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n    \"\"\"Update this dataset's variables with those from another dataset.\n\n    Parameters\n    ----------\n    other : Dataset or mapping\n        Variables with which to update this dataset. One of:\n\n        - Dataset\n        - mapping {var name: DataArray}\n        - mapping {var name: Variable}\n        - mapping {var name: (dimension name, array-like)}\n        - mapping {var name: (tuple of dimension names, array-like)}\n\n\n    Returns\n    -------\n    updated : Dataset\n        Updated dataset.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions would have inconsistent sizes in the updated\n        dataset.\n    \"\"\"\n    # Check if any variables in the update contain dask arrays\n    preserve_chunks = {}\n    if hasattr(other, 'data_vars'):\n        # other is a Dataset\n        for name, var in other.data_vars.items():\n            if hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    elif hasattr(other, 'items'):\n        # other is a mapping\n        for name, var in other.items():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    \n    merge_result = dataset_update_method(self, other)\n    result = self._replace(inplace=True, **merge_result._asdict())\n    \n    # Ensure dask arrays remain chunked after update\n    for name, chunks in preserve_chunks.items():\n        if name in result.data_vars and not hasattr(result[name].data, 'chunks'):\n            # Restore chunking if it was lost during the merge process\n            result = result.assign({name: result[name].chunk(chunks)})\n    \n    return result\n",
      "\ndef update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n    \"\"\"Update this dataset's variables with those from another dataset.\n\n    Parameters\n    ----------\n    other : Dataset or mapping\n        Variables with which to update this dataset. One of:\n\n        - Dataset\n        - mapping {var name: DataArray}\n        - mapping {var name: Variable}\n        - mapping {var name: (dimension name, array-like)}\n        - mapping {var name: (tuple of dimension names, array-like)}\n\n\n    Returns\n    -------\n    updated : Dataset\n        Updated dataset.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions would have inconsistent sizes in the updated\n        dataset.\n    \"\"\"\n    # Check if any variables in the update contain dask arrays\n    preserve_chunks = {}\n    if hasattr(other, 'items'):\n        for name, var in other.items():\n            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    elif hasattr(other, 'variables'):\n        for name, var in other.variables.items():\n            if hasattr(var.data, 'chunks'):\n                preserve_chunks[name] = var.chunks\n    \n    merge_result = dataset_update_method(self, other)\n    result = self._replace(inplace=True, **merge_result._asdict())\n    \n    # Ensure dask arrays remain chunked after update\n    if preserve_chunks:\n        for name, chunks in preserve_chunks.items():\n            if name in result.variables and not hasattr(result.variables[name].data, 'chunks'):\n                # Restore chunking if it was lost during merge\n                result = result.chunk({name: chunks})\n    \n    return result\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py\nindex debf3aad..a3415928 100644\n--- a/xarray/core/alignment.py\n+++ b/xarray/core/alignment.py\n@@ -280,10 +280,13 @@ def align(\n     all_indexes = defaultdict(list)\n     all_coords = defaultdict(list)\n     unlabeled_dim_sizes = defaultdict(set)\n+\n     for obj in objects:\n         for dim in obj.dims:\n             if dim not in exclude:\n-                all_coords[dim].append(obj.coords[dim])\n+                coord = obj.coords[dim]\n+                # Preserve dask arrays by avoiding operations that trigger computation\n+                all_coords[dim].append(coord)\n                 try:\n                     index = obj.indexes[dim]\n                 except KeyError:\n@@ -319,9 +322,13 @@ def align(\n             ):\n                 if join == \"exact\":\n                     raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n+\n                 index = joiner(matching_indexes)\n-                # make sure str coords are not cast to object\n-                index = maybe_coerce_to_str(index, all_coords[dim])\n+                # Check if any coordinates are dask arrays before coercing\n+                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n+                if not has_dask_coords:\n+                    # make sure str coords are not cast to object\n+                    index = maybe_coerce_to_str(index, all_coords[dim])\n                 joined_indexes[dim] = index\n             else:\n                 index = all_coords[dim][0]\n@@ -354,6 +361,7 @@ def align(\n             # fast path for no reindexing necessary\n             new_obj = obj.copy(deep=copy)\n         else:\n+\n             new_obj = obj.reindex(\n                 copy=copy, fill_value=fill_value, indexers=valid_indexers\n             )\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..3c3896f3 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3880,6 +3880,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 result = result._unstack_once(dim, fill_value)\n         return result\n \n+\n     def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n         \"\"\"Update this dataset's variables with those from another dataset.\n \n@@ -3906,8 +3907,27 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             If any dimensions would have inconsistent sizes in the updated\n             dataset.\n         \"\"\"\n+        # Store chunking information before merge\n+        chunk_info = {}\n+        if hasattr(other, 'items'):\n+            for key, value in other.items():\n+                if hasattr(value, 'data') and hasattr(value.data, 'chunks'):\n+                    chunk_info[key] = value.chunks\n+                elif isinstance(value, tuple) and len(value) == 2:\n+                    # Handle (dims, data) format\n+                    dims, data = value\n+                    if hasattr(data, 'chunks'):\n+                        chunk_info[key] = data.chunks\n+        \n         merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+        result = self._replace(inplace=True, **merge_result._asdict())\n+        \n+        # Restore chunking for variables that lost it\n+        for name, chunks in chunk_info.items():\n+            if name in result.variables and not hasattr(result.variables[name].data, 'chunks'):\n+                result = result.chunk({name: chunks})\n+        \n+        return result\n \n     def merge(\n         self,\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..0a83bd1d 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -905,6 +905,7 @@ def dataset_merge_method(\n     )\n \n \n+\n def dataset_update_method(\n     dataset: \"Dataset\", other: \"CoercibleMapping\"\n ) -> _MergeResult:\n",
    "variant_0": "diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py\nindex debf3aad..f573a0fe 100644\n--- a/xarray/core/alignment.py\n+++ b/xarray/core/alignment.py\n@@ -280,16 +280,19 @@ def align(\n     all_indexes = defaultdict(list)\n     all_coords = defaultdict(list)\n     unlabeled_dim_sizes = defaultdict(set)\n-    for obj in objects:\n-        for dim in obj.dims:\n-            if dim not in exclude:\n-                all_coords[dim].append(obj.coords[dim])\n-                try:\n-                    index = obj.indexes[dim]\n-                except KeyError:\n-                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n-                else:\n-                    all_indexes[dim].append(index)\n+\n+for obj in objects:\n+    for dim in obj.dims:\n+        if dim not in exclude:\n+            coord = obj.coords[dim]\n+            # Preserve dask arrays by avoiding operations that trigger computation\n+            all_coords[dim].append(coord)\n+            try:\n+                index = obj.indexes[dim]\n+            except KeyError:\n+                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n+            else:\n+                all_indexes[dim].append(index)\n \n     if join == \"override\":\n         objects = _override_indexes(objects, all_indexes, exclude)\n@@ -310,6 +313,16 @@ def align(\n             ):\n                 joined_indexes[dim] = indexes[dim]\n         else:\n+            if (\n+                any(\n+                    not matching_indexes[0].equals(other)\n+                    for other in matching_indexes[1:]\n+                )\n+                or dim in unlabeled_dim_sizes\n+            ):\n+                if join == \"exact\":\n+                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n+\n             if (\n                 any(\n                     not matching_indexes[0].equals(other)\n@@ -320,8 +333,11 @@ def align(\n                 if join == \"exact\":\n                     raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                 index = joiner(matching_indexes)\n-                # make sure str coords are not cast to object\n-                index = maybe_coerce_to_str(index, all_coords[dim])\n+                # Check if any coordinates are dask arrays before coercing to avoid triggering computation\n+                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n+                if not has_dask_coords:\n+                    # make sure str coords are not cast to object\n+                    index = maybe_coerce_to_str(index, all_coords[dim])\n                 joined_indexes[dim] = index\n             else:\n                 index = all_coords[dim][0]\n@@ -354,8 +370,12 @@ def align(\n             # fast path for no reindexing necessary\n             new_obj = obj.copy(deep=copy)\n         else:\n+\n             new_obj = obj.reindex(\n-                copy=copy, fill_value=fill_value, indexers=valid_indexers\n+                copy=copy, \n+                fill_value=fill_value, \n+                indexers=valid_indexers,\n+                method=None\n             )\n         new_obj.encoding = obj.encoding\n         result.append(new_obj)\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..a76c546b 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3880,34 +3880,55 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 result = result._unstack_once(dim, fill_value)\n         return result\n \n-    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n-        \"\"\"Update this dataset's variables with those from another dataset.\n \n-        Parameters\n-        ----------\n-        other : Dataset or mapping\n-            Variables with which to update this dataset. One of:\n+def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n+    \"\"\"Update this dataset's variables with those from another dataset.\n \n-            - Dataset\n-            - mapping {var name: DataArray}\n-            - mapping {var name: Variable}\n-            - mapping {var name: (dimension name, array-like)}\n-            - mapping {var name: (tuple of dimension names, array-like)}\n+    Parameters\n+    ----------\n+    other : Dataset or mapping\n+        Variables with which to update this dataset. One of:\n \n+        - Dataset\n+        - mapping {var name: DataArray}\n+        - mapping {var name: Variable}\n+        - mapping {var name: (dimension name, array-like)}\n+        - mapping {var name: (tuple of dimension names, array-like)}\n \n-        Returns\n-        -------\n-        updated : Dataset\n-            Updated dataset.\n \n-        Raises\n-        ------\n-        ValueError\n-            If any dimensions would have inconsistent sizes in the updated\n-            dataset.\n-        \"\"\"\n-        merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+    Returns\n+    -------\n+    updated : Dataset\n+        Updated dataset.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If any dimensions would have inconsistent sizes in the updated\n+        dataset.\n+    \"\"\"\n+    # Check if any variables in the update contain dask arrays\n+    preserve_chunks = {}\n+    if hasattr(other, 'items'):\n+        for name, var in other.items():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    elif hasattr(other, 'variables'):\n+        for name, var in other.variables.items():\n+            if hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    \n+    merge_result = dataset_update_method(self, other)\n+    result = self._replace(inplace=True, **merge_result._asdict())\n+    \n+    # Ensure dask arrays remain chunked after update\n+    if preserve_chunks:\n+        for name, chunks in preserve_chunks.items():\n+            if name in result.variables and not hasattr(result[name].data, 'chunks'):\n+                # Restore chunking if it was lost\n+                result = result.chunk({name: chunks})\n+    \n+    return result\n \n     def merge(\n         self,\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..e952d674 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -905,6 +905,7 @@ def dataset_merge_method(\n     )\n \n \n+\n def dataset_update_method(\n     dataset: \"Dataset\", other: \"CoercibleMapping\"\n ) -> _MergeResult:\n@@ -928,7 +929,12 @@ def dataset_update_method(\n                     if c not in value.dims and c in dataset.coords\n                 ]\n                 if coord_names:\n-                    other[key] = value.drop_vars(coord_names)\n+                    # Preserve dask arrays by creating a new DataArray without triggering computation\n+                    dropped_value = value.drop_vars(coord_names)\n+                    # Ensure chunking is preserved if original value was chunked\n+                    if hasattr(value.data, 'chunks') and not hasattr(dropped_value.data, 'chunks'):\n+                        dropped_value = dropped_value.chunk(value.chunks)\n+                    other[key] = dropped_value\n \n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n",
    "variant_1": "diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py\nindex debf3aad..e03d9f9d 100644\n--- a/xarray/core/alignment.py\n+++ b/xarray/core/alignment.py\n@@ -280,16 +280,19 @@ def align(\n     all_indexes = defaultdict(list)\n     all_coords = defaultdict(list)\n     unlabeled_dim_sizes = defaultdict(set)\n-    for obj in objects:\n-        for dim in obj.dims:\n-            if dim not in exclude:\n-                all_coords[dim].append(obj.coords[dim])\n-                try:\n-                    index = obj.indexes[dim]\n-                except KeyError:\n-                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n-                else:\n-                    all_indexes[dim].append(index)\n+\n+for obj in objects:\n+    for dim in obj.dims:\n+        if dim not in exclude:\n+            coord = obj.coords[dim]\n+            # Preserve dask arrays by avoiding operations that trigger computation\n+            all_coords[dim].append(coord)\n+            try:\n+                index = obj.indexes[dim]\n+            except KeyError:\n+                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n+            else:\n+                all_indexes[dim].append(index)\n \n     if join == \"override\":\n         objects = _override_indexes(objects, all_indexes, exclude)\n@@ -319,12 +322,25 @@ def align(\n             ):\n                 if join == \"exact\":\n                     raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n-                index = joiner(matching_indexes)\n-                # make sure str coords are not cast to object\n-                index = maybe_coerce_to_str(index, all_coords[dim])\n-                joined_indexes[dim] = index\n+\n             else:\n-                index = all_coords[dim][0]\n+                if (\n+                    any(\n+                        not matching_indexes[0].equals(other)\n+                        for other in matching_indexes[1:]\n+                    )\n+                    or dim in unlabeled_dim_sizes\n+                ):\n+                    if join == \"exact\":\n+                        raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n+                    index = joiner(matching_indexes)\n+                    # Check if any coordinates are dask arrays before coercing\n+                    has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n+                    if not has_dask_coords:\n+                        index = maybe_coerce_to_str(index, all_coords[dim])\n+                    joined_indexes[dim] = index\n+                else:\n+                    index = all_coords[dim][0]\n \n         if dim in unlabeled_dim_sizes:\n             unlabeled_sizes = unlabeled_dim_sizes[dim]\n@@ -354,8 +370,10 @@ def align(\n             # fast path for no reindexing necessary\n             new_obj = obj.copy(deep=copy)\n         else:\n+\n             new_obj = obj.reindex(\n-                copy=copy, fill_value=fill_value, indexers=valid_indexers\n+                copy=copy, fill_value=fill_value, indexers=valid_indexers,\n+                method=None  # Preserve dask arrays by avoiding interpolation\n             )\n         new_obj.encoding = obj.encoding\n         result.append(new_obj)\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..fad1d880 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3880,34 +3880,51 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 result = result._unstack_once(dim, fill_value)\n         return result\n \n-    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n-        \"\"\"Update this dataset's variables with those from another dataset.\n \n-        Parameters\n-        ----------\n-        other : Dataset or mapping\n-            Variables with which to update this dataset. One of:\n+def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n+    \"\"\"Update this dataset's variables with those from another dataset.\n \n-            - Dataset\n-            - mapping {var name: DataArray}\n-            - mapping {var name: Variable}\n-            - mapping {var name: (dimension name, array-like)}\n-            - mapping {var name: (tuple of dimension names, array-like)}\n+    Parameters\n+    ----------\n+    other : Dataset or mapping\n+        Variables with which to update this dataset. One of:\n \n+        - Dataset\n+        - mapping {var name: DataArray}\n+        - mapping {var name: Variable}\n+        - mapping {var name: (dimension name, array-like)}\n+        - mapping {var name: (tuple of dimension names, array-like)}\n \n-        Returns\n-        -------\n-        updated : Dataset\n-            Updated dataset.\n \n-        Raises\n-        ------\n-        ValueError\n-            If any dimensions would have inconsistent sizes in the updated\n-            dataset.\n-        \"\"\"\n-        merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+    Returns\n+    -------\n+    updated : Dataset\n+        Updated dataset.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If any dimensions would have inconsistent sizes in the updated\n+        dataset.\n+    \"\"\"\n+    # Check if any variables in the update contain dask arrays\n+    preserve_chunks = {}\n+    if hasattr(other, 'items'):\n+        for name, var in other.items():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    \n+    merge_result = dataset_update_method(self, other)\n+    result = self._replace(inplace=True, **merge_result._asdict())\n+    \n+    # Ensure dask arrays remain chunked after update\n+    if preserve_chunks:\n+        for name, chunks in preserve_chunks.items():\n+            if name in result.variables and not hasattr(result.variables[name].data, 'chunks'):\n+                # Restore chunking if it was lost during the merge process\n+                result = result.chunk({name: chunks})\n+    \n+    return result\n \n     def merge(\n         self,\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..29194a55 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -905,6 +905,7 @@ def dataset_merge_method(\n     )\n \n \n+\n def dataset_update_method(\n     dataset: \"Dataset\", other: \"CoercibleMapping\"\n ) -> _MergeResult:\n@@ -932,6 +933,20 @@ def dataset_update_method(\n \n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n+    \n+    # Check if any variables contain dask arrays to preserve chunking\n+    has_dask_data = False\n+    if isinstance(other, dict):\n+        for var in other.values():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                has_dask_data = True\n+                break\n+    elif hasattr(other, 'data_vars'):\n+        for var in other.data_vars.values():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                has_dask_data = True\n+                break\n+    \n     return merge_core(\n         [dataset, other],\n         priority_arg=1,\n",
    "variant_2": "diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py\nindex debf3aad..db683bd8 100644\n--- a/xarray/core/alignment.py\n+++ b/xarray/core/alignment.py\n@@ -280,10 +280,13 @@ def align(\n     all_indexes = defaultdict(list)\n     all_coords = defaultdict(list)\n     unlabeled_dim_sizes = defaultdict(set)\n+\n     for obj in objects:\n         for dim in obj.dims:\n             if dim not in exclude:\n-                all_coords[dim].append(obj.coords[dim])\n+                coord = obj.coords[dim]\n+                # Preserve dask arrays by avoiding operations that trigger computation\n+                all_coords[dim].append(coord)\n                 try:\n                     index = obj.indexes[dim]\n                 except KeyError:\n@@ -319,12 +322,21 @@ def align(\n             ):\n                 if join == \"exact\":\n                     raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n+\n                 index = joiner(matching_indexes)\n                 # make sure str coords are not cast to object\n-                index = maybe_coerce_to_str(index, all_coords[dim])\n+                # Check if any coordinates are dask arrays before coercing\n+                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n+                if not has_dask_coords:\n+                    index = maybe_coerce_to_str(index, all_coords[dim])\n                 joined_indexes[dim] = index\n             else:\n-                index = all_coords[dim][0]\n+                # For dask arrays, avoid operations that might trigger computation\n+                coord = all_coords[dim][0]\n+                if hasattr(coord.data, 'chunks'):\n+                    index = coord\n+                else:\n+                    index = all_coords[dim][0]\n \n         if dim in unlabeled_dim_sizes:\n             unlabeled_sizes = unlabeled_dim_sizes[dim]\n@@ -354,6 +366,7 @@ def align(\n             # fast path for no reindexing necessary\n             new_obj = obj.copy(deep=copy)\n         else:\n+\n             new_obj = obj.reindex(\n                 copy=copy, fill_value=fill_value, indexers=valid_indexers\n             )\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..f19904ad 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3880,34 +3880,51 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 result = result._unstack_once(dim, fill_value)\n         return result\n \n-    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n-        \"\"\"Update this dataset's variables with those from another dataset.\n \n-        Parameters\n-        ----------\n-        other : Dataset or mapping\n-            Variables with which to update this dataset. One of:\n+def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n+    \"\"\"Update this dataset's variables with those from another dataset.\n \n-            - Dataset\n-            - mapping {var name: DataArray}\n-            - mapping {var name: Variable}\n-            - mapping {var name: (dimension name, array-like)}\n-            - mapping {var name: (tuple of dimension names, array-like)}\n+    Parameters\n+    ----------\n+    other : Dataset or mapping\n+        Variables with which to update this dataset. One of:\n \n+        - Dataset\n+        - mapping {var name: DataArray}\n+        - mapping {var name: Variable}\n+        - mapping {var name: (dimension name, array-like)}\n+        - mapping {var name: (tuple of dimension names, array-like)}\n \n-        Returns\n-        -------\n-        updated : Dataset\n-            Updated dataset.\n \n-        Raises\n-        ------\n-        ValueError\n-            If any dimensions would have inconsistent sizes in the updated\n-            dataset.\n-        \"\"\"\n-        merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+    Returns\n+    -------\n+    updated : Dataset\n+        Updated dataset.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If any dimensions would have inconsistent sizes in the updated\n+        dataset.\n+    \"\"\"\n+    # Check if any variables in the update contain dask arrays\n+    preserve_chunks = {}\n+    if hasattr(other, 'items'):\n+        for name, var in other.items():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    \n+    merge_result = dataset_update_method(self, other)\n+    result = self._replace(inplace=True, **merge_result._asdict())\n+    \n+    # Ensure dask arrays remain chunked after update\n+    if preserve_chunks:\n+        for name, chunks in preserve_chunks.items():\n+            if name in result.variables and not hasattr(result.variables[name].data, 'chunks'):\n+                # Restore chunking if it was lost during merge\n+                result = result.chunk({name: chunks})\n+    \n+    return result\n \n     def merge(\n         self,\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..46234e84 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -905,6 +905,7 @@ def dataset_merge_method(\n     )\n \n \n+\n def dataset_update_method(\n     dataset: \"Dataset\", other: \"CoercibleMapping\"\n ) -> _MergeResult:\n@@ -932,6 +933,15 @@ def dataset_update_method(\n \n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n+    \n+    # Check if we're dealing with dask arrays and preserve chunking\n+    has_dask_data = False\n+    if hasattr(other, 'items'):\n+        for var in other.values():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                has_dask_data = True\n+                break\n+    \n     return merge_core(\n         [dataset, other],\n         priority_arg=1,\n",
    "variant_3": "diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py\nindex debf3aad..85c21091 100644\n--- a/xarray/core/alignment.py\n+++ b/xarray/core/alignment.py\n@@ -280,16 +280,19 @@ def align(\n     all_indexes = defaultdict(list)\n     all_coords = defaultdict(list)\n     unlabeled_dim_sizes = defaultdict(set)\n-    for obj in objects:\n-        for dim in obj.dims:\n-            if dim not in exclude:\n-                all_coords[dim].append(obj.coords[dim])\n-                try:\n-                    index = obj.indexes[dim]\n-                except KeyError:\n-                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n-                else:\n-                    all_indexes[dim].append(index)\n+\n+for obj in objects:\n+    for dim in obj.dims:\n+        if dim not in exclude:\n+            coord = obj.coords[dim]\n+            # Preserve dask arrays by avoiding operations that trigger computation\n+            all_coords[dim].append(coord)\n+            try:\n+                index = obj.indexes[dim]\n+            except KeyError:\n+                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n+            else:\n+                all_indexes[dim].append(index)\n \n     if join == \"override\":\n         objects = _override_indexes(objects, all_indexes, exclude)\n@@ -319,12 +322,21 @@ def align(\n             ):\n                 if join == \"exact\":\n                     raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n+\n                 index = joiner(matching_indexes)\n                 # make sure str coords are not cast to object\n-                index = maybe_coerce_to_str(index, all_coords[dim])\n+                # Check if any coordinates are dask arrays before coercing\n+                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n+                if not has_dask_coords:\n+                    index = maybe_coerce_to_str(index, all_coords[dim])\n                 joined_indexes[dim] = index\n             else:\n-                index = all_coords[dim][0]\n+                # For dask arrays, avoid operations that might trigger computation\n+                coord = all_coords[dim][0]\n+                if hasattr(coord.data, 'chunks'):\n+                    index = coord\n+                else:\n+                    index = all_coords[dim][0]\n \n         if dim in unlabeled_dim_sizes:\n             unlabeled_sizes = unlabeled_dim_sizes[dim]\n@@ -354,9 +366,25 @@ def align(\n             # fast path for no reindexing necessary\n             new_obj = obj.copy(deep=copy)\n         else:\n+\n+            # Check if reindexing involves dask arrays and preserve chunking\n+            has_dask_data = any(\n+                hasattr(getattr(obj.variables.get(k), 'data', None), 'chunks')\n+                for k in valid_indexers.keys()\n+            )\n+            \n             new_obj = obj.reindex(\n                 copy=copy, fill_value=fill_value, indexers=valid_indexers\n             )\n+            \n+            # Restore dask chunking if it was lost during reindexing\n+            if has_dask_data:\n+                for var_name, var in new_obj.variables.items():\n+                    orig_var = obj.variables.get(var_name)\n+                    if (orig_var is not None and \n+                        hasattr(orig_var.data, 'chunks') and \n+                        not hasattr(var.data, 'chunks')):\n+                        new_obj[var_name] = new_obj[var_name].chunk(orig_var.chunks)\n         new_obj.encoding = obj.encoding\n         result.append(new_obj)\n \ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..1aaf5e1c 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3880,34 +3880,52 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 result = result._unstack_once(dim, fill_value)\n         return result\n \n-    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n-        \"\"\"Update this dataset's variables with those from another dataset.\n \n-        Parameters\n-        ----------\n-        other : Dataset or mapping\n-            Variables with which to update this dataset. One of:\n+def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n+    \"\"\"Update this dataset's variables with those from another dataset.\n \n-            - Dataset\n-            - mapping {var name: DataArray}\n-            - mapping {var name: Variable}\n-            - mapping {var name: (dimension name, array-like)}\n-            - mapping {var name: (tuple of dimension names, array-like)}\n+    Parameters\n+    ----------\n+    other : Dataset or mapping\n+        Variables with which to update this dataset. One of:\n \n+        - Dataset\n+        - mapping {var name: DataArray}\n+        - mapping {var name: Variable}\n+        - mapping {var name: (dimension name, array-like)}\n+        - mapping {var name: (tuple of dimension names, array-like)}\n \n-        Returns\n-        -------\n-        updated : Dataset\n-            Updated dataset.\n \n-        Raises\n-        ------\n-        ValueError\n-            If any dimensions would have inconsistent sizes in the updated\n-            dataset.\n-        \"\"\"\n-        merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+    Returns\n+    -------\n+    updated : Dataset\n+        Updated dataset.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If any dimensions would have inconsistent sizes in the updated\n+        dataset.\n+    \"\"\"\n+    # Check if any variables in the update contain dask arrays\n+    preserve_chunks = {}\n+    if hasattr(other, 'values'):\n+        for name, var in other.items():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    \n+    merge_result = dataset_update_method(self, other)\n+    result = self._replace(inplace=True, **merge_result._asdict())\n+    \n+    # Ensure dask arrays remain chunked after update\n+    for name, chunks in preserve_chunks.items():\n+        if name in result.variables:\n+            var = result.variables[name]\n+            if not hasattr(var.data, 'chunks'):\n+                # Restore chunking if it was lost during merge\n+                result = result.chunk({name: chunks})\n+    \n+    return result\n \n     def merge(\n         self,\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..c3aa1e79 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -905,6 +905,7 @@ def dataset_merge_method(\n     )\n \n \n+\n def dataset_update_method(\n     dataset: \"Dataset\", other: \"CoercibleMapping\"\n ) -> _MergeResult:\n@@ -932,6 +933,15 @@ def dataset_update_method(\n \n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n+    \n+    # Check if we're dealing with dask arrays to preserve chunking\n+    has_dask_data = False\n+    if hasattr(other, 'items'):\n+        for var in other.values():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                has_dask_data = True\n+                break\n+    \n     return merge_core(\n         [dataset, other],\n         priority_arg=1,\n",
    "variant_4": "diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py\nindex debf3aad..ceb5d5a9 100644\n--- a/xarray/core/alignment.py\n+++ b/xarray/core/alignment.py\n@@ -280,10 +280,13 @@ def align(\n     all_indexes = defaultdict(list)\n     all_coords = defaultdict(list)\n     unlabeled_dim_sizes = defaultdict(set)\n+\n     for obj in objects:\n         for dim in obj.dims:\n             if dim not in exclude:\n-                all_coords[dim].append(obj.coords[dim])\n+                coord = obj.coords[dim]\n+                # Preserve dask arrays by avoiding operations that trigger computation\n+                all_coords[dim].append(coord)\n                 try:\n                     index = obj.indexes[dim]\n                 except KeyError:\n@@ -309,6 +312,17 @@ def align(\n                 or dim in unlabeled_dim_sizes\n             ):\n                 joined_indexes[dim] = indexes[dim]\n+        else:\n+            if (\n+                any(\n+                    not matching_indexes[0].equals(other)\n+                    for other in matching_indexes[1:]\n+                )\n+                or dim in unlabeled_dim_sizes\n+            ):\n+                if join == \"exact\":\n+                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n+\n         else:\n             if (\n                 any(\n@@ -320,11 +334,18 @@ def align(\n                 if join == \"exact\":\n                     raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                 index = joiner(matching_indexes)\n-                # make sure str coords are not cast to object\n-                index = maybe_coerce_to_str(index, all_coords[dim])\n+                # Check if any coordinates are dask arrays before coercing\n+                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n+                if not has_dask_coords:\n+                    # make sure str coords are not cast to object\n+                    index = maybe_coerce_to_str(index, all_coords[dim])\n                 joined_indexes[dim] = index\n             else:\n-                index = all_coords[dim][0]\n+                coord = all_coords[dim][0]\n+                if hasattr(coord.data, 'chunks'):\n+                    index = coord.to_index()  # Use dask-safe index creation\n+                else:\n+                    index = all_coords[dim][0]\n \n         if dim in unlabeled_dim_sizes:\n             unlabeled_sizes = unlabeled_dim_sizes[dim]\n@@ -354,8 +375,10 @@ def align(\n             # fast path for no reindexing necessary\n             new_obj = obj.copy(deep=copy)\n         else:\n+\n             new_obj = obj.reindex(\n-                copy=copy, fill_value=fill_value, indexers=valid_indexers\n+                copy=copy, fill_value=fill_value, indexers=valid_indexers,\n+                method=None\n             )\n         new_obj.encoding = obj.encoding\n         result.append(new_obj)\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..f25310f5 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3880,34 +3880,57 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 result = result._unstack_once(dim, fill_value)\n         return result\n \n-    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n-        \"\"\"Update this dataset's variables with those from another dataset.\n \n-        Parameters\n-        ----------\n-        other : Dataset or mapping\n-            Variables with which to update this dataset. One of:\n+def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n+    \"\"\"Update this dataset's variables with those from another dataset.\n \n-            - Dataset\n-            - mapping {var name: DataArray}\n-            - mapping {var name: Variable}\n-            - mapping {var name: (dimension name, array-like)}\n-            - mapping {var name: (tuple of dimension names, array-like)}\n+    Parameters\n+    ----------\n+    other : Dataset or mapping\n+        Variables with which to update this dataset. One of:\n \n+        - Dataset\n+        - mapping {var name: DataArray}\n+        - mapping {var name: Variable}\n+        - mapping {var name: (dimension name, array-like)}\n+        - mapping {var name: (tuple of dimension names, array-like)}\n \n-        Returns\n-        -------\n-        updated : Dataset\n-            Updated dataset.\n \n-        Raises\n-        ------\n-        ValueError\n-            If any dimensions would have inconsistent sizes in the updated\n-            dataset.\n-        \"\"\"\n-        merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+    Returns\n+    -------\n+    updated : Dataset\n+        Updated dataset.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If any dimensions would have inconsistent sizes in the updated\n+        dataset.\n+    \"\"\"\n+    # Check if any variables in the update contain dask arrays\n+    preserve_chunks = {}\n+    if hasattr(other, 'items'):\n+        for name, var in other.items():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    elif hasattr(other, 'variables'):\n+        for name, var in other.variables.items():\n+            if hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    \n+    merge_result = dataset_update_method(self, other)\n+    result = self._replace(inplace=True, **merge_result._asdict())\n+    \n+    # Ensure dask arrays remain chunked after update\n+    if preserve_chunks:\n+        for name, chunks in preserve_chunks.items():\n+            if name in result.variables:\n+                var = result.variables[name]\n+                if not hasattr(var.data, 'chunks'):\n+                    # Restore chunking if it was lost during merge\n+                    result = result.assign({name: result[name].chunk(chunks)})\n+    \n+    return result\n \n     def merge(\n         self,\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..7d816291 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -905,6 +905,7 @@ def dataset_merge_method(\n     )\n \n \n+\n def dataset_update_method(\n     dataset: \"Dataset\", other: \"CoercibleMapping\"\n ) -> _MergeResult:\n@@ -932,6 +933,21 @@ def dataset_update_method(\n \n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n+    \n+    # Check if we're dealing with dask arrays to preserve chunking\n+    has_dask_data = False\n+    if isinstance(other, dict):\n+        has_dask_data = any(\n+            hasattr(getattr(var, 'data', None), 'chunks')\n+            for var in other.values()\n+            if hasattr(var, 'data')\n+        )\n+    elif hasattr(other, 'data_vars'):\n+        has_dask_data = any(\n+            hasattr(var.data, 'chunks')\n+            for var in other.data_vars.values()\n+        )\n+    \n     return merge_core(\n         [dataset, other],\n         priority_arg=1,\n",
    "variant_5": "diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py\nindex debf3aad..d827076e 100644\n--- a/xarray/core/alignment.py\n+++ b/xarray/core/alignment.py\n@@ -280,16 +280,19 @@ def align(\n     all_indexes = defaultdict(list)\n     all_coords = defaultdict(list)\n     unlabeled_dim_sizes = defaultdict(set)\n-    for obj in objects:\n-        for dim in obj.dims:\n-            if dim not in exclude:\n-                all_coords[dim].append(obj.coords[dim])\n-                try:\n-                    index = obj.indexes[dim]\n-                except KeyError:\n-                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n-                else:\n-                    all_indexes[dim].append(index)\n+\n+for obj in objects:\n+    for dim in obj.dims:\n+        if dim not in exclude:\n+            coord = obj.coords[dim]\n+            # Preserve dask arrays by avoiding operations that trigger computation\n+            all_coords[dim].append(coord)\n+            try:\n+                index = obj.indexes[dim]\n+            except KeyError:\n+                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n+            else:\n+                all_indexes[dim].append(index)\n \n     if join == \"override\":\n         objects = _override_indexes(objects, all_indexes, exclude)\n@@ -317,11 +320,17 @@ def align(\n                 )\n                 or dim in unlabeled_dim_sizes\n             ):\n+                if join == \"exact\":\n+                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n+\n                 if join == \"exact\":\n                     raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                 index = joiner(matching_indexes)\n-                # make sure str coords are not cast to object\n-                index = maybe_coerce_to_str(index, all_coords[dim])\n+                # Check if any coordinates are dask arrays before coercing\n+                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n+                if not has_dask_coords:\n+                    # make sure str coords are not cast to object\n+                    index = maybe_coerce_to_str(index, all_coords[dim])\n                 joined_indexes[dim] = index\n             else:\n                 index = all_coords[dim][0]\n@@ -354,8 +363,10 @@ def align(\n             # fast path for no reindexing necessary\n             new_obj = obj.copy(deep=copy)\n         else:\n+\n             new_obj = obj.reindex(\n-                copy=copy, fill_value=fill_value, indexers=valid_indexers\n+                copy=copy, fill_value=fill_value, indexers=valid_indexers,\n+                method=None  # Preserve dask arrays by avoiding interpolation\n             )\n         new_obj.encoding = obj.encoding\n         result.append(new_obj)\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..dd90352c 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3880,34 +3880,55 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 result = result._unstack_once(dim, fill_value)\n         return result\n \n-    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n-        \"\"\"Update this dataset's variables with those from another dataset.\n \n-        Parameters\n-        ----------\n-        other : Dataset or mapping\n-            Variables with which to update this dataset. One of:\n+def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n+    \"\"\"Update this dataset's variables with those from another dataset.\n \n-            - Dataset\n-            - mapping {var name: DataArray}\n-            - mapping {var name: Variable}\n-            - mapping {var name: (dimension name, array-like)}\n-            - mapping {var name: (tuple of dimension names, array-like)}\n+    Parameters\n+    ----------\n+    other : Dataset or mapping\n+        Variables with which to update this dataset. One of:\n \n+        - Dataset\n+        - mapping {var name: DataArray}\n+        - mapping {var name: Variable}\n+        - mapping {var name: (dimension name, array-like)}\n+        - mapping {var name: (tuple of dimension names, array-like)}\n \n-        Returns\n-        -------\n-        updated : Dataset\n-            Updated dataset.\n \n-        Raises\n-        ------\n-        ValueError\n-            If any dimensions would have inconsistent sizes in the updated\n-            dataset.\n-        \"\"\"\n-        merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+    Returns\n+    -------\n+    updated : Dataset\n+        Updated dataset.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If any dimensions would have inconsistent sizes in the updated\n+        dataset.\n+    \"\"\"\n+    # Check if any variables in the update contain dask arrays\n+    preserve_chunks = {}\n+    if hasattr(other, 'items'):\n+        for name, var in other.items():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    elif hasattr(other, 'variables'):\n+        for name, var in other.variables.items():\n+            if hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    \n+    merge_result = dataset_update_method(self, other)\n+    result = self._replace(inplace=True, **merge_result._asdict())\n+    \n+    # Ensure dask arrays remain chunked after update\n+    if preserve_chunks:\n+        for name, chunks in preserve_chunks.items():\n+            if name in result.variables and not hasattr(result.variables[name].data, 'chunks'):\n+                # Restore chunking if it was lost during merge\n+                result._variables[name] = result._variables[name].chunk(chunks)\n+    \n+    return result\n \n     def merge(\n         self,\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..f672b6f6 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -905,6 +905,7 @@ def dataset_merge_method(\n     )\n \n \n+\n def dataset_update_method(\n     dataset: \"Dataset\", other: \"CoercibleMapping\"\n ) -> _MergeResult:\n@@ -932,6 +933,15 @@ def dataset_update_method(\n \n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n+    \n+    # Check if we're dealing with dask arrays and preserve their chunking\n+    has_dask_data = False\n+    if hasattr(other, 'items'):\n+        for var in other.values():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                has_dask_data = True\n+                break\n+    \n     return merge_core(\n         [dataset, other],\n         priority_arg=1,\n",
    "variant_6": "diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py\nindex debf3aad..4d2e4d93 100644\n--- a/xarray/core/alignment.py\n+++ b/xarray/core/alignment.py\n@@ -280,16 +280,19 @@ def align(\n     all_indexes = defaultdict(list)\n     all_coords = defaultdict(list)\n     unlabeled_dim_sizes = defaultdict(set)\n-    for obj in objects:\n-        for dim in obj.dims:\n-            if dim not in exclude:\n-                all_coords[dim].append(obj.coords[dim])\n-                try:\n-                    index = obj.indexes[dim]\n-                except KeyError:\n-                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n-                else:\n-                    all_indexes[dim].append(index)\n+\n+for obj in objects:\n+    for dim in obj.dims:\n+        if dim not in exclude:\n+            coord = obj.coords[dim]\n+            # Preserve dask arrays by avoiding operations that trigger computation\n+            all_coords[dim].append(coord)\n+            try:\n+                index = obj.indexes[dim]\n+            except KeyError:\n+                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n+            else:\n+                all_indexes[dim].append(index)\n \n     if join == \"override\":\n         objects = _override_indexes(objects, all_indexes, exclude)\n@@ -317,11 +320,17 @@ def align(\n                 )\n                 or dim in unlabeled_dim_sizes\n             ):\n+                if join == \"exact\":\n+                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n+\n                 if join == \"exact\":\n                     raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                 index = joiner(matching_indexes)\n-                # make sure str coords are not cast to object\n-                index = maybe_coerce_to_str(index, all_coords[dim])\n+                # Check if any coordinates are dask arrays before coercing\n+                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n+                if not has_dask_coords:\n+                    # make sure str coords are not cast to object\n+                    index = maybe_coerce_to_str(index, all_coords[dim])\n                 joined_indexes[dim] = index\n             else:\n                 index = all_coords[dim][0]\n@@ -354,6 +363,7 @@ def align(\n             # fast path for no reindexing necessary\n             new_obj = obj.copy(deep=copy)\n         else:\n+\n             new_obj = obj.reindex(\n                 copy=copy, fill_value=fill_value, indexers=valid_indexers\n             )\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..b8a8df17 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3880,34 +3880,52 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 result = result._unstack_once(dim, fill_value)\n         return result\n \n-    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n-        \"\"\"Update this dataset's variables with those from another dataset.\n \n-        Parameters\n-        ----------\n-        other : Dataset or mapping\n-            Variables with which to update this dataset. One of:\n+def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n+    \"\"\"Update this dataset's variables with those from another dataset.\n \n-            - Dataset\n-            - mapping {var name: DataArray}\n-            - mapping {var name: Variable}\n-            - mapping {var name: (dimension name, array-like)}\n-            - mapping {var name: (tuple of dimension names, array-like)}\n+    Parameters\n+    ----------\n+    other : Dataset or mapping\n+        Variables with which to update this dataset. One of:\n \n+        - Dataset\n+        - mapping {var name: DataArray}\n+        - mapping {var name: Variable}\n+        - mapping {var name: (dimension name, array-like)}\n+        - mapping {var name: (tuple of dimension names, array-like)}\n \n-        Returns\n-        -------\n-        updated : Dataset\n-            Updated dataset.\n \n-        Raises\n-        ------\n-        ValueError\n-            If any dimensions would have inconsistent sizes in the updated\n-            dataset.\n-        \"\"\"\n-        merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+    Returns\n+    -------\n+    updated : Dataset\n+        Updated dataset.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If any dimensions would have inconsistent sizes in the updated\n+        dataset.\n+    \"\"\"\n+    # Check if any variables in the update contain dask arrays\n+    preserve_chunks = {}\n+    if hasattr(other, 'items'):\n+        for name, var in other.items():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    \n+    merge_result = dataset_update_method(self, other)\n+    result = self._replace(inplace=True, **merge_result._asdict())\n+    \n+    # Ensure dask arrays remain chunked after update\n+    for name, chunks in preserve_chunks.items():\n+        if name in result.variables:\n+            var = result.variables[name]\n+            if not hasattr(var.data, 'chunks'):\n+                # Restore chunking if it was lost during merge\n+                result = result.assign({name: result[name].chunk(chunks)})\n+    \n+    return result\n \n     def merge(\n         self,\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..b2d79e64 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -905,6 +905,7 @@ def dataset_merge_method(\n     )\n \n \n+\n def dataset_update_method(\n     dataset: \"Dataset\", other: \"CoercibleMapping\"\n ) -> _MergeResult:\n@@ -932,6 +933,20 @@ def dataset_update_method(\n \n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n+    \n+    # Check if we're dealing with dask arrays to preserve chunking\n+    has_dask_data = False\n+    if isinstance(other, dict):\n+        for var in other.values():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                has_dask_data = True\n+                break\n+    elif hasattr(other, 'data_vars'):\n+        for var in other.data_vars.values():\n+            if hasattr(var.data, 'chunks'):\n+                has_dask_data = True\n+                break\n+    \n     return merge_core(\n         [dataset, other],\n         priority_arg=1,\n",
    "variant_7": "diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py\nindex debf3aad..1109e70a 100644\n--- a/xarray/core/alignment.py\n+++ b/xarray/core/alignment.py\n@@ -280,16 +280,19 @@ def align(\n     all_indexes = defaultdict(list)\n     all_coords = defaultdict(list)\n     unlabeled_dim_sizes = defaultdict(set)\n-    for obj in objects:\n-        for dim in obj.dims:\n-            if dim not in exclude:\n-                all_coords[dim].append(obj.coords[dim])\n-                try:\n-                    index = obj.indexes[dim]\n-                except KeyError:\n-                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n-                else:\n-                    all_indexes[dim].append(index)\n+\n+for obj in objects:\n+    for dim in obj.dims:\n+        if dim not in exclude:\n+            coord = obj.coords[dim]\n+            # Preserve dask arrays by avoiding operations that trigger computation\n+            all_coords[dim].append(coord)\n+            try:\n+                index = obj.indexes[dim]\n+            except KeyError:\n+                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n+            else:\n+                all_indexes[dim].append(index)\n \n     if join == \"override\":\n         objects = _override_indexes(objects, all_indexes, exclude)\n@@ -309,6 +312,17 @@ def align(\n                 or dim in unlabeled_dim_sizes\n             ):\n                 joined_indexes[dim] = indexes[dim]\n+        else:\n+            if (\n+                any(\n+                    not matching_indexes[0].equals(other)\n+                    for other in matching_indexes[1:]\n+                )\n+                or dim in unlabeled_dim_sizes\n+            ):\n+                if join == \"exact\":\n+                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n+\n         else:\n             if (\n                 any(\n@@ -320,11 +334,18 @@ def align(\n                 if join == \"exact\":\n                     raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                 index = joiner(matching_indexes)\n-                # make sure str coords are not cast to object\n-                index = maybe_coerce_to_str(index, all_coords[dim])\n+                # Check if any coordinates are dask arrays before coercing\n+                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n+                if not has_dask_coords:\n+                    # make sure str coords are not cast to object\n+                    index = maybe_coerce_to_str(index, all_coords[dim])\n                 joined_indexes[dim] = index\n             else:\n-                index = all_coords[dim][0]\n+                coord = all_coords[dim][0]\n+                if hasattr(coord.data, 'chunks'):\n+                    index = coord.to_index()  # Use dask-safe index creation\n+                else:\n+                    index = all_coords[dim][0]\n \n         if dim in unlabeled_dim_sizes:\n             unlabeled_sizes = unlabeled_dim_sizes[dim]\n@@ -354,8 +375,12 @@ def align(\n             # fast path for no reindexing necessary\n             new_obj = obj.copy(deep=copy)\n         else:\n+\n             new_obj = obj.reindex(\n-                copy=copy, fill_value=fill_value, indexers=valid_indexers\n+                copy=copy, \n+                fill_value=fill_value, \n+                indexers=valid_indexers,\n+                method=None\n             )\n         new_obj.encoding = obj.encoding\n         result.append(new_obj)\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..2f5f6459 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3880,34 +3880,56 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 result = result._unstack_once(dim, fill_value)\n         return result\n \n-    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n-        \"\"\"Update this dataset's variables with those from another dataset.\n \n-        Parameters\n-        ----------\n-        other : Dataset or mapping\n-            Variables with which to update this dataset. One of:\n+def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n+    \"\"\"Update this dataset's variables with those from another dataset.\n \n-            - Dataset\n-            - mapping {var name: DataArray}\n-            - mapping {var name: Variable}\n-            - mapping {var name: (dimension name, array-like)}\n-            - mapping {var name: (tuple of dimension names, array-like)}\n+    Parameters\n+    ----------\n+    other : Dataset or mapping\n+        Variables with which to update this dataset. One of:\n \n+        - Dataset\n+        - mapping {var name: DataArray}\n+        - mapping {var name: Variable}\n+        - mapping {var name: (dimension name, array-like)}\n+        - mapping {var name: (tuple of dimension names, array-like)}\n \n-        Returns\n-        -------\n-        updated : Dataset\n-            Updated dataset.\n \n-        Raises\n-        ------\n-        ValueError\n-            If any dimensions would have inconsistent sizes in the updated\n-            dataset.\n-        \"\"\"\n-        merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+    Returns\n+    -------\n+    updated : Dataset\n+        Updated dataset.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If any dimensions would have inconsistent sizes in the updated\n+        dataset.\n+    \"\"\"\n+    # Check if any variables in the update contain dask arrays\n+    preserve_chunks = {}\n+    if hasattr(other, 'data_vars'):\n+        # other is a Dataset\n+        for name, var in other.data_vars.items():\n+            if hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    elif hasattr(other, 'items'):\n+        # other is a mapping\n+        for name, var in other.items():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    \n+    merge_result = dataset_update_method(self, other)\n+    result = self._replace(inplace=True, **merge_result._asdict())\n+    \n+    # Ensure dask arrays remain chunked after update\n+    for name, chunks in preserve_chunks.items():\n+        if name in result.data_vars and not hasattr(result[name].data, 'chunks'):\n+            # Restore chunking if it was lost during the merge process\n+            result = result.assign({name: result[name].chunk(chunks)})\n+    \n+    return result\n \n     def merge(\n         self,\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..039f86bc 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -905,6 +905,7 @@ def dataset_merge_method(\n     )\n \n \n+\n def dataset_update_method(\n     dataset: \"Dataset\", other: \"CoercibleMapping\"\n ) -> _MergeResult:\n@@ -932,6 +933,20 @@ def dataset_update_method(\n \n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n+    \n+    # Check if we have dask arrays to preserve during merge\n+    has_dask_data = False\n+    if isinstance(other, dict):\n+        for var in other.values():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                has_dask_data = True\n+                break\n+    elif hasattr(other, 'data_vars'):\n+        for var in other.data_vars.values():\n+            if hasattr(var.data, 'chunks'):\n+                has_dask_data = True\n+                break\n+    \n     return merge_core(\n         [dataset, other],\n         priority_arg=1,\n",
    "variant_8": "diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py\nindex debf3aad..a595f61e 100644\n--- a/xarray/core/alignment.py\n+++ b/xarray/core/alignment.py\n@@ -280,16 +280,19 @@ def align(\n     all_indexes = defaultdict(list)\n     all_coords = defaultdict(list)\n     unlabeled_dim_sizes = defaultdict(set)\n-    for obj in objects:\n-        for dim in obj.dims:\n-            if dim not in exclude:\n-                all_coords[dim].append(obj.coords[dim])\n-                try:\n-                    index = obj.indexes[dim]\n-                except KeyError:\n-                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n-                else:\n-                    all_indexes[dim].append(index)\n+\n+for obj in objects:\n+    for dim in obj.dims:\n+        if dim not in exclude:\n+            coord = obj.coords[dim]\n+            # Preserve dask arrays by avoiding operations that trigger computation\n+            all_coords[dim].append(coord)\n+            try:\n+                index = obj.indexes[dim]\n+            except KeyError:\n+                unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n+            else:\n+                all_indexes[dim].append(index)\n \n     if join == \"override\":\n         objects = _override_indexes(objects, all_indexes, exclude)\n@@ -317,11 +320,17 @@ def align(\n                 )\n                 or dim in unlabeled_dim_sizes\n             ):\n+                if join == \"exact\":\n+                    raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n+\n                 if join == \"exact\":\n                     raise ValueError(f\"indexes along dimension {dim!r} are not equal\")\n                 index = joiner(matching_indexes)\n-                # make sure str coords are not cast to object\n-                index = maybe_coerce_to_str(index, all_coords[dim])\n+                # Check if any coordinates are dask arrays before coercing\n+                has_dask_coords = any(hasattr(coord.data, 'chunks') for coord in all_coords[dim])\n+                if not has_dask_coords:\n+                    # make sure str coords are not cast to object\n+                    index = maybe_coerce_to_str(index, all_coords[dim])\n                 joined_indexes[dim] = index\n             else:\n                 index = all_coords[dim][0]\n@@ -354,8 +363,12 @@ def align(\n             # fast path for no reindexing necessary\n             new_obj = obj.copy(deep=copy)\n         else:\n+\n             new_obj = obj.reindex(\n-                copy=copy, fill_value=fill_value, indexers=valid_indexers\n+                copy=copy, \n+                fill_value=fill_value, \n+                indexers=valid_indexers,\n+                method=None\n             )\n         new_obj.encoding = obj.encoding\n         result.append(new_obj)\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b487..77671d20 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3880,34 +3880,55 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 result = result._unstack_once(dim, fill_value)\n         return result\n \n-    def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n-        \"\"\"Update this dataset's variables with those from another dataset.\n \n-        Parameters\n-        ----------\n-        other : Dataset or mapping\n-            Variables with which to update this dataset. One of:\n+def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n+    \"\"\"Update this dataset's variables with those from another dataset.\n \n-            - Dataset\n-            - mapping {var name: DataArray}\n-            - mapping {var name: Variable}\n-            - mapping {var name: (dimension name, array-like)}\n-            - mapping {var name: (tuple of dimension names, array-like)}\n+    Parameters\n+    ----------\n+    other : Dataset or mapping\n+        Variables with which to update this dataset. One of:\n \n+        - Dataset\n+        - mapping {var name: DataArray}\n+        - mapping {var name: Variable}\n+        - mapping {var name: (dimension name, array-like)}\n+        - mapping {var name: (tuple of dimension names, array-like)}\n \n-        Returns\n-        -------\n-        updated : Dataset\n-            Updated dataset.\n \n-        Raises\n-        ------\n-        ValueError\n-            If any dimensions would have inconsistent sizes in the updated\n-            dataset.\n-        \"\"\"\n-        merge_result = dataset_update_method(self, other)\n-        return self._replace(inplace=True, **merge_result._asdict())\n+    Returns\n+    -------\n+    updated : Dataset\n+        Updated dataset.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If any dimensions would have inconsistent sizes in the updated\n+        dataset.\n+    \"\"\"\n+    # Check if any variables in the update contain dask arrays\n+    preserve_chunks = {}\n+    if hasattr(other, 'items'):\n+        for name, var in other.items():\n+            if hasattr(var, 'data') and hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    elif hasattr(other, 'variables'):\n+        for name, var in other.variables.items():\n+            if hasattr(var.data, 'chunks'):\n+                preserve_chunks[name] = var.chunks\n+    \n+    merge_result = dataset_update_method(self, other)\n+    result = self._replace(inplace=True, **merge_result._asdict())\n+    \n+    # Ensure dask arrays remain chunked after update\n+    if preserve_chunks:\n+        for name, chunks in preserve_chunks.items():\n+            if name in result.variables and not hasattr(result.variables[name].data, 'chunks'):\n+                # Restore chunking if it was lost during merge\n+                result = result.chunk({name: chunks})\n+    \n+    return result\n \n     def merge(\n         self,\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..a503e2cc 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -905,6 +905,7 @@ def dataset_merge_method(\n     )\n \n \n+\n def dataset_update_method(\n     dataset: \"Dataset\", other: \"CoercibleMapping\"\n ) -> _MergeResult:\n@@ -932,6 +933,22 @@ def dataset_update_method(\n \n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n+    \n+    # Check if any variables contain dask arrays to preserve chunking\n+    has_dask_data = False\n+    if hasattr(other, 'values'):\n+        has_dask_data = any(\n+            hasattr(getattr(var, 'data', None), 'chunks')\n+            for var in other.values()\n+            if hasattr(var, 'data')\n+        )\n+    elif isinstance(other, dict):\n+        has_dask_data = any(\n+            hasattr(getattr(var, 'data', None), 'chunks')\n+            for var in other.values()\n+            if hasattr(var, 'data')\n+        )\n+    \n     return merge_core(\n         [dataset, other],\n         priority_arg=1,\n"
  }
}