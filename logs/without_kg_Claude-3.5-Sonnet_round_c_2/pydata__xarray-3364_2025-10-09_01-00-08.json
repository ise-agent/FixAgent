{
  "zero_patches": {
    "Location 1 (/root/hy/projects_2/xarray/xarray/core/concat.py lines 222-233)": "\n            # For outer join behavior, collect variables that exist in any dataset\n            all_vars_in_subset = set()\n            for dataset in datasets:\n                all_vars_in_subset.update(getattr(dataset, subset))\n            \n            # Filter to only include variables that were explicitly requested and exist somewhere\n            valid_vars = [k for k in opt if k in all_vars_in_subset]\n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            \n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(valid_vars)\n",
    "Location 2 (/root/hy/projects_2/xarray/xarray/core/concat.py lines 359-367)": "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            # Collect variables from all datasets, handling missing ones\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a variable filled with NaN with same dims as first occurrence\n                    first_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                    fill_var = Variable(first_var.dims, \n                                      pd.api.types.pandas_dtype(first_var.dtype).type(fill_value) \n                                      if pd.api.types.is_scalar(fill_value) \n                                      else fill_value).broadcast_like(first_var)\n                    vars_list.append(fill_var)\n            \n            vars = ensure_common_dims(vars_list)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
    "Location 3 (/root/hy/projects_2/xarray/xarray/core/concat.py lines 314-320)": "\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                # For outer join behavior, we'll handle missing variables during alignment\n                # rather than raising an error. Skip datasets that don't have all merge vars.\n                pass\n",
    "Location 4 (/root/hy/projects_2/xarray/xarray/core/concat.py lines 181-238)": "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # all nonindexes that are not the same in each dataset\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n\n            elif opt == \"all\":\n                concat_over.update(\n                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n                )\n            elif opt == \"minimal\":\n                pass\n            else:\n                raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n        else:\n            # Collect all variables of this subset type across all datasets\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            # Check if specified variables exist in at least one dataset\n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates in \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"in any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(opt)\n\n    process_subset_opt(data_vars, \"data_vars\")\n    process_subset_opt(coords, \"coords\")\n    return concat_over, equals, concat_dim_lengths\n",
    "Location 5 (/root/hy/projects_2/xarray/xarray/core/concat.py lines 350-385)": "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            # Collect variables that exist, using fill_value for missing ones\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a dummy variable with appropriate shape filled with fill_value\n                    # Use the same dims as other variables for this key, or common_dims\n                    ref_var = next((ds.variables[k] for ds in datasets if k in ds.variables), None)\n                    if ref_var is not None:\n                        # Create variable with same dims but filled with fill_value\n                        dummy_shape = tuple(dims_sizes.get(d, 1) for d in ref_var.dims)\n                        dummy_data = utils.full_like(ref_var.data, fill_value, shape=dummy_shape)\n                        dummy_var = Variable(ref_var.dims, dummy_data, ref_var.attrs.copy())\n                        vars_list.append(dummy_var)\n            \n            if vars_list:\n                vars = ensure_common_dims(vars_list)\n                combined = concat_vars(vars, dim, positions)\n                assert isinstance(combined, Variable)\n                result_vars[k] = combined\n\n    result = Dataset(result_vars, attrs=result_attrs)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names:\n        raise ValueError(\n            \"Variables %r are coordinates in some datasets but not others.\"\n            % absent_coord_names\n        )\n    result = result.set_coords(coord_names)\n    result.encoding = result_encoding\n\n    result = result.drop(unlabeled_dims, errors=\"ignore\")\n\n    if coord is not None:\n        # add concat dimension last to ensure that its in the final Dataset\n        result[coord.name] = coord\n\n    return result\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_2/xarray/xarray/core/concat.py lines 222-233)": [
      "\n        else:\n            # For outer join behavior, collect variables that exist in ANY dataset\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            # Filter to only include variables that were explicitly requested and exist somewhere\n            valid_vars = [k for k in opt if k in all_vars_in_subset]\n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            \n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(valid_vars)\n",
      "\n            # Collect variables that exist in ANY dataset for outer join behavior\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            # Filter to only include variables that were explicitly requested and exist somewhere\n            valid_vars = [k for k in opt if k in all_vars_in_subset]\n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            \n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(valid_vars)\n",
      "\n            # For outer join behavior, collect variables that exist in ANY dataset\n            # rather than restricting to only variables in the first dataset\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            # Filter to only include variables that were explicitly requested and exist somewhere\n            valid_vars = [k for k in opt if k in all_vars_in_subset]\n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            \n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(valid_vars)\n",
      "\n        else:\n            # For outer join behavior, collect variables that exist in ANY dataset\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            # Filter to only include variables that were explicitly requested and exist somewhere\n            valid_vars = [k for k in opt if k in all_vars_in_subset]\n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            \n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(valid_vars)\n",
      "\n        else:\n            # For outer join behavior, collect all specified variables that exist in ANY dataset\n            all_available_vars = set()\n            for ds in datasets:\n                all_available_vars.update(getattr(ds, subset))\n            \n            # Filter to only include variables that actually exist somewhere\n            valid_vars = [k for k in opt if k in all_available_vars]\n            invalid_vars = [k for k in opt if k not in all_available_vars]\n            \n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(valid_vars)\n",
      "\n        invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n        if invalid_vars:\n            # Check if invalid variables exist in any other dataset\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            truly_invalid = [k for k in invalid_vars if k not in all_vars]\n            if truly_invalid:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (truly_invalid,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (truly_invalid,)\n                    )\n",
      "\n        else:\n            # Collect variables that exist in any dataset for outer join behavior\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            # Filter to only include variables that were explicitly requested and exist somewhere\n            valid_vars = [k for k in opt if k in all_vars_in_subset]\n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            \n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(valid_vars)\n",
      "\n            # Collect variables that exist in ANY dataset, not just the first one\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            # Filter to only include variables that actually exist somewhere\n            valid_vars = [k for k in opt if k in all_vars_in_subset]\n            \n            # Warn about completely invalid variables (don't exist in any dataset)\n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(valid_vars)\n"
    ],
    "Location 2 (/root/hy/projects_2/xarray/xarray/core/concat.py lines 359-367)": [
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            # Collect variables from all datasets, handling missing variables gracefully\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a dummy variable filled with NaN for missing variables\n                    # Use the same structure as other variables but filled with NaN\n                    if vars_list:\n                        # Use the first available variable as template for shape/dims\n                        template_var = vars_list[0]\n                        dummy_var = template_var._constructor(\n                            template_var.dims,\n                            dtypes.NA,\n                            template_var._attrs,\n                            template_var._encoding,\n                            fastpath=True\n                        )\n                        vars_list.append(dummy_var)\n                    else:\n                        # If this is the first dataset and variable is missing, \n                        # we'll handle it when we find the first occurrence\n                        continue\n            \n            if vars_list:\n                try:\n                    vars = ensure_common_dims(vars_list)\n                    combined = concat_vars(vars, dim, positions)\n                    assert isinstance(combined, Variable)\n                    result_vars[k] = combined\n                except Exception:\n                    # If concatenation fails, skip this variable\n                    continue\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            vars = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars.append(ds.variables[k])\n                else:\n                    # Create a variable filled with NaN for missing variables\n                    # Get the variable from a dataset that has it to determine structure\n                    ref_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                    # Create shape for this dataset matching reference variable but with this dataset's dimensions\n                    var_dims = ref_var.dims\n                    var_shape = []\n                    for d in var_dims:\n                        if d == dim:\n                            var_shape.append(1)  # Will be handled by concat_vars\n                        elif d in ds.dims:\n                            var_shape.append(ds.dims[d])\n                        else:\n                            var_shape.append(ref_var.sizes[d])\n                    \n                    # Create variable filled with appropriate fill value\n                    import numpy as np\n                    fill_val = np.nan if ref_var.dtype.kind in 'fc' else 0\n                    data = np.full(var_shape, fill_val, dtype=ref_var.dtype)\n                    missing_var = Variable(var_dims, data, ref_var.attrs.copy())\n                    vars.append(missing_var)\n            \n            vars = ensure_common_dims(vars)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                # Collect variables from all datasets, handling missing ones\n                vars_to_concat = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars_to_concat.append(ds.variables[k])\n                    else:\n                        # Create a variable filled with NaN for missing data\n                        # Use the shape from the first available variable\n                        if vars_to_concat:\n                            template_var = vars_to_concat[0]\n                        else:\n                            # Find first dataset that has this variable\n                            template_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                        \n                        # Create NaN-filled variable with same dims and dtype\n                        import numpy as np\n                        fill_value = np.nan if template_var.dtype.kind in 'fc' else None\n                        nan_var = template_var._constructor(\n                            template_var.dims,\n                            np.full_like(template_var.data, fill_value),\n                            template_var.attrs.copy(),\n                            template_var.encoding.copy()\n                        )\n                        vars_to_concat.append(nan_var)\n                \n                vars = ensure_common_dims(vars_to_concat)\n            except (KeyError, StopIteration):\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a variable filled with NaN for missing variables\n                    template_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                    missing_var = template_var.copy()\n                    missing_var = missing_var.where(False)  # Fill with NaN\n                    vars_list.append(missing_var)\n            \n            vars = ensure_common_dims(vars_list)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            # Collect variables from all datasets, using None for missing ones\n            vars_to_concat = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_to_concat.append(ds.variables[k])\n                else:\n                    # Create a placeholder variable with NaN values matching the structure\n                    # of the first occurrence of this variable\n                    first_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                    placeholder = first_var._constructor(\n                        first_var.dims, \n                        utils.full_like(first_var, fill_value, dtype=first_var.dtype),\n                        first_var.attrs\n                    )\n                    vars_to_concat.append(placeholder)\n            \n            vars = ensure_common_dims(vars_to_concat)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            # Collect variables from all datasets, handling missing variables gracefully\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a dummy variable with NaN values for missing variables\n                    # Use the same dims and shape as would be expected after alignment\n                    ref_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                    dummy_data = utils.full_like(ref_var, fill_value=dtypes.NA, dtype=ref_var.dtype)\n                    vars_list.append(Variable(ref_var.dims, dummy_data, ref_var.attrs))\n            \n            vars = ensure_common_dims(vars_list)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            except KeyError:\n                # Handle missing variables by creating aligned variables with fill_value\n                vars_list = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars_list.append(ds.variables[k])\n                    else:\n                        # Create a variable with NaN/fill_value for missing data\n                        template_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                        missing_var = template_var._constructor(\n                            template_var.dims, \n                            template_var._default_fill_value,\n                            template_var.attrs\n                        )\n                        vars_list.append(missing_var)\n                vars = ensure_common_dims(vars_list)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            # Collect variables that exist in datasets, handle missing ones\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a variable filled with NaN for missing cases\n                    # Use the shape from aligned datasets to maintain consistency\n                    template_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                    fill_var = template_var._constructor(\n                        dims=template_var.dims,\n                        data=pd.api.types.pandas_array([fill_value] * template_var.size).reshape(template_var.shape),\n                        attrs=template_var.attrs,\n                        encoding=template_var.encoding\n                    )\n                    vars_list.append(fill_var)\n            \n            vars = ensure_common_dims(vars_list)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n"
    ],
    "Location 3 (/root/hy/projects_2/xarray/xarray/core/concat.py lines 314-320)": [
      "\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                # For outer join behavior, collect variables from all datasets\n                # and let align() handle missing values with fill_value\n                for var in absent_merge_vars:\n                    # Skip variables that don't exist in this dataset\n                    # They will be handled by align() with fill_value\n                    pass\n",
      "\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            # Allow variables to be present in some datasets but not others\n            # The align() function will handle missing values with fill_value\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                # If variable is missing from this dataset, it will be handled by align()\n",
      "\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                # For outer join behavior, collect variables that exist in any dataset\n                # and let align() handle missing values with fill_value\n                for var in absent_merge_vars:\n                    if var not in to_merge:\n                        to_merge[var] = []\n                # Skip datasets that don't have certain variables - they'll be handled by align()\n                continue\n",
      "\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            # Skip validation for absent variables - they will be handled by alignment\n            # This enables outer join behavior similar to pandas concat\n            for var in variables_to_merge.intersection(set(ds.variables)):\n                to_merge[var].append(ds.variables[var])\n",
      "\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                # For outer join behavior, collect variables that exist in any dataset\n                # and let align() handle missing values with fill_value\n                for var in absent_merge_vars:\n                    if var not in to_merge:\n                        to_merge[var] = []\n                # Add placeholders for missing variables in this dataset\n                for var in absent_merge_vars:\n                    # Create a placeholder that will be handled by align()\n                    to_merge[var].append(None)\n",
      "\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                # For outer join behavior, collect variables that exist in any dataset\n                # and let align() handle missing values with fill_value\n                for var in absent_merge_vars:\n                    if var not in to_merge:\n                        to_merge[var] = []\n                # Add placeholder None for missing variables to maintain dataset order\n                for var in variables_to_merge:\n                    if var in ds.variables:\n                        to_merge[var].append(ds.variables[var])\n                    else:\n                        to_merge[var].append(None)\n            else:\n                for var in variables_to_merge:\n                    to_merge[var].append(ds.variables[var])\n",
      "\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                # For outer join behavior, skip datasets that don't have all merge variables\n                # The align() function will handle filling missing values appropriately\n                pass\n",
      "\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                # For outer join behavior, collect variables from all datasets\n                # and let align() handle missing values with fill_value\n                for var in absent_merge_vars:\n                    if var not in to_merge:\n                        to_merge[var] = []\n                # Add empty placeholders for missing variables in this dataset\n                # They will be filled by align() later\n                continue\n"
    ],
    "Location 4 (/root/hy/projects_2/xarray/xarray/core/concat.py lines 181-238)": [
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # all nonindexes that are not the same in each dataset\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n\n            elif opt == \"all\":\n                concat_over.update(\n                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n                )\n            elif opt == \"minimal\":\n                pass\n            else:\n                raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n        else:\n            # Collect variables from all datasets for outer join behavior\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            # Check that specified variables exist in at least one dataset\n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates in \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"in any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(opt)\n\n    process_subset_opt(data_vars, \"data_vars\")\n    process_subset_opt(coords, \"coords\")\n    return concat_over, equals, concat_dim_lengths\n",
      "\ndef process_subset_opt(opt, subset):\n    if isinstance(opt, str):\n        if opt == \"different\":\n            if compat == \"override\":\n                raise ValueError(\n                    \"Cannot specify both %s='different' and compat='override'.\"\n                    % subset\n                )\n            # all nonindexes that are not the same in each dataset\n            for k in getattr(datasets[0], subset):\n                if k not in concat_over:\n                    # Compare the variable of all datasets vs. the one\n                    # of the first dataset. Perform the minimum amount of\n                    # loads in order to avoid multiple loads from disk\n                    # while keeping the RAM footprint low.\n                    v_lhs = datasets[0].variables[k].load()\n                    # We'll need to know later on if variables are equal.\n                    computed = []\n                    for ds_rhs in datasets[1:]:\n                        v_rhs = ds_rhs.variables[k].compute()\n                        computed.append(v_rhs)\n                        if not getattr(v_lhs, compat)(v_rhs):\n                            concat_over.add(k)\n                            equals[k] = False\n                            # computed variables are not to be re-computed\n                            # again in the future\n                            for ds, v in zip(datasets[1:], computed):\n                                ds.variables[k].data = v.data\n                            break\n                    else:\n                        equals[k] = True\n\n        elif opt == \"all\":\n            concat_over.update(\n                set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n            )\n        elif opt == \"minimal\":\n            pass\n        else:\n            raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n    else:\n        # Collect variables from all datasets for outer join behavior\n        all_vars_in_subset = set()\n        for ds in datasets:\n            all_vars_in_subset.update(getattr(ds, subset))\n        \n        # Check if specified variables exist in at least one dataset\n        invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n        if invalid_vars:\n            if subset == \"coords\":\n                raise ValueError(\n                    \"some variables in coords are not coordinates on \"\n                    \"any dataset: %s\" % (invalid_vars,)\n                )\n            else:\n                raise ValueError(\n                    \"some variables in data_vars are not data variables \"\n                    \"on any dataset: %s\" % (invalid_vars,)\n                )\n        concat_over.update(opt)\n",
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # all nonindexes that are not the same in each dataset\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n\n            elif opt == \"all\":\n                concat_over.update(\n                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n                )\n            elif opt == \"minimal\":\n                pass\n            else:\n                raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n        else:\n            # Collect variables from all datasets for outer join behavior\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            # Check if specified variables exist in at least one dataset\n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(opt)\n\n    process_subset_opt(data_vars, \"data_vars\")\n    process_subset_opt(coords, \"coords\")\n    return concat_over, equals, concat_dim_lengths\n",
      "\ndef process_subset_opt(opt, subset):\n    if isinstance(opt, str):\n        if opt == \"different\":\n            if compat == \"override\":\n                raise ValueError(\n                    \"Cannot specify both %s='different' and compat='override'.\"\n                    % subset\n                )\n            # all nonindexes that are not the same in each dataset\n            for k in getattr(datasets[0], subset):\n                if k not in concat_over:\n                    # Compare the variable of all datasets vs. the one\n                    # of the first dataset. Perform the minimum amount of\n                    # loads in order to avoid multiple loads from disk\n                    # while keeping the RAM footprint low.\n                    v_lhs = datasets[0].variables[k].load()\n                    # We'll need to know later on if variables are equal.\n                    computed = []\n                    for ds_rhs in datasets[1:]:\n                        v_rhs = ds_rhs.variables[k].compute()\n                        computed.append(v_rhs)\n                        if not getattr(v_lhs, compat)(v_rhs):\n                            concat_over.add(k)\n                            equals[k] = False\n                            # computed variables are not to be re-computed\n                            # again in the future\n                            for ds, v in zip(datasets[1:], computed):\n                                ds.variables[k].data = v.data\n                            break\n                    else:\n                        equals[k] = True\n\n        elif opt == \"all\":\n            concat_over.update(\n                set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n            )\n        elif opt == \"minimal\":\n            pass\n        else:\n            raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n    else:\n        # Collect all variables of the specified subset type across all datasets\n        all_subset_vars = set()\n        for ds in datasets:\n            all_subset_vars.update(getattr(ds, subset))\n        \n        # Check if specified variables exist in any dataset (outer join behavior)\n        invalid_vars = [k for k in opt if k not in all_subset_vars]\n        if invalid_vars:\n            if subset == \"coords\":\n                raise ValueError(\n                    \"some variables in coords are not coordinates on \"\n                    \"any dataset: %s\" % (invalid_vars,)\n                )\n            else:\n                raise ValueError(\n                    \"some variables in data_vars are not data variables \"\n                    \"on any dataset: %s\" % (invalid_vars,)\n                )\n        concat_over.update(opt)\n",
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # all nonindexes that are not the same in each dataset\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n\n            elif opt == \"all\":\n                concat_over.update(\n                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n                )\n            elif opt == \"minimal\":\n                pass\n            else:\n                raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n        else:\n            # Collect all variables of this subset type across all datasets\n            all_subset_vars = set()\n            for ds in datasets:\n                all_subset_vars.update(getattr(ds, subset))\n            \n            # Check if specified variables exist in at least one dataset\n            invalid_vars = [k for k in opt if k not in all_subset_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(opt)\n\n    process_subset_opt(data_vars, \"data_vars\")\n    process_subset_opt(coords, \"coords\")\n    return concat_over, equals, concat_dim_lengths\n",
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # all nonindexes that are not the same in each dataset\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n\n            elif opt == \"all\":\n                concat_over.update(\n                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n                )\n            elif opt == \"minimal\":\n                pass\n            else:\n                raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n        else:\n            # Collect all variables of this subset type across all datasets\n            all_subset_vars = set()\n            for ds in datasets:\n                all_subset_vars.update(getattr(ds, subset))\n            \n            # Check if specified variables exist in at least one dataset\n            invalid_vars = [k for k in opt if k not in all_subset_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            concat_over.update(opt)\n\n    process_subset_opt(data_vars, \"data_vars\")\n    process_subset_opt(coords, \"coords\")\n    return concat_over, equals, concat_dim_lengths\n",
      "\ndef process_subset_opt(opt, subset):\n    if isinstance(opt, str):\n        if opt == \"different\":\n            if compat == \"override\":\n                raise ValueError(\n                    \"Cannot specify both %s='different' and compat='override'.\"\n                    % subset\n                )\n            # all nonindexes that are not the same in each dataset\n            for k in getattr(datasets[0], subset):\n                if k not in concat_over:\n                    # Compare the variable of all datasets vs. the one\n                    # of the first dataset. Perform the minimum amount of\n                    # loads in order to avoid multiple loads from disk\n                    # while keeping the RAM footprint low.\n                    v_lhs = datasets[0].variables[k].load()\n                    # We'll need to know later on if variables are equal.\n                    computed = []\n                    for ds_rhs in datasets[1:]:\n                        v_rhs = ds_rhs.variables[k].compute()\n                        computed.append(v_rhs)\n                        if not getattr(v_lhs, compat)(v_rhs):\n                            concat_over.add(k)\n                            equals[k] = False\n                            # computed variables are not to be re-computed\n                            # again in the future\n                            for ds, v in zip(datasets[1:], computed):\n                                ds.variables[k].data = v.data\n                            break\n                    else:\n                        equals[k] = True\n\n        elif opt == \"all\":\n            concat_over.update(\n                set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n            )\n        elif opt == \"minimal\":\n            pass\n        else:\n            raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n    else:\n        # Collect variables from all datasets for outer join behavior\n        all_vars_in_subset = set()\n        for ds in datasets:\n            all_vars_in_subset.update(getattr(ds, subset))\n        \n        invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n        if invalid_vars:\n            if subset == \"coords\":\n                raise ValueError(\n                    \"some variables in coords are not coordinates in \"\n                    \"any dataset: %s\" % (invalid_vars,)\n                )\n            else:\n                raise ValueError(\n                    \"some variables in data_vars are not data variables \"\n                    \"in any dataset: %s\" % (invalid_vars,)\n                )\n        concat_over.update(opt)\n",
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # all nonindexes that are not the same in each dataset\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n\n            elif opt == \"all\":\n                concat_over.update(\n                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n                )\n            elif opt == \"minimal\":\n                pass\n            else:\n                raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n        else:\n            # Collect variables that exist in ANY dataset for outer join behavior\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            # Only validate against variables that exist in at least one dataset\n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n            # Only add variables that actually exist in at least one dataset\n            concat_over.update(k for k in opt if k in all_vars_in_subset)\n"
    ],
    "Location 5 (/root/hy/projects_2/xarray/xarray/core/concat.py lines 350-385)": [
      "\ndef ensure_common_dims(vars):\n    # ensure each variable with the given name shares the same\n    # dimensions and the same shape for all of them except along the\n    # concat dimension\n    common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n    if dim not in common_dims:\n        common_dims = (dim,) + common_dims\n    for var, dim_len in zip(vars, concat_dim_lengths):\n        if var.dims != common_dims:\n            common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n            var = var.set_dims(common_dims, common_shape)\n        yield var\n\n# stack up each variable to fill-out the dataset (in order)\n# n.b. this loop preserves variable order, needed for groupby.\nfor k in datasets[0].variables:\n    if k in concat_over:\n        try:\n            # Get variables that exist, use None for missing ones\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a dummy variable with NaN values for missing variables\n                    if k in coord_names:\n                        # For coordinates, use the first available variable as template\n                        template_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                        dummy_var = template_var._constructor(\n                            template_var.dims, \n                            dtypes.full_like(template_var, fill_value, dtype=template_var.dtype),\n                            template_var.attrs\n                        )\n                    else:\n                        # For data variables, create minimal compatible variable\n                        dummy_var = Variable((dim,), [fill_value])\n                    vars_list.append(dummy_var)\n            \n            vars = ensure_common_dims(vars_list)\n        except KeyError:\n            raise ValueError(\"%r is not present in all datasets.\" % k)\n        combined = concat_vars(vars, dim, positions)\n        assert isinstance(combined, Variable)\n        result_vars[k] = combined\n\nresult = Dataset(result_vars, attrs=result_attrs)\nabsent_coord_names = coord_names - set(result.variables)\nif absent_coord_names:\n    raise ValueError(\n        \"Variables %r are coordinates in some datasets but not others.\"\n        % absent_coord_names\n    )\nresult = result.set_coords(coord_names)\nresult.encoding = result_encoding\n\nresult = result.drop(unlabeled_dims, errors=\"ignore\")\n\nif coord is not None:\n    # add concat dimension last to ensure that its in the final Dataset\n    result[coord.name] = coord\n\nreturn result\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a variable filled with fill_value for missing variables\n                    first_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                    missing_var = Variable(\n                        first_var.dims, \n                        utils.full_like(first_var, fill_value, dtype=first_var.dtype)\n                    )\n                    vars_list.append(missing_var)\n            \n            vars = ensure_common_dims(vars_list)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n\n    result = Dataset(result_vars, attrs=result_attrs)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names:\n        raise ValueError(\n            \"Variables %r are coordinates in some datasets but not others.\"\n            % absent_coord_names\n        )\n    result = result.set_coords(coord_names)\n    result.encoding = result_encoding\n\n    result = result.drop(unlabeled_dims, errors=\"ignore\")\n\n    if coord is not None:\n        # add concat dimension last to ensure that its in the final Dataset\n        result[coord.name] = coord\n\n    return result\n",
      "\ndef ensure_common_dims(vars):\n    # ensure each variable with the given name shares the same\n    # dimensions and the same shape for all of them except along the\n    # concat dimension\n    common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n    if dim not in common_dims:\n        common_dims = (dim,) + common_dims\n    for var, dim_len in zip(vars, concat_dim_lengths):\n        if var.dims != common_dims:\n            common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n            var = var.set_dims(common_dims, common_shape)\n        yield var\n\n# stack up each variable to fill-out the dataset (in order)\n# n.b. this loop preserves variable order, needed for groupby.\nfor k in datasets[0].variables:\n    if k in concat_over:\n        # Handle missing variables gracefully for outer join behavior\n        vars_list = []\n        for ds in datasets:\n            if k in ds.variables:\n                vars_list.append(ds.variables[k])\n            else:\n                # Create a dummy variable filled with fill_value for missing variables\n                dummy_var = Variable(datasets[0].variables[k].dims, \n                                   utils.full_like(datasets[0].variables[k], fill_value))\n                vars_list.append(dummy_var)\n        \n        vars = ensure_common_dims(vars_list)\n        combined = concat_vars(vars, dim, positions)\n        assert isinstance(combined, Variable)\n        result_vars[k] = combined\n\nresult = Dataset(result_vars, attrs=result_attrs)\nabsent_coord_names = coord_names - set(result.variables)\nif absent_coord_names:\n    raise ValueError(\n        \"Variables %r are coordinates in some datasets but not others.\"\n        % absent_coord_names\n    )\nresult = result.set_coords(coord_names)\nresult.encoding = result_encoding\n\nresult = result.drop(unlabeled_dims, errors=\"ignore\")\n\nif coord is not None:\n    # add concat dimension last to ensure that its in the final Dataset\n    result[coord.name] = coord\n\nreturn result\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a variable filled with fill_value for missing variables\n                    # Get the common dimensions from existing variables\n                    if vars_list:\n                        template_var = vars_list[0]\n                        dims = template_var.dims\n                        shape = template_var.shape\n                    else:\n                        # If this is the first variable, we need to determine dims from other datasets\n                        for other_ds in datasets:\n                            if k in other_ds.variables:\n                                template_var = other_ds.variables[k]\n                                dims = template_var.dims\n                                shape = template_var.shape\n                                break\n                    \n                    # Create a variable with the same structure but filled with fill_value\n                    import numpy as np\n                    fill_data = np.full(shape, fill_value, dtype=template_var.dtype)\n                    fill_var = Variable(dims, fill_data, attrs=template_var.attrs)\n                    vars_list.append(fill_var)\n            \n            if vars_list:\n                vars = ensure_common_dims(vars_list)\n                combined = concat_vars(vars, dim, positions)\n                assert isinstance(combined, Variable)\n                result_vars[k] = combined\n\n    result = Dataset(result_vars, attrs=result_attrs)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names:\n        raise ValueError(\n            \"Variables %r are coordinates in some datasets but not others.\"\n            % absent_coord_names\n        )\n    result = result.set_coords(coord_names)\n    result.encoding = result_encoding\n\n    result = result.drop(unlabeled_dims, errors=\"ignore\")\n\n    if coord is not None:\n        # add concat dimension last to ensure that its in the final Dataset\n        result[coord.name] = coord\n\n    return result\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            except KeyError:\n                # Handle missing variables with outer join behavior\n                vars_with_missing = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars_with_missing.append(ds.variables[k])\n                    else:\n                        # Create a dummy variable filled with fill_value\n                        first_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                        dummy_var = Variable(first_var.dims, \n                                           dtypes.full_like(first_var, fill_value, dtype=first_var.dtype))\n                        vars_with_missing.append(dummy_var)\n                vars = ensure_common_dims(vars_with_missing)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n\n    result = Dataset(result_vars, attrs=result_attrs)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names:\n        raise ValueError(\n            \"Variables %r are coordinates in some datasets but not others.\"\n            % absent_coord_names\n        )\n    result = result.set_coords(coord_names)\n    result.encoding = result_encoding\n\n    result = result.drop(unlabeled_dims, errors=\"ignore\")\n\n    if coord is not None:\n        # add concat dimension last to ensure that its in the final Dataset\n        result[coord.name] = coord\n\n    return result\n",
      "\ndef ensure_common_dims(vars):\n    # ensure each variable with the given name shares the same\n    # dimensions and the same shape for all of them except along the\n    # concat dimension\n    common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n    if dim not in common_dims:\n        common_dims = (dim,) + common_dims\n    for var, dim_len in zip(vars, concat_dim_lengths):\n        if var.dims != common_dims:\n            common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n            var = var.set_dims(common_dims, common_shape)\n        yield var\n\n# stack up each variable to fill-out the dataset (in order)\n# n.b. this loop preserves variable order, needed for groupby.\nfor k in datasets[0].variables:\n    if k in concat_over:\n        # Handle missing variables gracefully for outer join behavior\n        available_vars = []\n        for ds in datasets:\n            if k in ds.variables:\n                available_vars.append(ds.variables[k])\n            else:\n                # Create a dummy variable filled with fill_value for missing variables\n                # Use the common dimensions and appropriate shape\n                if available_vars:\n                    template_var = available_vars[0]\n                    dummy_dims = template_var.dims\n                    dummy_shape = template_var.shape\n                else:\n                    # If this is the first variable, use minimal dimensions\n                    dummy_dims = (dim,)\n                    dummy_shape = (concat_dim_lengths[len(available_vars)],)\n                \n                dummy_var = Variable(dummy_dims, \n                                   utils.full_like(template_var.data if available_vars else 0, \n                                                 fill_value, shape=dummy_shape),\n                                   attrs={})\n                available_vars.append(dummy_var)\n        \n        vars = ensure_common_dims(available_vars)\n        combined = concat_vars(vars, dim, positions)\n        assert isinstance(combined, Variable)\n        result_vars[k] = combined\n\nresult = Dataset(result_vars, attrs=result_attrs)\nabsent_coord_names = coord_names - set(result.variables)\nif absent_coord_names:\n    raise ValueError(\n        \"Variables %r are coordinates in some datasets but not others.\"\n        % absent_coord_names\n    )\nresult = result.set_coords(coord_names)\nresult.encoding = result_encoding\n\nresult = result.drop(unlabeled_dims, errors=\"ignore\")\n\nif coord is not None:\n    # add concat dimension last to ensure that its in the final Dataset\n    result[coord.name] = coord\n\nreturn result\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a variable filled with fill_value for missing variables\n                    # Use the first available variable as template for dims and dtype\n                    template_var = None\n                    for other_ds in datasets:\n                        if k in other_ds.variables:\n                            template_var = other_ds.variables[k]\n                            break\n                    \n                    if template_var is not None:\n                        # Create variable with same dims but filled with fill_value\n                        import numpy as np\n                        fill_data = np.full(template_var.shape, fill_value, dtype=template_var.dtype)\n                        missing_var = Variable(template_var.dims, fill_data, template_var.attrs)\n                        vars_list.append(missing_var)\n            \n            if vars_list:  # Only process if we have at least one variable\n                vars = ensure_common_dims(vars_list)\n                combined = concat_vars(vars, dim, positions)\n                assert isinstance(combined, Variable)\n                result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            # Collect variables that exist, handling missing ones gracefully\n            vars_to_concat = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_to_concat.append(ds.variables[k])\n                else:\n                    # Create a variable filled with fill_value for missing variables\n                    # Use the dimensions and shape from the first available variable\n                    if vars_to_concat:\n                        template_var = vars_to_concat[0]\n                        missing_var = Variable(\n                            template_var.dims,\n                            dtypes.full_like(template_var, fill_value, dtype=template_var.dtype),\n                            template_var.attrs\n                        )\n                        vars_to_concat.append(missing_var)\n                    else:\n                        # If this is the first variable, we need to find a template from other datasets\n                        for other_ds in datasets:\n                            if k in other_ds.variables:\n                                template_var = other_ds.variables[k]\n                                missing_var = Variable(\n                                    template_var.dims,\n                                    dtypes.full_like(template_var, fill_value, dtype=template_var.dtype),\n                                    template_var.attrs\n                                )\n                                vars_to_concat.append(missing_var)\n                                break\n            \n            if vars_to_concat:\n                vars = ensure_common_dims(vars_to_concat)\n                combined = concat_vars(vars, dim, positions)\n                assert isinstance(combined, Variable)\n                result_vars[k] = combined\n\n    result = Dataset(result_vars, attrs=result_attrs)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names:\n        raise ValueError(\n            \"Variables %r are coordinates in some datasets but not others.\"\n            % absent_coord_names\n        )\n    result = result.set_coords(coord_names)\n    result.encoding = result_encoding\n\n    result = result.drop(unlabeled_dims, errors=\"ignore\")\n\n    if coord is not None:\n        # add concat dimension last to ensure that its in the final Dataset\n        result[coord.name] = coord\n\n    return result\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..ec945574 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -178,6 +178,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n+\n     def process_subset_opt(opt, subset):\n         if isinstance(opt, str):\n             if opt == \"different\":\n@@ -197,45 +198,54 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n                         for ds_rhs in datasets[1:]:\n-                            v_rhs = ds_rhs.variables[k].compute()\n-                            computed.append(v_rhs)\n-                            if not getattr(v_lhs, compat)(v_rhs):\n+                            if k in ds_rhs.variables:\n+                                v_rhs = ds_rhs.variables[k].compute()\n+                                computed.append(v_rhs)\n+                                if not getattr(v_lhs, compat)(v_rhs):\n+                                    concat_over.add(k)\n+                                    equals[k] = False\n+                                    # computed variables are not to be re-computed\n+                                    # again in the future\n+                                    for ds, v in zip(datasets[1:], computed):\n+                                        if k in ds.variables:\n+                                            ds.variables[k].data = v.data\n+                                    break\n+                            else:\n+                                # Variable missing in some datasets, add to concat_over\n                                 concat_over.add(k)\n                                 equals[k] = False\n-                                # computed variables are not to be re-computed\n-                                # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n-                                    ds.variables[k].data = v.data\n                                 break\n                         else:\n                             equals[k] = True\n \n             elif opt == \"all\":\n-                concat_over.update(\n-                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n-                )\n+                # Collect all variables from all datasets for outer join\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                concat_over.update(all_vars - set(datasets[0].dims))\n             elif opt == \"minimal\":\n                 pass\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n-            if invalid_vars:\n-                if subset == \"coords\":\n-                    raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n-                    )\n-                else:\n-                    raise ValueError(\n-                        \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n-                    )\n-            concat_over.update(opt)\n+            # Collect variables that exist in ANY dataset for outer join behavior\n+            all_available_vars = set()\n+            if subset == \"coords\":\n+                for ds in datasets:\n+                    all_available_vars.update(ds.coords)\n+            else:\n+                for ds in datasets:\n+                    all_available_vars.update(ds.data_vars)\n+            \n+            # Filter to only include variables that actually exist somewhere\n+            valid_vars = [k for k in opt if k in all_available_vars]\n+            concat_over.update(valid_vars)\n \n     process_subset_opt(data_vars, \"data_vars\")\n     process_subset_opt(coords, \"coords\")\n     return concat_over, equals, concat_dim_lengths\n+    return concat_over, equals, concat_dim_lengths\n \n \n # determine dimensional coordinate names and a dict mapping name to DataArray\n@@ -311,13 +321,17 @@ def _dataset_concat(\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n \n+\n         for ds in datasets:\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n             if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+                # For outer join behavior, collect variables that exist in this dataset\n+                for var in variables_to_merge:\n+                    if var in ds.variables:\n+                        to_merge[var].append(ds.variables[var])\n+            else:\n+                for var in variables_to_merge:\n+                    to_merge[var].append(ds.variables[var])\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -347,6 +361,7 @@ def _dataset_concat(\n         # concat dimension\n         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n         if dim not in common_dims:\n+\n             common_dims = (dim,) + common_dims\n         for var, dim_len in zip(vars, concat_dim_lengths):\n             if var.dims != common_dims:\n@@ -356,13 +371,18 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n-        if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n-            combined = concat_vars(vars, dim, positions)\n+    for k in concat_over:\n+        # Collect variables from datasets that have this variable\n+        available_vars = []\n+        available_positions = []\n+        for i, ds in enumerate(datasets):\n+            if k in ds.variables:\n+                available_vars.append(ds.variables[k])\n+                available_positions.append(positions[i] if positions is not None else i)\n+        \n+        if available_vars:\n+            vars = ensure_common_dims(available_vars)\n+            combined = concat_vars(vars, dim, available_positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n \n@@ -384,6 +404,14 @@ def _dataset_concat(\n \n     return result\n \n+    result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n+    if coord is not None:\n+        # add concat dimension last to ensure that its in the final Dataset\n+        result[coord.name] = coord\n+\n+    return result\n+\n \n def _dataarray_concat(\n     arrays,\n",
    "variant_0": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..eecb6758 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -178,6 +178,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n+\n     def process_subset_opt(opt, subset):\n         if isinstance(opt, str):\n             if opt == \"different\":\n@@ -219,18 +220,34 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+            # Collect all variables of this subset type across all datasets\n+            all_vars_in_subset = set()\n+            for ds in datasets:\n+                all_vars_in_subset.update(getattr(ds, subset))\n+            \n+            # Check if specified variables exist in at least one dataset\n+            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"some variables in coords are not coordinates in \"\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"in any dataset: %s\" % (invalid_vars,)\n+                    )\n+            concat_over.update(opt)\n+\n+    process_subset_opt(data_vars, \"data_vars\")\n+    process_subset_opt(coords, \"coords\")\n+    return concat_over, equals, concat_dim_lengths\n+                    raise ValueError(\n+                        \"some variables in data_vars are not data variables \"\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n+            concat_over.update(valid_vars)\n             concat_over.update(opt)\n \n     process_subset_opt(data_vars, \"data_vars\")\n@@ -311,13 +328,13 @@ def _dataset_concat(\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n \n+\n         for ds in datasets:\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n             if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+                # For outer join behavior, we'll handle missing variables during alignment\n+                # rather than raising an error. Skip datasets that don't have all merge vars.\n+                pass\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -347,24 +364,32 @@ def _dataset_concat(\n         # concat dimension\n         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n         if dim not in common_dims:\n-            common_dims = (dim,) + common_dims\n-        for var, dim_len in zip(vars, concat_dim_lengths):\n-            if var.dims != common_dims:\n-                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n-                var = var.set_dims(common_dims, common_shape)\n-            yield var\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n     for k in datasets[0].variables:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n-            combined = concat_vars(vars, dim, positions)\n-            assert isinstance(combined, Variable)\n-            result_vars[k] = combined\n+            # Collect variables that exist, using fill_value for missing ones\n+            vars_list = []\n+            for ds in datasets:\n+                if k in ds.variables:\n+                    vars_list.append(ds.variables[k])\n+                else:\n+                    # Create a dummy variable with appropriate shape filled with fill_value\n+                    # Use the same dims as other variables for this key, or common_dims\n+                    ref_var = next((ds.variables[k] for ds in datasets if k in ds.variables), None)\n+                    if ref_var is not None:\n+                        # Create variable with same dims but filled with fill_value\n+                        dummy_shape = tuple(dims_sizes.get(d, 1) for d in ref_var.dims)\n+                        dummy_data = utils.full_like(ref_var.data, fill_value, shape=dummy_shape)\n+                        dummy_var = Variable(ref_var.dims, dummy_data, ref_var.attrs.copy())\n+                        vars_list.append(dummy_var)\n+            \n+            if vars_list:\n+                vars = ensure_common_dims(vars_list)\n+                combined = concat_vars(vars, dim, positions)\n+                assert isinstance(combined, Variable)\n+                result_vars[k] = combined\n \n     result = Dataset(result_vars, attrs=result_attrs)\n     absent_coord_names = coord_names - set(result.variables)\n@@ -378,6 +403,20 @@ def _dataset_concat(\n \n     result = result.drop(unlabeled_dims, errors=\"ignore\")\n \n+    if coord is not None:\n+        # add concat dimension last to ensure that its in the final Dataset\n+        result[coord.name] = coord\n+\n+    return result\n+        raise ValueError(\n+            \"Variables %r are coordinates in some datasets but not others.\"\n+            % absent_coord_names\n+        )\n+    result = result.set_coords(coord_names)\n+    result.encoding = result_encoding\n+\n+    result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n     if coord is not None:\n         # add concat dimension last to ensure that its in the final Dataset\n         result[coord.name] = coord\n",
    "variant_1": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..639d08ff 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -178,6 +178,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n+\n     def process_subset_opt(opt, subset):\n         if isinstance(opt, str):\n             if opt == \"different\":\n@@ -219,20 +220,37 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+            # Collect variables from all datasets for outer join behavior\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            # Check that specified variables exist in at least one dataset\n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"some variables in coords are not coordinates in \"\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"in any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n+    process_subset_opt(data_vars, \"data_vars\")\n+    process_subset_opt(coords, \"coords\")\n+    return concat_over, equals, concat_dim_lengths\n+                else:\n+                    raise ValueError(\n+                        \"some variables in data_vars are not data variables \"\n+                        \"on any dataset: %s\" % (invalid_vars,)\n+                    )\n+            concat_over.update(valid_vars)\n+            concat_over.update(opt)\n+\n     process_subset_opt(data_vars, \"data_vars\")\n     process_subset_opt(coords, \"coords\")\n     return concat_over, equals, concat_dim_lengths\n@@ -311,13 +329,16 @@ def _dataset_concat(\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n \n+\n         for ds in datasets:\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n             if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+                # For outer join behavior, collect variables from all datasets\n+                # and let align() handle missing values with fill_value\n+                for var in absent_merge_vars:\n+                    # Skip variables that don't exist in this dataset\n+                    # They will be handled by align() with fill_value\n+                    pass\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -347,24 +368,80 @@ def _dataset_concat(\n         # concat dimension\n         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n         if dim not in common_dims:\n-            common_dims = (dim,) + common_dims\n-        for var, dim_len in zip(vars, concat_dim_lengths):\n-            if var.dims != common_dims:\n-                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n-                var = var.set_dims(common_dims, common_shape)\n-            yield var\n-\n-    # stack up each variable to fill-out the dataset (in order)\n-    # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n-        if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n-            combined = concat_vars(vars, dim, positions)\n-            assert isinstance(combined, Variable)\n-            result_vars[k] = combined\n+\n+def ensure_common_dims(vars):\n+    # ensure each variable with the given name shares the same\n+    # dimensions and the same shape for all of them except along the\n+    # concat dimension\n+    common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n+    if dim not in common_dims:\n+        common_dims = (dim,) + common_dims\n+    for var, dim_len in zip(vars, concat_dim_lengths):\n+        if var.dims != common_dims:\n+            common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+            var = var.set_dims(common_dims, common_shape)\n+        yield var\n+\n+# stack up each variable to fill-out the dataset (in order)\n+# n.b. this loop preserves variable order, needed for groupby.\n+for k in datasets[0].variables:\n+    if k in concat_over:\n+        try:\n+            # Get variables that exist, use None for missing ones\n+            vars_list = []\n+            for ds in datasets:\n+                if k in ds.variables:\n+                    vars_list.append(ds.variables[k])\n+                else:\n+                    # Create a dummy variable with NaN values for missing variables\n+                    if k in coord_names:\n+                        # For coordinates, use the first available variable as template\n+                        template_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n+                        dummy_var = template_var._constructor(\n+                            template_var.dims, \n+                            dtypes.full_like(template_var, fill_value, dtype=template_var.dtype),\n+                            template_var.attrs\n+                        )\n+                    else:\n+                        # For data variables, create minimal compatible variable\n+                        dummy_var = Variable((dim,), [fill_value])\n+                    vars_list.append(dummy_var)\n+            \n+            vars = ensure_common_dims(vars_list)\n+        except KeyError:\n+            raise ValueError(\"%r is not present in all datasets.\" % k)\n+        combined = concat_vars(vars, dim, positions)\n+        assert isinstance(combined, Variable)\n+        result_vars[k] = combined\n+\n+result = Dataset(result_vars, attrs=result_attrs)\n+absent_coord_names = coord_names - set(result.variables)\n+if absent_coord_names:\n+    raise ValueError(\n+        \"Variables %r are coordinates in some datasets but not others.\"\n+        % absent_coord_names\n+    )\n+result = result.set_coords(coord_names)\n+result.encoding = result_encoding\n+\n+result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n+if coord is not None:\n+    # add concat dimension last to ensure that its in the final Dataset\n+    result[coord.name] = coord\n+\n+return result\n+                        continue\n+            \n+            if vars_list:\n+                try:\n+                    vars = ensure_common_dims(vars_list)\n+                    combined = concat_vars(vars, dim, positions)\n+                    assert isinstance(combined, Variable)\n+                    result_vars[k] = combined\n+                except Exception:\n+                    # If concatenation fails, skip this variable\n+                    continue\n \n     result = Dataset(result_vars, attrs=result_attrs)\n     absent_coord_names = coord_names - set(result.variables)\n",
    "variant_2": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..3c7058ae 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -178,59 +178,72 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n-    def process_subset_opt(opt, subset):\n-        if isinstance(opt, str):\n-            if opt == \"different\":\n-                if compat == \"override\":\n-                    raise ValueError(\n-                        \"Cannot specify both %s='different' and compat='override'.\"\n-                        % subset\n-                    )\n-                # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n-                    if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n-                        # We'll need to know later on if variables are equal.\n-                        computed = []\n-                        for ds_rhs in datasets[1:]:\n-                            v_rhs = ds_rhs.variables[k].compute()\n-                            computed.append(v_rhs)\n-                            if not getattr(v_lhs, compat)(v_rhs):\n-                                concat_over.add(k)\n-                                equals[k] = False\n-                                # computed variables are not to be re-computed\n-                                # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n-                                    ds.variables[k].data = v.data\n-                                break\n-                        else:\n-                            equals[k] = True\n-\n-            elif opt == \"all\":\n-                concat_over.update(\n-                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n+\n+def process_subset_opt(opt, subset):\n+    if isinstance(opt, str):\n+        if opt == \"different\":\n+            if compat == \"override\":\n+                raise ValueError(\n+                    \"Cannot specify both %s='different' and compat='override'.\"\n+                    % subset\n                 )\n-            elif opt == \"minimal\":\n-                pass\n-            else:\n-                raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n+            # all nonindexes that are not the same in each dataset\n+            for k in getattr(datasets[0], subset):\n+                if k not in concat_over:\n+                    # Compare the variable of all datasets vs. the one\n+                    # of the first dataset. Perform the minimum amount of\n+                    # loads in order to avoid multiple loads from disk\n+                    # while keeping the RAM footprint low.\n+                    v_lhs = datasets[0].variables[k].load()\n+                    # We'll need to know later on if variables are equal.\n+                    computed = []\n+                    for ds_rhs in datasets[1:]:\n+                        v_rhs = ds_rhs.variables[k].compute()\n+                        computed.append(v_rhs)\n+                        if not getattr(v_lhs, compat)(v_rhs):\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            # computed variables are not to be re-computed\n+                            # again in the future\n+                            for ds, v in zip(datasets[1:], computed):\n+                                ds.variables[k].data = v.data\n+                            break\n+                    else:\n+                        equals[k] = True\n+\n+        elif opt == \"all\":\n+            concat_over.update(\n+                set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n+            )\n+        elif opt == \"minimal\":\n+            pass\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n-            if invalid_vars:\n-                if subset == \"coords\":\n-                    raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n-                    )\n-                else:\n+            raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n+    else:\n+        # Collect variables from all datasets for outer join behavior\n+        all_vars_in_subset = set()\n+        for ds in datasets:\n+            all_vars_in_subset.update(getattr(ds, subset))\n+        \n+        # Check if specified variables exist in at least one dataset\n+        invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n+        if invalid_vars:\n+            if subset == \"coords\":\n+                raise ValueError(\n+                    \"some variables in coords are not coordinates on \"\n+                    \"any dataset: %s\" % (invalid_vars,)\n+                )\n+            else:\n+                raise ValueError(\n+                    \"some variables in data_vars are not data variables \"\n+                    \"on any dataset: %s\" % (invalid_vars,)\n+                )\n+        concat_over.update(opt)\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n+            concat_over.update(valid_vars)\n             concat_over.update(opt)\n \n     process_subset_opt(data_vars, \"data_vars\")\n@@ -311,13 +324,15 @@ def _dataset_concat(\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n \n+\n         for ds in datasets:\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+            # Allow variables to be present in some datasets but not others\n+            # The align() function will handle missing values with fill_value\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                # If variable is missing from this dataset, it will be handled by align()\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -347,21 +362,51 @@ def _dataset_concat(\n         # concat dimension\n         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n         if dim not in common_dims:\n-            common_dims = (dim,) + common_dims\n-        for var, dim_len in zip(vars, concat_dim_lengths):\n-            if var.dims != common_dims:\n-                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n-                var = var.set_dims(common_dims, common_shape)\n-            yield var\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n     for k in datasets[0].variables:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+            vars_list = []\n+            for ds in datasets:\n+                if k in ds.variables:\n+                    vars_list.append(ds.variables[k])\n+                else:\n+                    # Create a variable filled with fill_value for missing variables\n+                    first_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n+                    missing_var = Variable(\n+                        first_var.dims, \n+                        utils.full_like(first_var, fill_value, dtype=first_var.dtype)\n+                    )\n+                    vars_list.append(missing_var)\n+            \n+            vars = ensure_common_dims(vars_list)\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n+    absent_coord_names = coord_names - set(result.variables)\n+    if absent_coord_names:\n+        raise ValueError(\n+            \"Variables %r are coordinates in some datasets but not others.\"\n+            % absent_coord_names\n+        )\n+    result = result.set_coords(coord_names)\n+    result.encoding = result_encoding\n+\n+    result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n+    if coord is not None:\n+        # add concat dimension last to ensure that its in the final Dataset\n+        result[coord.name] = coord\n+\n+    return result\n+                    data = np.full(var_shape, fill_val, dtype=ref_var.dtype)\n+                    missing_var = Variable(var_dims, data, ref_var.attrs.copy())\n+                    vars.append(missing_var)\n+            \n+            vars = ensure_common_dims(vars)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n",
    "variant_3": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..fe94d6b4 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -178,6 +178,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n+\n     def process_subset_opt(opt, subset):\n         if isinstance(opt, str):\n             if opt == \"different\":\n@@ -219,18 +220,35 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+            # Collect variables from all datasets for outer join behavior\n+            all_vars_in_subset = set()\n+            for ds in datasets:\n+                all_vars_in_subset.update(getattr(ds, subset))\n+            \n+            # Check if specified variables exist in at least one dataset\n+            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n+                    )\n+                else:\n+                    raise ValueError(\n+                        \"some variables in data_vars are not data variables \"\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n+            concat_over.update(opt)\n+\n+    process_subset_opt(data_vars, \"data_vars\")\n+    process_subset_opt(coords, \"coords\")\n+    return concat_over, equals, concat_dim_lengths\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n+            concat_over.update(valid_vars)\n             concat_over.update(opt)\n \n     process_subset_opt(data_vars, \"data_vars\")\n@@ -311,13 +329,17 @@ def _dataset_concat(\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n \n+\n         for ds in datasets:\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n             if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+                # For outer join behavior, collect variables that exist in any dataset\n+                # and let align() handle missing values with fill_value\n+                for var in absent_merge_vars:\n+                    if var not in to_merge:\n+                        to_merge[var] = []\n+                # Skip datasets that don't have certain variables - they'll be handled by align()\n+                continue\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -347,20 +369,63 @@ def _dataset_concat(\n         # concat dimension\n         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n         if dim not in common_dims:\n-            common_dims = (dim,) + common_dims\n-        for var, dim_len in zip(vars, concat_dim_lengths):\n-            if var.dims != common_dims:\n-                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n-                var = var.set_dims(common_dims, common_shape)\n-            yield var\n-\n-    # stack up each variable to fill-out the dataset (in order)\n-    # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n-        if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n+\n+def ensure_common_dims(vars):\n+    # ensure each variable with the given name shares the same\n+    # dimensions and the same shape for all of them except along the\n+    # concat dimension\n+    common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n+    if dim not in common_dims:\n+        common_dims = (dim,) + common_dims\n+    for var, dim_len in zip(vars, concat_dim_lengths):\n+        if var.dims != common_dims:\n+            common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+            var = var.set_dims(common_dims, common_shape)\n+        yield var\n+\n+# stack up each variable to fill-out the dataset (in order)\n+# n.b. this loop preserves variable order, needed for groupby.\n+for k in datasets[0].variables:\n+    if k in concat_over:\n+        # Handle missing variables gracefully for outer join behavior\n+        vars_list = []\n+        for ds in datasets:\n+            if k in ds.variables:\n+                vars_list.append(ds.variables[k])\n+            else:\n+                # Create a dummy variable filled with fill_value for missing variables\n+                dummy_var = Variable(datasets[0].variables[k].dims, \n+                                   utils.full_like(datasets[0].variables[k], fill_value))\n+                vars_list.append(dummy_var)\n+        \n+        vars = ensure_common_dims(vars_list)\n+        combined = concat_vars(vars, dim, positions)\n+        assert isinstance(combined, Variable)\n+        result_vars[k] = combined\n+\n+result = Dataset(result_vars, attrs=result_attrs)\n+absent_coord_names = coord_names - set(result.variables)\n+if absent_coord_names:\n+    raise ValueError(\n+        \"Variables %r are coordinates in some datasets but not others.\"\n+        % absent_coord_names\n+    )\n+result = result.set_coords(coord_names)\n+result.encoding = result_encoding\n+\n+result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n+if coord is not None:\n+    # add concat dimension last to ensure that its in the final Dataset\n+    result[coord.name] = coord\n+\n+return result\n+                            template_var.encoding.copy()\n+                        )\n+                        vars_to_concat.append(nan_var)\n+                \n+                vars = ensure_common_dims(vars_to_concat)\n+            except (KeyError, StopIteration):\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n",
    "variant_4": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..d2c5a92f 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -178,59 +178,73 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n-    def process_subset_opt(opt, subset):\n-        if isinstance(opt, str):\n-            if opt == \"different\":\n-                if compat == \"override\":\n-                    raise ValueError(\n-                        \"Cannot specify both %s='different' and compat='override'.\"\n-                        % subset\n-                    )\n-                # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n-                    if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n-                        # We'll need to know later on if variables are equal.\n-                        computed = []\n-                        for ds_rhs in datasets[1:]:\n-                            v_rhs = ds_rhs.variables[k].compute()\n-                            computed.append(v_rhs)\n-                            if not getattr(v_lhs, compat)(v_rhs):\n-                                concat_over.add(k)\n-                                equals[k] = False\n-                                # computed variables are not to be re-computed\n-                                # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n-                                    ds.variables[k].data = v.data\n-                                break\n-                        else:\n-                            equals[k] = True\n \n-            elif opt == \"all\":\n-                concat_over.update(\n-                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n+def process_subset_opt(opt, subset):\n+    if isinstance(opt, str):\n+        if opt == \"different\":\n+            if compat == \"override\":\n+                raise ValueError(\n+                    \"Cannot specify both %s='different' and compat='override'.\"\n+                    % subset\n                 )\n-            elif opt == \"minimal\":\n-                pass\n-            else:\n-                raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n+            # all nonindexes that are not the same in each dataset\n+            for k in getattr(datasets[0], subset):\n+                if k not in concat_over:\n+                    # Compare the variable of all datasets vs. the one\n+                    # of the first dataset. Perform the minimum amount of\n+                    # loads in order to avoid multiple loads from disk\n+                    # while keeping the RAM footprint low.\n+                    v_lhs = datasets[0].variables[k].load()\n+                    # We'll need to know later on if variables are equal.\n+                    computed = []\n+                    for ds_rhs in datasets[1:]:\n+                        v_rhs = ds_rhs.variables[k].compute()\n+                        computed.append(v_rhs)\n+                        if not getattr(v_lhs, compat)(v_rhs):\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            # computed variables are not to be re-computed\n+                            # again in the future\n+                            for ds, v in zip(datasets[1:], computed):\n+                                ds.variables[k].data = v.data\n+                            break\n+                    else:\n+                        equals[k] = True\n+\n+        elif opt == \"all\":\n+            concat_over.update(\n+                set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n+            )\n+        elif opt == \"minimal\":\n+            pass\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n-            if invalid_vars:\n-                if subset == \"coords\":\n-                    raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n-                    )\n+            raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n+    else:\n+        # Collect all variables of the specified subset type across all datasets\n+        all_subset_vars = set()\n+        for ds in datasets:\n+            all_subset_vars.update(getattr(ds, subset))\n+        \n+        # Check if specified variables exist in any dataset (outer join behavior)\n+        invalid_vars = [k for k in opt if k not in all_subset_vars]\n+        if invalid_vars:\n+            if subset == \"coords\":\n+                raise ValueError(\n+                    \"some variables in coords are not coordinates on \"\n+                    \"any dataset: %s\" % (invalid_vars,)\n+                )\n+            else:\n+                raise ValueError(\n+                    \"some variables in data_vars are not data variables \"\n+                    \"on any dataset: %s\" % (invalid_vars,)\n+                )\n+        concat_over.update(opt)\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n+            concat_over.update(valid_vars)\n             concat_over.update(opt)\n \n     process_subset_opt(data_vars, \"data_vars\")\n@@ -311,13 +325,13 @@ def _dataset_concat(\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n \n+\n         for ds in datasets:\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+            # Skip validation for absent variables - they will be handled by alignment\n+            # This enables outer join behavior similar to pandas concat\n+            for var in variables_to_merge.intersection(set(ds.variables)):\n+                to_merge[var].append(ds.variables[var])\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -347,24 +361,42 @@ def _dataset_concat(\n         # concat dimension\n         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n         if dim not in common_dims:\n-            common_dims = (dim,) + common_dims\n-        for var, dim_len in zip(vars, concat_dim_lengths):\n-            if var.dims != common_dims:\n-                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n-                var = var.set_dims(common_dims, common_shape)\n-            yield var\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n     for k in datasets[0].variables:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n-            combined = concat_vars(vars, dim, positions)\n-            assert isinstance(combined, Variable)\n-            result_vars[k] = combined\n+            vars_list = []\n+            for ds in datasets:\n+                if k in ds.variables:\n+                    vars_list.append(ds.variables[k])\n+                else:\n+                    # Create a variable filled with fill_value for missing variables\n+                    # Get the common dimensions from existing variables\n+                    if vars_list:\n+                        template_var = vars_list[0]\n+                        dims = template_var.dims\n+                        shape = template_var.shape\n+                    else:\n+                        # If this is the first variable, we need to determine dims from other datasets\n+                        for other_ds in datasets:\n+                            if k in other_ds.variables:\n+                                template_var = other_ds.variables[k]\n+                                dims = template_var.dims\n+                                shape = template_var.shape\n+                                break\n+                    \n+                    # Create a variable with the same structure but filled with fill_value\n+                    import numpy as np\n+                    fill_data = np.full(shape, fill_value, dtype=template_var.dtype)\n+                    fill_var = Variable(dims, fill_data, attrs=template_var.attrs)\n+                    vars_list.append(fill_var)\n+            \n+            if vars_list:\n+                vars = ensure_common_dims(vars_list)\n+                combined = concat_vars(vars, dim, positions)\n+                assert isinstance(combined, Variable)\n+                result_vars[k] = combined\n \n     result = Dataset(result_vars, attrs=result_attrs)\n     absent_coord_names = coord_names - set(result.variables)\n@@ -378,6 +410,17 @@ def _dataset_concat(\n \n     result = result.drop(unlabeled_dims, errors=\"ignore\")\n \n+    if coord is not None:\n+        # add concat dimension last to ensure that its in the final Dataset\n+        result[coord.name] = coord\n+\n+    return result\n+        )\n+    result = result.set_coords(coord_names)\n+    result.encoding = result_encoding\n+\n+    result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n     if coord is not None:\n         # add concat dimension last to ensure that its in the final Dataset\n         result[coord.name] = coord\n",
    "variant_5": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..12ce3ed9 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -178,6 +178,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n+\n     def process_subset_opt(opt, subset):\n         if isinstance(opt, str):\n             if opt == \"different\":\n@@ -219,18 +220,35 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+            # Collect all variables of this subset type across all datasets\n+            all_subset_vars = set()\n+            for ds in datasets:\n+                all_subset_vars.update(getattr(ds, subset))\n+            \n+            # Check if specified variables exist in at least one dataset\n+            invalid_vars = [k for k in opt if k not in all_subset_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n+                    )\n+            concat_over.update(opt)\n+\n+    process_subset_opt(data_vars, \"data_vars\")\n+    process_subset_opt(coords, \"coords\")\n+    return concat_over, equals, concat_dim_lengths\n+                else:\n+                    raise ValueError(\n+                        \"some variables in data_vars are not data variables \"\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n+            concat_over.update(valid_vars)\n             concat_over.update(opt)\n \n     process_subset_opt(data_vars, \"data_vars\")\n@@ -311,13 +329,19 @@ def _dataset_concat(\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n \n+\n         for ds in datasets:\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n             if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+                # For outer join behavior, collect variables that exist in any dataset\n+                # and let align() handle missing values with fill_value\n+                for var in absent_merge_vars:\n+                    if var not in to_merge:\n+                        to_merge[var] = []\n+                # Add placeholders for missing variables in this dataset\n+                for var in absent_merge_vars:\n+                    # Create a placeholder that will be handled by align()\n+                    to_merge[var].append(None)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -347,12 +371,6 @@ def _dataset_concat(\n         # concat dimension\n         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n         if dim not in common_dims:\n-            common_dims = (dim,) + common_dims\n-        for var, dim_len in zip(vars, concat_dim_lengths):\n-            if var.dims != common_dims:\n-                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n-                var = var.set_dims(common_dims, common_shape)\n-            yield var\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n@@ -361,7 +379,18 @@ def _dataset_concat(\n             try:\n                 vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n             except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+                # Handle missing variables with outer join behavior\n+                vars_with_missing = []\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars_with_missing.append(ds.variables[k])\n+                    else:\n+                        # Create a dummy variable filled with fill_value\n+                        first_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n+                        dummy_var = Variable(first_var.dims, \n+                                           dtypes.full_like(first_var, fill_value, dtype=first_var.dtype))\n+                        vars_with_missing.append(dummy_var)\n+                vars = ensure_common_dims(vars_with_missing)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n@@ -383,6 +412,22 @@ def _dataset_concat(\n         result[coord.name] = coord\n \n     return result\n+    absent_coord_names = coord_names - set(result.variables)\n+    if absent_coord_names:\n+        raise ValueError(\n+            \"Variables %r are coordinates in some datasets but not others.\"\n+            % absent_coord_names\n+        )\n+    result = result.set_coords(coord_names)\n+    result.encoding = result_encoding\n+\n+    result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n+    if coord is not None:\n+        # add concat dimension last to ensure that its in the final Dataset\n+        result[coord.name] = coord\n+\n+    return result\n \n \n def _dataarray_concat(\n",
    "variant_6": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..cfe26d68 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -178,6 +178,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n+\n     def process_subset_opt(opt, subset):\n         if isinstance(opt, str):\n             if opt == \"different\":\n@@ -219,17 +220,31 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+            # Collect all variables of this subset type across all datasets\n+            all_subset_vars = set()\n+            for ds in datasets:\n+                all_subset_vars.update(getattr(ds, subset))\n+            \n+            # Check if specified variables exist in at least one dataset\n+            invalid_vars = [k for k in opt if k not in all_subset_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n+                    )\n+            concat_over.update(opt)\n+\n+    process_subset_opt(data_vars, \"data_vars\")\n+    process_subset_opt(coords, \"coords\")\n+    return concat_over, equals, concat_dim_lengths\n+                        \"some variables in data_vars are not data variables \"\n+                        \"on any dataset: %s\" % (truly_invalid,)\n                     )\n             concat_over.update(opt)\n \n@@ -311,13 +326,24 @@ def _dataset_concat(\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n \n+\n         for ds in datasets:\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n             if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+                # For outer join behavior, collect variables that exist in any dataset\n+                # and let align() handle missing values with fill_value\n+                for var in absent_merge_vars:\n+                    if var not in to_merge:\n+                        to_merge[var] = []\n+                # Add placeholder None for missing variables to maintain dataset order\n+                for var in variables_to_merge:\n+                    if var in ds.variables:\n+                        to_merge[var].append(ds.variables[var])\n+                    else:\n+                        to_merge[var].append(None)\n+            else:\n+                for var in variables_to_merge:\n+                    to_merge[var].append(ds.variables[var])\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -347,30 +373,69 @@ def _dataset_concat(\n         # concat dimension\n         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n         if dim not in common_dims:\n-            common_dims = (dim,) + common_dims\n-        for var, dim_len in zip(vars, concat_dim_lengths):\n-            if var.dims != common_dims:\n-                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n-                var = var.set_dims(common_dims, common_shape)\n-            yield var\n-\n-    # stack up each variable to fill-out the dataset (in order)\n-    # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n-        if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n-            combined = concat_vars(vars, dim, positions)\n-            assert isinstance(combined, Variable)\n-            result_vars[k] = combined\n-\n-    result = Dataset(result_vars, attrs=result_attrs)\n-    absent_coord_names = coord_names - set(result.variables)\n-    if absent_coord_names:\n-        raise ValueError(\n-            \"Variables %r are coordinates in some datasets but not others.\"\n+\n+def ensure_common_dims(vars):\n+    # ensure each variable with the given name shares the same\n+    # dimensions and the same shape for all of them except along the\n+    # concat dimension\n+    common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n+    if dim not in common_dims:\n+        common_dims = (dim,) + common_dims\n+    for var, dim_len in zip(vars, concat_dim_lengths):\n+        if var.dims != common_dims:\n+            common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+            var = var.set_dims(common_dims, common_shape)\n+        yield var\n+\n+# stack up each variable to fill-out the dataset (in order)\n+# n.b. this loop preserves variable order, needed for groupby.\n+for k in datasets[0].variables:\n+    if k in concat_over:\n+        # Handle missing variables gracefully for outer join behavior\n+        available_vars = []\n+        for ds in datasets:\n+            if k in ds.variables:\n+                available_vars.append(ds.variables[k])\n+            else:\n+                # Create a dummy variable filled with fill_value for missing variables\n+                # Use the common dimensions and appropriate shape\n+                if available_vars:\n+                    template_var = available_vars[0]\n+                    dummy_dims = template_var.dims\n+                    dummy_shape = template_var.shape\n+                else:\n+                    # If this is the first variable, use minimal dimensions\n+                    dummy_dims = (dim,)\n+                    dummy_shape = (concat_dim_lengths[len(available_vars)],)\n+                \n+                dummy_var = Variable(dummy_dims, \n+                                   utils.full_like(template_var.data if available_vars else 0, \n+                                                 fill_value, shape=dummy_shape),\n+                                   attrs={})\n+                available_vars.append(dummy_var)\n+        \n+        vars = ensure_common_dims(available_vars)\n+        combined = concat_vars(vars, dim, positions)\n+        assert isinstance(combined, Variable)\n+        result_vars[k] = combined\n+\n+result = Dataset(result_vars, attrs=result_attrs)\n+absent_coord_names = coord_names - set(result.variables)\n+if absent_coord_names:\n+    raise ValueError(\n+        \"Variables %r are coordinates in some datasets but not others.\"\n+        % absent_coord_names\n+    )\n+result = result.set_coords(coord_names)\n+result.encoding = result_encoding\n+\n+result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n+if coord is not None:\n+    # add concat dimension last to ensure that its in the final Dataset\n+    result[coord.name] = coord\n+\n+return result\n             % absent_coord_names\n         )\n     result = result.set_coords(coord_names)\n",
    "variant_7": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..0dba9399 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -178,59 +178,72 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n-    def process_subset_opt(opt, subset):\n-        if isinstance(opt, str):\n-            if opt == \"different\":\n-                if compat == \"override\":\n-                    raise ValueError(\n-                        \"Cannot specify both %s='different' and compat='override'.\"\n-                        % subset\n-                    )\n-                # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n-                    if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n-                        # We'll need to know later on if variables are equal.\n-                        computed = []\n-                        for ds_rhs in datasets[1:]:\n-                            v_rhs = ds_rhs.variables[k].compute()\n-                            computed.append(v_rhs)\n-                            if not getattr(v_lhs, compat)(v_rhs):\n-                                concat_over.add(k)\n-                                equals[k] = False\n-                                # computed variables are not to be re-computed\n-                                # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n-                                    ds.variables[k].data = v.data\n-                                break\n-                        else:\n-                            equals[k] = True\n-\n-            elif opt == \"all\":\n-                concat_over.update(\n-                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n+\n+def process_subset_opt(opt, subset):\n+    if isinstance(opt, str):\n+        if opt == \"different\":\n+            if compat == \"override\":\n+                raise ValueError(\n+                    \"Cannot specify both %s='different' and compat='override'.\"\n+                    % subset\n                 )\n-            elif opt == \"minimal\":\n-                pass\n-            else:\n-                raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n+            # all nonindexes that are not the same in each dataset\n+            for k in getattr(datasets[0], subset):\n+                if k not in concat_over:\n+                    # Compare the variable of all datasets vs. the one\n+                    # of the first dataset. Perform the minimum amount of\n+                    # loads in order to avoid multiple loads from disk\n+                    # while keeping the RAM footprint low.\n+                    v_lhs = datasets[0].variables[k].load()\n+                    # We'll need to know later on if variables are equal.\n+                    computed = []\n+                    for ds_rhs in datasets[1:]:\n+                        v_rhs = ds_rhs.variables[k].compute()\n+                        computed.append(v_rhs)\n+                        if not getattr(v_lhs, compat)(v_rhs):\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            # computed variables are not to be re-computed\n+                            # again in the future\n+                            for ds, v in zip(datasets[1:], computed):\n+                                ds.variables[k].data = v.data\n+                            break\n+                    else:\n+                        equals[k] = True\n+\n+        elif opt == \"all\":\n+            concat_over.update(\n+                set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n+            )\n+        elif opt == \"minimal\":\n+            pass\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n-            if invalid_vars:\n-                if subset == \"coords\":\n-                    raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n-                    )\n+            raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n+    else:\n+        # Collect variables from all datasets for outer join behavior\n+        all_vars_in_subset = set()\n+        for ds in datasets:\n+            all_vars_in_subset.update(getattr(ds, subset))\n+        \n+        invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n+        if invalid_vars:\n+            if subset == \"coords\":\n+                raise ValueError(\n+                    \"some variables in coords are not coordinates in \"\n+                    \"any dataset: %s\" % (invalid_vars,)\n+                )\n+            else:\n+                raise ValueError(\n+                    \"some variables in data_vars are not data variables \"\n+                    \"in any dataset: %s\" % (invalid_vars,)\n+                )\n+        concat_over.update(opt)\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n+            concat_over.update(valid_vars)\n             concat_over.update(opt)\n \n     process_subset_opt(data_vars, \"data_vars\")\n@@ -311,13 +324,13 @@ def _dataset_concat(\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n \n+\n         for ds in datasets:\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n             if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+                # For outer join behavior, skip datasets that don't have all merge variables\n+                # The align() function will handle filling missing values appropriately\n+                pass\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -347,25 +360,36 @@ def _dataset_concat(\n         # concat dimension\n         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n         if dim not in common_dims:\n-            common_dims = (dim,) + common_dims\n-        for var, dim_len in zip(vars, concat_dim_lengths):\n-            if var.dims != common_dims:\n-                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n-                var = var.set_dims(common_dims, common_shape)\n-            yield var\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n     for k in datasets[0].variables:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n-            combined = concat_vars(vars, dim, positions)\n-            assert isinstance(combined, Variable)\n-            result_vars[k] = combined\n-\n+            vars_list = []\n+            for ds in datasets:\n+                if k in ds.variables:\n+                    vars_list.append(ds.variables[k])\n+                else:\n+                    # Create a variable filled with fill_value for missing variables\n+                    # Use the first available variable as template for dims and dtype\n+                    template_var = None\n+                    for other_ds in datasets:\n+                        if k in other_ds.variables:\n+                            template_var = other_ds.variables[k]\n+                            break\n+                    \n+                    if template_var is not None:\n+                        # Create variable with same dims but filled with fill_value\n+                        import numpy as np\n+                        fill_data = np.full(template_var.shape, fill_value, dtype=template_var.dtype)\n+                        missing_var = Variable(template_var.dims, fill_data, template_var.attrs)\n+                        vars_list.append(missing_var)\n+            \n+            if vars_list:  # Only process if we have at least one variable\n+                vars = ensure_common_dims(vars_list)\n+                combined = concat_vars(vars, dim, positions)\n+                assert isinstance(combined, Variable)\n+                result_vars[k] = combined\n     result = Dataset(result_vars, attrs=result_attrs)\n     absent_coord_names = coord_names - set(result.variables)\n     if absent_coord_names:\n",
    "variant_8": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..9e2b0790 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -178,6 +178,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n+\n     def process_subset_opt(opt, subset):\n         if isinstance(opt, str):\n             if opt == \"different\":\n@@ -219,18 +220,32 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+            # Collect variables that exist in ANY dataset for outer join behavior\n+            all_vars_in_subset = set()\n+            for ds in datasets:\n+                all_vars_in_subset.update(getattr(ds, subset))\n+            \n+            # Only validate against variables that exist in at least one dataset\n+            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n+            # Only add variables that actually exist in at least one dataset\n+            concat_over.update(k for k in opt if k in all_vars_in_subset)\n+                else:\n+                    raise ValueError(\n+                        \"some variables in data_vars are not data variables \"\n+                        \"on any dataset: %s\" % (invalid_vars,)\n+                    )\n+            concat_over.update(valid_vars)\n             concat_over.update(opt)\n \n     process_subset_opt(data_vars, \"data_vars\")\n@@ -311,13 +326,18 @@ def _dataset_concat(\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n \n+\n         for ds in datasets:\n             absent_merge_vars = variables_to_merge - set(ds.variables)\n             if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+                # For outer join behavior, collect variables from all datasets\n+                # and let align() handle missing values with fill_value\n+                for var in absent_merge_vars:\n+                    if var not in to_merge:\n+                        to_merge[var] = []\n+                # Add empty placeholders for missing variables in this dataset\n+                # They will be filled by align() later\n+                continue\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -347,25 +367,63 @@ def _dataset_concat(\n         # concat dimension\n         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n         if dim not in common_dims:\n-            common_dims = (dim,) + common_dims\n-        for var, dim_len in zip(vars, concat_dim_lengths):\n-            if var.dims != common_dims:\n-                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n-                var = var.set_dims(common_dims, common_shape)\n-            yield var\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n     for k in datasets[0].variables:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n-            combined = concat_vars(vars, dim, positions)\n-            assert isinstance(combined, Variable)\n-            result_vars[k] = combined\n+            # Collect variables that exist, handling missing ones gracefully\n+            vars_to_concat = []\n+            for ds in datasets:\n+                if k in ds.variables:\n+                    vars_to_concat.append(ds.variables[k])\n+                else:\n+                    # Create a variable filled with fill_value for missing variables\n+                    # Use the dimensions and shape from the first available variable\n+                    if vars_to_concat:\n+                        template_var = vars_to_concat[0]\n+                        missing_var = Variable(\n+                            template_var.dims,\n+                            dtypes.full_like(template_var, fill_value, dtype=template_var.dtype),\n+                            template_var.attrs\n+                        )\n+                        vars_to_concat.append(missing_var)\n+                    else:\n+                        # If this is the first variable, we need to find a template from other datasets\n+                        for other_ds in datasets:\n+                            if k in other_ds.variables:\n+                                template_var = other_ds.variables[k]\n+                                missing_var = Variable(\n+                                    template_var.dims,\n+                                    dtypes.full_like(template_var, fill_value, dtype=template_var.dtype),\n+                                    template_var.attrs\n+                                )\n+                                vars_to_concat.append(missing_var)\n+                                break\n+            \n+            if vars_to_concat:\n+                vars = ensure_common_dims(vars_to_concat)\n+                combined = concat_vars(vars, dim, positions)\n+                assert isinstance(combined, Variable)\n+                result_vars[k] = combined\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n+    absent_coord_names = coord_names - set(result.variables)\n+    if absent_coord_names:\n+        raise ValueError(\n+            \"Variables %r are coordinates in some datasets but not others.\"\n+            % absent_coord_names\n+        )\n+    result = result.set_coords(coord_names)\n+    result.encoding = result_encoding\n \n+    result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n+    if coord is not None:\n+        # add concat dimension last to ensure that its in the final Dataset\n+        result[coord.name] = coord\n+\n+    return result\n     result = Dataset(result_vars, attrs=result_attrs)\n     absent_coord_names = coord_names - set(result.variables)\n     if absent_coord_names:\n"
  }
}