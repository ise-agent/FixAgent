{
  "zero_patches": {
    "Location 1 (/root/hy/projects_4/xarray/xarray/core/concat.py lines 222-233)": "\n            # For outer join behavior, check if variables exist in ANY dataset, not just the first\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates in \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"in any dataset: %s\" % (invalid_vars,)\n                    )\n",
    "Location 2 (/root/hy/projects_4/xarray/xarray/core/concat.py lines 315-320)": "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            # For outer join behavior, handle missing variables by creating NaN-filled placeholders\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a NaN-filled variable with appropriate shape and dtype\n                    original_var = None\n                    for other_ds in datasets:\n                        if var in other_ds.variables:\n                            original_var = other_ds.variables[var]\n                            break\n                    \n                    if original_var is not None:\n                        # Create variable with same dims/coords but filled with NaN\n                        shape = tuple(ds.dims.get(d, original_var.sizes.get(d, 1)) for d in original_var.dims)\n                        data = utils.full_like(original_var.data, fill_value=dtypes.get_fill_value(original_var.dtype), \n                                             shape=shape, dtype=original_var.dtype)\n                        attrs = original_var.attrs.copy() if original_var.attrs else {}\n                        nan_var = Variable(original_var.dims, data, attrs)\n                        to_merge[var].append(nan_var)\n",
    "Location 3 (/root/hy/projects_4/xarray/xarray/core/concat.py lines 359-367)": "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    \n    # Collect all unique variable names from all datasets\n    all_var_names = set()\n    for ds in datasets:\n        all_var_names.update(ds.variables.keys())\n    \n    for k in all_var_names:\n        if k in concat_over:\n            # Get variables from datasets that have this variable\n            vars_with_k = []\n            datasets_with_k = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_with_k.append(ds.variables[k])\n                    datasets_with_k.append(ds)\n            \n            if len(vars_with_k) == len(datasets):\n                # All datasets have this variable - normal case\n                vars = ensure_common_dims(vars_with_k)\n            else:\n                # Some datasets are missing this variable - create fill values\n                if not vars_with_k:\n                    continue  # Skip if no datasets have this variable\n                \n                # Use first available variable as template\n                template_var = vars_with_k[0]\n                all_vars = []\n                \n                for ds in datasets:\n                    if k in ds.variables:\n                        all_vars.append(ds.variables[k])\n                    else:\n                        # Create a variable filled with NaN/fill_value for missing datasets\n                        fill_value = fill_value if fill_value is not None else (\n                            dtypes.NA if template_var.dtype.kind in 'UO' else \n                            dtypes.get_fill_value(template_var.dtype)\n                        )\n                        \n                        # Create variable with same dims as template but filled with fill_value\n                        fill_data = utils.full_like(template_var, fill_value, template_var.dtype)\n                        fill_var = Variable(template_var.dims, fill_data, template_var.attrs)\n                        all_vars.append(fill_var)\n                \n                vars = ensure_common_dims(all_vars)\n            \n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
    "Location 4 (/root/hy/projects_4/xarray/xarray/core/concat.py lines 190-211)": "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # Collect all variables from all datasets\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                # Check which variables differ across datasets\n                for k in all_vars:\n                    if k not in concat_over:\n                        # Skip variables that don't exist in all datasets\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) < len(datasets):\n                            # Variable missing from some datasets, include in concat\n                            concat_over.add(k)\n                            equals[k] = False\n                            continue\n                            \n                        # Compare the variable across all datasets\n                        v_lhs = datasets_with_var[0].variables[k].load()\n                        computed = []\n                        for ds_rhs in datasets_with_var[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets_with_var[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n",
    "Location 5 (/root/hy/projects_4/xarray/xarray/core/concat.py lines 362-364)": "\n            try:\n                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            except KeyError:\n                # Handle missing variables by creating NaN-filled variables for datasets that don't have this variable\n                vars = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars.append(ds.variables[k])\n                    else:\n                        # Create a NaN-filled variable with the same dimensions as other datasets\n                        if vars:\n                            # Use the first available variable as a template\n                            template_var = vars[0]\n                            fill_value = dtypes.NA if template_var.dtype.kind in 'fc' else dtypes.get_fill_value(template_var.dtype)\n                            missing_var = Variable(template_var.dims, \n                                                 utils.full_like(template_var.data, fill_value, dtype=template_var.dtype))\n                            vars.append(missing_var)\n                        else:\n                            # This shouldn't happen if we're iterating over variables that exist somewhere\n                            raise ValueError(\"%r is not present in all datasets.\" % k)\n                vars = ensure_common_dims(vars)\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_4/xarray/xarray/core/concat.py lines 222-233)": [
      "\n        else:\n            # For user-specified variable lists, check if variables exist in ANY dataset\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # Check if variables exist in ANY dataset, not just the first one\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates in \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"in any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # For outer join behavior, check if variables exist in ANY dataset\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # Check if variables exist in ANY dataset, not just the first\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # For outer join behavior, check if variables exist in ANY dataset, not just the first\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates in \"\n                        \"any of the datasets: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"in any of the datasets: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # For outer join behavior, check if variables exist in ANY dataset\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates in \"\n                        \"any of the datasets: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"in any of the datasets: %s\" % (invalid_vars,)\n                    )\n",
      "\n        else:\n            # Collect all variables from all datasets for validation\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n",
      "\n            # For outer join behavior, check if variables exist in ANY dataset, not just the first\n            all_vars_in_subset = set()\n            for ds in datasets:\n                all_vars_in_subset.update(getattr(ds, subset))\n            \n            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n            if invalid_vars:\n                if subset == \"coords\":\n                    raise ValueError(\n                        \"some variables in coords are not coordinates on \"\n                        \"any dataset: %s\" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        \"some variables in data_vars are not data variables \"\n                        \"on any dataset: %s\" % (invalid_vars,)\n                    )\n"
    ],
    "Location 2 (/root/hy/projects_4/xarray/xarray/core/concat.py lines 315-320)": [
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a dummy variable filled with NaN for missing variables\n                    # Get the variable from a dataset that has it to determine shape and dtype\n                    ref_var = None\n                    for ref_ds in datasets:\n                        if var in ref_ds.variables:\n                            ref_var = ref_ds.variables[var]\n                            break\n                    \n                    if ref_var is not None:\n                        # Create NaN-filled variable with same dims and dtype as reference\n                        import numpy as np\n                        shape = tuple(ds.dims.get(dim, ref_var.sizes.get(dim, 1)) \n                                    for dim in ref_var.dims)\n                        fill_value = np.nan if ref_var.dtype.kind in 'fc' else None\n                        if fill_value is not None:\n                            data = np.full(shape, fill_value, dtype=ref_var.dtype)\n                        else:\n                            # For non-numeric dtypes, use object array with None\n                            data = np.full(shape, None, dtype=object)\n                        \n                        dummy_var = Variable(ref_var.dims, data, attrs=ref_var.attrs)\n                        to_merge[var].append(dummy_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    # Get the shape from the first available instance of this variable\n                    ref_var = None\n                    for ref_ds in datasets:\n                        if var in ref_ds.variables:\n                            ref_var = ref_ds.variables[var]\n                            break\n                    \n                    if ref_var is not None:\n                        # Create NaN-filled variable with same dtype and dims as reference\n                        fill_value = dtypes.NA if ref_var.dtype.kind in 'SU' else dtypes.get_fill_value(ref_var.dtype)\n                        placeholder_data = utils.full_like(ref_var.data, fill_value, dtype=ref_var.dtype)\n                        placeholder_var = Variable(ref_var.dims, placeholder_data, ref_var.attrs)\n                        to_merge[var].append(placeholder_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            # For outer join behavior, create missing variables with NaN fill\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a NaN-filled variable with the same shape as other datasets\n                    if to_merge[var]:  # If we have other instances of this variable\n                        template_var = to_merge[var][0]\n                        # Create NaN-filled variable with same dtype and dims\n                        fill_value = dtypes.NA if template_var.dtype.kind in 'fc' else dtypes.get_fill_value(template_var.dtype)\n                        nan_var = Variable(template_var.dims, \n                                         fill_value * template_var.shape, \n                                         attrs=template_var.attrs)\n                        to_merge[var].append(nan_var)\n                    else:\n                        # This is the first occurrence, we'll handle missing vars later\n                        continue\n\n        for var in variables_to_merge:\n            result_vars[var] = unique_variable(\n                var, to_merge[var], compat=compat, equals=equals.get(var, None)\n            )\n    else:\n        result_vars = {}\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a placeholder variable filled with NaN for missing variables\n                    # Get the variable from a dataset that has it to determine shape and dtype\n                    ref_var = None\n                    for ref_ds in datasets:\n                        if var in ref_ds.variables:\n                            ref_var = ref_ds.variables[var]\n                            break\n                    \n                    if ref_var is not None:\n                        # Create NaN-filled variable with same dims and dtype as reference\n                        import numpy as np\n                        shape = tuple(ds.dims.get(dim, 1) if dim in ds.dims else ref_var.sizes[dim] \n                                    for dim in ref_var.dims)\n                        fill_value = np.nan if ref_var.dtype.kind in 'fc' else None\n                        if fill_value is not None:\n                            data = np.full(shape, fill_value, dtype=ref_var.dtype)\n                        else:\n                            # For non-numeric dtypes, create array of appropriate type\n                            data = np.empty(shape, dtype=ref_var.dtype)\n                            data.fill(None)\n                        \n                        placeholder_var = Variable(ref_var.dims, data, ref_var.attrs)\n                        to_merge[var].append(placeholder_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            # For outer join behavior, handle missing variables by creating NaN-filled ones\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a NaN-filled variable with appropriate shape and dtype\n                    ref_var = None\n                    for other_ds in datasets:\n                        if var in other_ds.variables:\n                            ref_var = other_ds.variables[var]\n                            break\n                    \n                    if ref_var is not None:\n                        # Create variable with same dims and dtype but filled with NaN\n                        shape = tuple(ds.dims.get(d, ref_var.sizes[d]) if d in ref_var.dims else ref_var.sizes[d] \n                                    for d in ref_var.dims)\n                        fill_value = dtypes.NA if ref_var.dtype.kind in 'SUO' else dtypes.get_fill_value(ref_var.dtype)\n                        data = utils.full_like(ref_var.data, fill_value, shape=shape, dtype=ref_var.dtype)\n                        nan_var = Variable(ref_var.dims, data, ref_var.attrs.copy())\n                        to_merge[var].append(nan_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            # Instead of raising error for missing variables, handle them gracefully\n            present_vars = set(ds.variables) & variables_to_merge\n            absent_vars = variables_to_merge - present_vars\n            \n            for var in present_vars:\n                to_merge[var].append(ds.variables[var])\n            \n            # For absent variables, create NaN-filled placeholder variables\n            for var in absent_vars:\n                if var in coord_names:\n                    # For coordinates, we need to be more careful about dimensions\n                    # Skip creating placeholders for coordinates that don't exist\n                    to_merge[var].append(None)\n                else:\n                    # For data variables, create NaN-filled variables with appropriate shape\n                    to_merge[var].append(None)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a dummy variable filled with NaN for missing variables\n                    if variables_to_merge and var in to_merge and to_merge[var]:\n                        # Use the first available variable as template for shape/dims\n                        template_var = to_merge[var][0]\n                        dummy_var = Variable(\n                            template_var.dims,\n                            pd.array([fill_value] * template_var.size).reshape(template_var.shape),\n                            template_var.attrs\n                        )\n                        to_merge[var].append(dummy_var)\n",
      "\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            # For outer join behavior, create missing variables with fill_value\n            for var in variables_to_merge:\n                if var in ds.variables:\n                    to_merge[var].append(ds.variables[var])\n                else:\n                    # Create a variable filled with fill_value for missing variables\n                    if var in variables_to_merge:\n                        # Find a reference variable from another dataset to get shape/dims\n                        ref_var = None\n                        for other_ds in datasets:\n                            if var in other_ds.variables:\n                                ref_var = other_ds.variables[var]\n                                break\n                        \n                        if ref_var is not None:\n                            # Create variable with same dims but filled with fill_value\n                            missing_var = Variable(\n                                ref_var.dims,\n                                dtypes.full_like(ref_var, fill_value, dtype=ref_var.dtype),\n                                ref_var.attrs\n                            )\n                            to_merge[var].append(missing_var)\n"
    ],
    "Location 3 (/root/hy/projects_4/xarray/xarray/core/concat.py lines 359-367)": [
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    \n    # Collect all unique variable names from all datasets\n    all_var_names = set()\n    for ds in datasets:\n        all_var_names.update(ds.variables.keys())\n    \n    for k in all_var_names:\n        if k in concat_over:\n            # Get variables from datasets that have this variable\n            vars_with_k = []\n            datasets_with_k = []\n            for i, ds in enumerate(datasets):\n                if k in ds.variables:\n                    vars_with_k.append(ds.variables[k])\n                    datasets_with_k.append(i)\n            \n            if not vars_with_k:\n                continue\n                \n            # If not all datasets have this variable, create missing ones with fill_value\n            if len(vars_with_k) < len(datasets):\n                # Use the first available variable as template\n                template_var = vars_with_k[0]\n                vars = []\n                \n                for i, ds in enumerate(datasets):\n                    if i in datasets_with_k:\n                        vars.append(ds.variables[k])\n                    else:\n                        # Create missing variable filled with fill_value\n                        missing_var = template_var._constructor(\n                            template_var.dims,\n                            fill_value,\n                            template_var.attrs.copy(),\n                            template_var.encoding.copy(),\n                            fastpath=True\n                        )\n                        vars.append(missing_var)\n            else:\n                vars = vars_with_k\n                \n            try:\n                vars = ensure_common_dims(vars)\n            except KeyError:\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    \n    # Collect all unique variable names from all datasets\n    all_var_names = set()\n    for ds in datasets:\n        all_var_names.update(ds.variables.keys())\n    \n    for k in all_var_names:\n        if k in concat_over:\n            # Get variables from datasets that have this variable\n            available_vars = []\n            missing_datasets = []\n            \n            for i, ds in enumerate(datasets):\n                if k in ds.variables:\n                    available_vars.append(ds.variables[k])\n                else:\n                    missing_datasets.append(i)\n            \n            if not available_vars:\n                continue  # Skip if no datasets have this variable\n                \n            # Create missing variables filled with NaN for datasets that don't have this variable\n            if missing_datasets:\n                # Use the first available variable as a template\n                template_var = available_vars[0]\n                vars_with_missing = []\n                \n                for i, ds in enumerate(datasets):\n                    if i in missing_datasets:\n                        # Create a NaN-filled variable with the same structure\n                        missing_var = Variable(\n                            template_var.dims,\n                            utils.full_like(template_var.data, fill_value=dtypes.NA),\n                            attrs=template_var.attrs\n                        )\n                        vars_with_missing.append(missing_var)\n                    else:\n                        vars_with_missing.append(ds.variables[k])\n                \n                vars = ensure_common_dims(vars_with_missing)\n            else:\n                vars = ensure_common_dims(available_vars)\n                \n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    \n    # Collect all unique variable names from all datasets\n    all_var_names = set()\n    for ds in datasets:\n        all_var_names.update(ds.variables.keys())\n    \n    for k in all_var_names:\n        if k in concat_over:\n            # Get variables from datasets that have this variable\n            available_vars = []\n            missing_indices = []\n            \n            for i, ds in enumerate(datasets):\n                if k in ds.variables:\n                    available_vars.append(ds.variables[k])\n                else:\n                    missing_indices.append(i)\n            \n            if not available_vars:\n                continue  # Skip if no datasets have this variable\n            \n            # Create missing variables filled with NaN for datasets that don't have this variable\n            if missing_indices:\n                # Use the first available variable as a template\n                template_var = available_vars[0]\n                \n                # Create NaN-filled variables for missing datasets\n                full_vars = []\n                available_idx = 0\n                \n                for i, ds in enumerate(datasets):\n                    if i in missing_indices:\n                        # Create a NaN-filled variable with the same structure as template\n                        shape = tuple(dims_sizes.get(d, concat_dim_lengths[i]) if d == dim \n                                    else dims_sizes[d] for d in template_var.dims)\n                        fill_value = dtypes.NA if template_var.dtype.kind in 'SUO' else np.nan\n                        data = np.full(shape, fill_value, dtype=template_var.dtype)\n                        missing_var = Variable(template_var.dims, data, template_var.attrs)\n                        full_vars.append(missing_var)\n                    else:\n                        full_vars.append(available_vars[available_idx])\n                        available_idx += 1\n                \n                vars = ensure_common_dims(full_vars)\n            else:\n                vars = ensure_common_dims(available_vars)\n            \n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    \n    # Collect all unique variable names from all datasets\n    all_var_names = set()\n    for ds in datasets:\n        all_var_names.update(ds.variables.keys())\n    \n    for k in all_var_names:\n        if k in concat_over:\n            # Get variables from datasets that have this variable\n            vars_list = []\n            missing_datasets = []\n            \n            for i, ds in enumerate(datasets):\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    missing_datasets.append(i)\n            \n            if vars_list:\n                # If some datasets are missing this variable, create dummy variables\n                if missing_datasets:\n                    # Use the first available variable as template\n                    template_var = vars_list[0]\n                    dummy_shape = list(template_var.shape)\n                    \n                    # Insert dummy variables for missing datasets\n                    for missing_idx in missing_datasets:\n                        # Create a variable filled with NaN with the same dims as template\n                        dummy_var = Variable(\n                            template_var.dims,\n                            dtypes.get_fill_value(template_var.dtype) * \n                            utils.ones_like(template_var.data, dtype=template_var.dtype),\n                            template_var.attrs\n                        )\n                        vars_list.insert(missing_idx, dummy_var)\n                \n                vars = ensure_common_dims(vars_list)\n                combined = concat_vars(vars, dim, positions)\n                assert isinstance(combined, Variable)\n                result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    \n    # Collect all unique variable names from all datasets\n    all_var_names = set()\n    for ds in datasets:\n        all_var_names.update(ds.variables.keys())\n    \n    for k in all_var_names:\n        if k in concat_over:\n            # Get variables from datasets that have this variable\n            vars_list = []\n            for ds in datasets:\n                if k in ds.variables:\n                    vars_list.append(ds.variables[k])\n                else:\n                    # Create a NaN-filled variable with appropriate shape\n                    if vars_list:\n                        # Use the first available variable as template\n                        template_var = vars_list[0]\n                        shape = template_var.shape\n                        dims = template_var.dims\n                        dtype = template_var.dtype\n                    else:\n                        # Find any dataset that has this variable to use as template\n                        template_var = None\n                        for other_ds in datasets:\n                            if k in other_ds.variables:\n                                template_var = other_ds.variables[k]\n                                break\n                        if template_var is None:\n                            continue  # Skip if variable not found anywhere\n                        shape = template_var.shape\n                        dims = template_var.dims\n                        dtype = template_var.dtype\n                    \n                    # Create NaN-filled variable\n                    import numpy as np\n                    if np.issubdtype(dtype, np.floating):\n                        fill_value = np.nan\n                    elif np.issubdtype(dtype, np.integer):\n                        fill_value = 0\n                    else:\n                        fill_value = None\n                    \n                    nan_data = np.full(shape, fill_value, dtype=dtype)\n                    nan_var = Variable(dims, nan_data, template_var.attrs)\n                    vars_list.append(nan_var)\n            \n            if vars_list:\n                vars = ensure_common_dims(vars_list)\n                combined = concat_vars(vars, dim, positions)\n                assert isinstance(combined, Variable)\n                result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    \n    # Collect all unique variable names from all datasets for outer join behavior\n    all_var_names = set()\n    for ds in datasets:\n        all_var_names.update(ds.variables.keys())\n    \n    for k in all_var_names:\n        if k in concat_over:\n            # Check which datasets have this variable\n            available_vars = []\n            missing_datasets = []\n            \n            for i, ds in enumerate(datasets):\n                if k in ds.variables:\n                    available_vars.append((i, ds.variables[k]))\n                else:\n                    missing_datasets.append(i)\n            \n            if not available_vars:\n                continue  # Skip if no datasets have this variable\n                \n            # Get a reference variable for shape/dtype information\n            ref_var = available_vars[0][1]\n            \n            # Create list of variables, filling missing ones with NaN\n            vars_to_concat = []\n            for i, ds in enumerate(datasets):\n                if k in ds.variables:\n                    vars_to_concat.append(ds.variables[k])\n                else:\n                    # Create a variable filled with NaN with same shape as reference\n                    fill_value = dtypes.NA if ref_var.dtype.kind in 'SUO' else dtypes.get_fill_value(ref_var.dtype)\n                    missing_var = Variable(ref_var.dims, \n                                         utils.full_like(ref_var, fill_value, dtype=ref_var.dtype))\n                    vars_to_concat.append(missing_var)\n            \n            try:\n                vars = ensure_common_dims(vars_to_concat)\n                combined = concat_vars(vars, dim, positions)\n                assert isinstance(combined, Variable)\n                result_vars[k] = combined\n            except Exception:\n                # If we can't concatenate (e.g., incompatible dimensions), skip this variable\n                continue\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    \n    # Collect all unique variable names from all datasets\n    all_var_names = set()\n    for ds in datasets:\n        all_var_names.update(ds.variables.keys())\n    \n    for k in all_var_names:\n        if k in concat_over:\n            # Get variables from datasets that have this variable\n            available_vars = []\n            missing_indices = []\n            \n            for i, ds in enumerate(datasets):\n                if k in ds.variables:\n                    available_vars.append(ds.variables[k])\n                else:\n                    missing_indices.append(i)\n            \n            if not available_vars:\n                continue  # Skip if no datasets have this variable\n                \n            # Create missing variables with NaN fill for datasets that don't have this variable\n            if missing_indices:\n                # Use the first available variable as a template\n                template_var = available_vars[0]\n                vars_with_missing = []\n                \n                for i, ds in enumerate(datasets):\n                    if i in missing_indices:\n                        # Create a NaN-filled variable with the same shape as template\n                        missing_var = Variable(\n                            template_var.dims,\n                            utils.full_like(template_var, fill_value, template_var.dtype),\n                            template_var.attrs\n                        )\n                        vars_with_missing.append(missing_var)\n                    else:\n                        vars_with_missing.append(ds.variables[k])\n                \n                vars = ensure_common_dims(vars_with_missing)\n            else:\n                vars = ensure_common_dims(available_vars)\n                \n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n",
      "\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    \n    # Collect all unique variable names from all datasets\n    all_var_names = set()\n    for ds in datasets:\n        all_var_names.update(ds.variables.keys())\n    \n    for k in all_var_names:\n        if k in concat_over:\n            # Get variables from datasets that have this variable\n            vars_with_k = []\n            datasets_with_k = []\n            for i, ds in enumerate(datasets):\n                if k in ds.variables:\n                    vars_with_k.append(ds.variables[k])\n                    datasets_with_k.append(i)\n            \n            if not vars_with_k:\n                continue\n                \n            # If variable is missing from some datasets, create fill variables\n            if len(vars_with_k) < len(datasets):\n                # Use the first available variable as template\n                template_var = vars_with_k[0]\n                \n                # Create list with all variables (including fill variables)\n                all_vars = []\n                var_idx = 0\n                for i, ds in enumerate(datasets):\n                    if i in datasets_with_k:\n                        all_vars.append(vars_with_k[var_idx])\n                        var_idx += 1\n                    else:\n                        # Create fill variable with same dims/coords but filled with fill_value\n                        fill_var = template_var._constructor(\n                            template_var.dims,\n                            utils.full_like(template_var, fill_value, dtype=template_var.dtype),\n                            template_var.attrs,\n                            template_var.encoding,\n                            fastpath=True\n                        )\n                        all_vars.append(fill_var)\n                \n                vars = ensure_common_dims(all_vars)\n            else:\n                vars = ensure_common_dims(vars_with_k)\n                \n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n"
    ],
    "Location 4 (/root/hy/projects_4/xarray/xarray/core/concat.py lines 190-211)": [
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # Collect all variables from all datasets first\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                # all nonindexes that are not the same in each dataset\n                for k in all_vars:\n                    if k not in concat_over:\n                        # Check if variable exists in all datasets first\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) < len(datasets):\n                            # Variable doesn't exist in all datasets, so it's different\n                            concat_over.add(k)\n                            equals[k] = False\n                            continue\n                            \n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets_with_var[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets_with_var[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets_with_var[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n",
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # Collect all variables from all datasets for comparison\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                # Check each variable across all datasets\n                for k in all_vars:\n                    if k not in concat_over:\n                        # Skip variables that don't exist in all datasets\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) <= 1:\n                            continue\n                            \n                        # Compare the variable across datasets that have it\n                        v_lhs = datasets_with_var[0].variables[k].load()\n                        computed = []\n                        variables_differ = False\n                        \n                        for ds_rhs in datasets_with_var[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                variables_differ = True\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets_with_var[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        \n                        if not variables_differ:\n                            equals[k] = True\n",
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # Collect all variables from all datasets\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                # Check which variables differ across datasets\n                for k in all_vars:\n                    if k not in concat_over:\n                        # Skip variables that don't exist in all datasets\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) <= 1:\n                            continue\n                            \n                        # Compare the variable across datasets that have it\n                        v_lhs = datasets_with_var[0].variables[k].load()\n                        computed = []\n                        for ds_rhs in datasets_with_var[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets_with_var[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n",
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # Collect all variables from all datasets\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                # all nonindexes that are not the same in each dataset\n                for k in all_vars:\n                    if k not in concat_over:\n                        # Check if variable exists in all datasets\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        \n                        if len(datasets_with_var) < len(datasets):\n                            # Variable missing from some datasets - automatically add to concat_over\n                            concat_over.add(k)\n                            equals[k] = False\n                        else:\n                            # Compare the variable of all datasets vs. the one\n                            # of the first dataset. Perform the minimum amount of\n                            # loads in order to avoid multiple loads from disk\n                            # while keeping the RAM footprint low.\n                            v_lhs = datasets[0].variables[k].load()\n                            # We'll need to know later on if variables are equal.\n                            computed = []\n                            for ds_rhs in datasets[1:]:\n                                v_rhs = ds_rhs.variables[k].compute()\n                                computed.append(v_rhs)\n                                if not getattr(v_lhs, compat)(v_rhs):\n                                    concat_over.add(k)\n                                    equals[k] = False\n                                    # computed variables are not to be re-computed\n                                    # again in the future\n                                    for ds, v in zip(datasets[1:], computed):\n                                        ds.variables[k].data = v.data\n                                    break\n                            else:\n                                equals[k] = True\n",
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # Collect all variables from all datasets first\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                # Check which variables differ across datasets\n                for k in all_vars:\n                    if k not in concat_over:\n                        # Skip variables that don't exist in all datasets\n                        if not all(k in getattr(ds, subset) for ds in datasets):\n                            concat_over.add(k)\n                            equals[k] = False\n                            continue\n                            \n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n",
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # Collect all variables from all datasets\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                # all nonindexes that are not the same in each dataset\n                for k in all_vars:\n                    if k not in concat_over:\n                        # Check if variable exists in all datasets\n                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                        if len(datasets_with_var) < len(datasets):\n                            # Variable missing from some datasets - include in concat_over\n                            concat_over.add(k)\n                            equals[k] = False\n                            continue\n                            \n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets_with_var[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets_with_var[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets_with_var[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n",
      "\ndef process_subset_opt(opt, subset):\n    if isinstance(opt, str):\n        if opt == \"different\":\n            if compat == \"override\":\n                raise ValueError(\n                    \"Cannot specify both %s='different' and compat='override'.\"\n                    % subset\n                )\n            # Collect all variables from all datasets\n            all_vars = set()\n            for ds in datasets:\n                all_vars.update(getattr(ds, subset))\n            \n            # Check which variables differ across datasets\n            for k in all_vars:\n                if k not in concat_over:\n                    # Check if variable exists in all datasets\n                    datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n                    if len(datasets_with_var) < len(datasets):\n                        # Variable missing from some datasets - add to concat_over\n                        concat_over.add(k)\n                        equals[k] = False\n                        continue\n                    \n                    # Compare the variable of all datasets vs. the one\n                    # of the first dataset. Perform the minimum amount of\n                    # loads in order to avoid multiple loads from disk\n                    # while keeping the RAM footprint low.\n                    v_lhs = datasets_with_var[0].variables[k].load()\n                    # We'll need to know later on if variables are equal.\n                    computed = []\n                    for ds_rhs in datasets_with_var[1:]:\n                        v_rhs = ds_rhs.variables[k].compute()\n                        computed.append(v_rhs)\n                        if not getattr(v_lhs, compat)(v_rhs):\n                            concat_over.add(k)\n                            equals[k] = False\n                            # computed variables are not to be re-computed\n                            # again in the future\n                            for ds, v in zip(datasets_with_var[1:], computed):\n                                ds.variables[k].data = v.data\n                            break\n                    else:\n                        equals[k] = True\n",
      "\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == \"different\":\n                if compat == \"override\":\n                    raise ValueError(\n                        \"Cannot specify both %s='different' and compat='override'.\"\n                        % subset\n                    )\n                # Collect all variables from all datasets first\n                all_vars = set()\n                for ds in datasets:\n                    all_vars.update(getattr(ds, subset))\n                \n                # Check which variables are different across datasets\n                for k in all_vars:\n                    if k not in concat_over:\n                        # Skip variables that don't exist in all datasets\n                        if not all(k in getattr(ds, subset) for ds in datasets):\n                            concat_over.add(k)\n                            equals[k] = False\n                            continue\n                            \n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n"
    ],
    "Location 5 (/root/hy/projects_4/xarray/xarray/core/concat.py lines 362-364)": [
      "\n            try:\n                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            except KeyError:\n                # Handle missing variables by creating NaN-filled variables for datasets that don't have this variable\n                vars = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars.append(ds.variables[k])\n                    else:\n                        # Get the variable from a dataset that has it to determine shape and dtype\n                        ref_var = None\n                        for ref_ds in datasets:\n                            if k in ref_ds.variables:\n                                ref_var = ref_ds.variables[k]\n                                break\n                        \n                        if ref_var is not None:\n                            # Create a NaN-filled variable with the same dims and dtype\n                            shape = tuple(ds.dims.get(d, ref_var.sizes[d]) for d in ref_var.dims)\n                            if ref_var.dtype.kind in 'SU':  # string dtypes\n                                fill_value = ''\n                            elif ref_var.dtype.kind == 'b':  # boolean dtype\n                                fill_value = False\n                            else:\n                                fill_value = dtypes.NA\n                            \n                            data = dtypes.full_like(ref_var, fill_value, dtype=ref_var.dtype)\n                            if hasattr(data, 'reshape'):\n                                data = data.reshape(shape)\n                            \n                            missing_var = Variable(ref_var.dims, data, ref_var.attrs)\n                            vars.append(missing_var)\n                        else:\n                            raise ValueError(\"%r is not present in all datasets.\" % k)\n                \n                vars = ensure_common_dims(vars)\n",
      "\n            try:\n                # Get variables from datasets that have this variable\n                available_vars = []\n                missing_datasets = []\n                \n                for i, ds in enumerate(datasets):\n                    if k in ds.variables:\n                        available_vars.append(ds.variables[k])\n                    else:\n                        missing_datasets.append(i)\n                \n                if not available_vars:\n                    # This shouldn't happen since k came from datasets[0].variables\n                    raise ValueError(\"%r is not present in any datasets.\" % k)\n                \n                # If some datasets are missing this variable, create dummy variables\n                if missing_datasets:\n                    # Use the first available variable as a template\n                    template_var = available_vars[0]\n                    \n                    # Create dummy variables for missing datasets\n                    all_vars = []\n                    var_idx = 0\n                    for i, ds in enumerate(datasets):\n                        if i in missing_datasets:\n                            # Create a dummy variable with NaN values\n                            dummy_shape = tuple(ds.dims.get(d, template_var.sizes.get(d, 1)) \n                                              for d in template_var.dims)\n                            dummy_data = utils.full_like(template_var, fill_value=dtypes.NA, \n                                                       shape=dummy_shape)\n                            dummy_var = Variable(template_var.dims, dummy_data, \n                                               template_var.attrs)\n                            all_vars.append(dummy_var)\n                        else:\n                            all_vars.append(available_vars[var_idx])\n                            var_idx += 1\n                    \n                    vars = ensure_common_dims(all_vars)\n                else:\n                    vars = ensure_common_dims(available_vars)\n                    \n            except KeyError:\n                raise ValueError(\"%r is not present in all datasets.\" % k)\n",
      "\n                vars_list = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars_list.append(ds.variables[k])\n                    else:\n                        # Create a dummy variable with NaN fill for missing variables\n                        if vars_list:\n                            # Use the first available variable as a template\n                            template_var = vars_list[0]\n                            dummy_shape = template_var.shape\n                            dummy_data = utils.full_like(template_var, fill_value=fill_value, dtype=template_var.dtype)\n                            dummy_var = Variable(template_var.dims, dummy_data, template_var.attrs)\n                        else:\n                            # If this is the first dataset and variable is missing, skip for now\n                            # We'll handle this when we find the variable in a later dataset\n                            break\n                        vars_list.append(dummy_var)\n                \n                if vars_list and len(vars_list) == len(datasets):\n                    vars = ensure_common_dims(vars_list)\n",
      "\n            try:\n                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            except KeyError:\n                # Handle missing variables by creating NaN-filled variables for datasets that don't have this variable\n                vars = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars.append(ds.variables[k])\n                    else:\n                        # Get the variable from a dataset that has it to determine shape and dtype\n                        ref_var = None\n                        for ref_ds in datasets:\n                            if k in ref_ds.variables:\n                                ref_var = ref_ds.variables[k]\n                                break\n                        \n                        if ref_var is not None:\n                            # Create a variable with the same dims but filled with NaN\n                            import numpy as np\n                            shape = tuple(ds.dims.get(d, ref_var.sizes[d]) for d in ref_var.dims)\n                            if ref_var.dtype.kind in 'fc':  # float or complex\n                                fill_value = np.nan\n                            else:\n                                fill_value = dtypes.get_fill_value(ref_var.dtype)\n                            \n                            data = np.full(shape, fill_value, dtype=ref_var.dtype)\n                            missing_var = Variable(ref_var.dims, data, ref_var.attrs)\n                            vars.append(missing_var)\n                        else:\n                            raise ValueError(\"%r is not present in all datasets.\" % k)\n                \n                vars = ensure_common_dims(vars)\n",
      "\n            try:\n                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            except KeyError:\n                # Handle missing variables by creating NaN-filled variables for datasets that don't have this variable\n                vars = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars.append(ds.variables[k])\n                    else:\n                        # Get the variable from a dataset that has it to determine shape and dtype\n                        ref_var = None\n                        for ref_ds in datasets:\n                            if k in ref_ds.variables:\n                                ref_var = ref_ds.variables[k]\n                                break\n                        \n                        if ref_var is not None:\n                            # Create a variable filled with NaN with the same dims as the reference\n                            import numpy as np\n                            fill_value = np.nan if ref_var.dtype.kind in 'fc' else None\n                            shape = tuple(ds.dims.get(d, ref_var.sizes[d]) for d in ref_var.dims)\n                            \n                            if fill_value is not None:\n                                data = np.full(shape, fill_value, dtype=ref_var.dtype)\n                            else:\n                                # For non-numeric dtypes, use the variable's default fill value\n                                data = np.full(shape, ref_var._FillValue if hasattr(ref_var, '_FillValue') else 0, dtype=ref_var.dtype)\n                            \n                            missing_var = Variable(ref_var.dims, data, ref_var.attrs.copy())\n                            vars.append(missing_var)\n                        else:\n                            raise ValueError(\"%r is not present in all datasets.\" % k)\n                \n                vars = ensure_common_dims(vars)\n",
      "\n            try:\n                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            except KeyError:\n                # Handle missing variables by creating NaN-filled variables for datasets that lack this variable\n                vars = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars.append(ds.variables[k])\n                    else:\n                        # Get the variable from a dataset that has it to determine shape and attributes\n                        reference_var = None\n                        for ref_ds in datasets:\n                            if k in ref_ds.variables:\n                                reference_var = ref_ds.variables[k]\n                                break\n                        \n                        if reference_var is not None:\n                            # Create a variable with the same dims but filled with NaN\n                            import numpy as np\n                            fill_value = np.nan if reference_var.dtype.kind in 'fcmM' else None\n                            shape = tuple(ds.dims.get(d, reference_var.sizes[d]) for d in reference_var.dims)\n                            \n                            if fill_value is not None:\n                                data = np.full(shape, fill_value, dtype=reference_var.dtype)\n                            else:\n                                # For non-numeric dtypes, create an array of the appropriate type\n                                data = np.empty(shape, dtype=reference_var.dtype)\n                                data.fill(reference_var._data.flat[0] if reference_var.size > 0 else None)\n                            \n                            missing_var = Variable(reference_var.dims, data, reference_var.attrs)\n                            vars.append(missing_var)\n                        else:\n                            raise ValueError(\"%r is not present in all datasets.\" % k)\n                \n                vars = ensure_common_dims(vars)\n",
      "\n            try:\n                # Get variables that exist in current datasets for this variable name\n                available_vars = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        available_vars.append(ds.variables[k])\n                    else:\n                        # Create a NaN-filled variable with appropriate shape for missing variables\n                        # Use the first available variable as a template for dimensions and dtype\n                        if available_vars:\n                            template_var = available_vars[0]\n                            # Create variable filled with NaN, matching template dimensions\n                            fill_value = dtypes.NA if template_var.dtype.kind in 'cfmM' else dtypes.get_fill_value(template_var.dtype)\n                            missing_var = Variable(template_var.dims, \n                                                 utils.full_like(template_var.data, fill_value, dtype=template_var.dtype),\n                                                 attrs=template_var.attrs.copy())\n                            available_vars.append(missing_var)\n                        else:\n                            # Skip this variable if no datasets have it\n                            continue\n                \n                if available_vars:\n                    vars = ensure_common_dims(available_vars)\n                else:\n                    continue\n            except KeyError:\n                # This should not happen with the new logic above, but keep as fallback\n                continue\n",
      "\n                # Handle missing variables by creating NaN-filled placeholders\n                vars_list = []\n                for ds in datasets:\n                    if k in ds.variables:\n                        vars_list.append(ds.variables[k])\n                    else:\n                        # Create a placeholder variable filled with NaN\n                        # Get the shape from the first available variable\n                        first_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n                        placeholder_data = utils.full_like(first_var._data, fill_value=fill_value, dtype=first_var.dtype)\n                        placeholder_var = Variable(first_var.dims, placeholder_data, first_var.attrs)\n                        vars_list.append(placeholder_var)\n                \n                vars = ensure_common_dims(vars_list)\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..aa060c31 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,26 +187,37 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+                # Collect all variables from all datasets for comparison\n+                all_subset_vars = set()\n+                for ds in datasets:\n+                    all_subset_vars.update(getattr(ds, subset))\n+                \n+                for k in all_subset_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n-                        # We'll need to know later on if variables are equal.\n-                        computed = []\n-                        for ds_rhs in datasets[1:]:\n-                            v_rhs = ds_rhs.variables[k].compute()\n-                            computed.append(v_rhs)\n-                            if not getattr(v_lhs, compat)(v_rhs):\n-                                concat_over.add(k)\n-                                equals[k] = False\n-                                # computed variables are not to be re-computed\n-                                # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n-                                    ds.variables[k].data = v.data\n-                                break\n+                        # Find datasets that have this variable\n+                        datasets_with_var = [ds for ds in datasets if k in ds.variables]\n+                        \n+                        if len(datasets_with_var) < len(datasets):\n+                            # Variable is missing from some datasets, so it should be concatenated\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                        elif len(datasets_with_var) > 1:\n+                            # Compare the variable across datasets that have it\n+                            v_lhs = datasets_with_var[0].variables[k].load()\n+                            computed = []\n+                            for ds_rhs in datasets_with_var[1:]:\n+                                v_rhs = ds_rhs.variables[k].compute()\n+                                computed.append(v_rhs)\n+                                if not getattr(v_lhs, compat)(v_rhs):\n+                                    concat_over.add(k)\n+                                    equals[k] = False\n+                                    # computed variables are not to be re-computed\n+                                    for ds, v in zip(datasets_with_var[1:], computed):\n+                                        ds.variables[k].data = v.data\n+                                    break\n+                            else:\n+                                equals[k] = True\n                         else:\n                             equals[k] = True\n \n@@ -219,17 +230,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # Collect all variables from all datasets for validation\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +329,12 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+            # Handle missing variables gracefully - collect variables present in each dataset\n+            for var in variables_to_merge:\n+                var_datasets = [ds for ds in datasets if var in ds.variables]\n+                if var_datasets:  # Only process if variable exists in at least one dataset\n+                    to_merge[var] = [ds.variables[var] for ds in var_datasets]\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -356,12 +373,47 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n+\n+    # Collect all unique variables from all datasets to process\n+    all_variables = set()\n+    for ds in datasets:\n+        all_variables.update(ds.variables)\n+    \n+    # Process all variables that need concatenation\n+    for k in all_variables:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+            # Collect variables from datasets that have this variable\n+            vars_to_concat = []\n+            for i, ds in enumerate(datasets):\n+                if k in ds.variables:\n+                    vars_to_concat.append(ds.variables[k])\n+                else:\n+                    # Create a dummy variable filled with NaN for missing variables\n+                    # Use the first available variable as template for shape/dims\n+                    template_var = None\n+                    for other_ds in datasets:\n+                        if k in other_ds.variables:\n+                            template_var = other_ds.variables[k]\n+                            break\n+                    \n+                    if template_var is not None:\n+                        # Create NaN-filled variable with same dims but appropriate fill value\n+                        import numpy as np\n+                        fill_val = fill_value if fill_value is not None else np.nan\n+                        dummy_data = np.full_like(template_var.data, fill_val)\n+                        dummy_var = Variable(template_var.dims, dummy_data, template_var.attrs)\n+                        vars_to_concat.append(dummy_var)\n+            \n+            if vars_to_concat:\n+                vars = ensure_common_dims(vars_to_concat)\n+                combined = concat_vars(vars, dim, positions)\n+                assert isinstance(combined, Variable)\n+                result_vars[k] = combined\n+                \n+                if available_vars:\n+                    vars = ensure_common_dims(available_vars)\n+                else:\n+                    raise ValueError(\"%r is not present in any datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n",
    "variant_0": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..ca6d49a9 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,16 +187,35 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+    def process_subset_opt(opt, subset):\n+        if isinstance(opt, str):\n+            if opt == \"different\":\n+                if compat == \"override\":\n+                    raise ValueError(\n+                        \"Cannot specify both %s='different' and compat='override'.\"\n+                        % subset\n+                    )\n+                # Collect all variables from all datasets\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                # Check which variables differ across datasets\n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n-                        # We'll need to know later on if variables are equal.\n+                        # Skip variables that don't exist in all datasets\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) < len(datasets):\n+                            # Variable missing from some datasets, include in concat\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            continue\n+                            \n+                        # Compare the variable across all datasets\n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         computed = []\n-                        for ds_rhs in datasets[1:]:\n+                        for ds_rhs in datasets_with_var[1:]:\n                             v_rhs = ds_rhs.variables[k].compute()\n                             computed.append(v_rhs)\n                             if not getattr(v_lhs, compat)(v_rhs):\n@@ -204,7 +223,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                                 equals[k] = False\n                                 # computed variables are not to be re-computed\n                                 # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n+                                for ds, v in zip(datasets_with_var[1:], computed):\n                                     ds.variables[k].data = v.data\n                                 break\n                         else:\n@@ -219,17 +238,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # For outer join behavior, check if variables exist in ANY dataset, not just the first\n+            all_vars_in_subset = set()\n+            for ds in datasets:\n+                all_vars_in_subset.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"some variables in coords are not coordinates in \"\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"in any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +337,35 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            # For outer join behavior, handle missing variables by creating NaN-filled placeholders\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a NaN-filled variable with appropriate shape and dtype\n+                    original_var = None\n+                    for other_ds in datasets:\n+                        if var in other_ds.variables:\n+                            original_var = other_ds.variables[var]\n+                            break\n+                    \n+                    if original_var is not None:\n+                        # Create variable with same dims/coords but filled with NaN\n+                        shape = tuple(ds.dims.get(d, original_var.sizes.get(d, 1)) for d in original_var.dims)\n+                        data = utils.full_like(original_var.data, fill_value=dtypes.get_fill_value(original_var.dtype), \n+                                             shape=shape, dtype=original_var.dtype)\n+                        attrs = original_var.attrs.copy() if original_var.attrs else {}\n+                        nan_var = Variable(original_var.dims, data, attrs)\n+                        to_merge[var].append(nan_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -356,12 +404,73 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    \n+    # Collect all unique variable names from all datasets\n+    all_var_names = set()\n+    for ds in datasets:\n+        all_var_names.update(ds.variables.keys())\n+    \n+    for k in all_var_names:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+            # Get variables from datasets that have this variable\n+            vars_with_k = []\n+            datasets_with_k = []\n+            for ds in datasets:\n+                if k in ds.variables:\n+                    vars_with_k.append(ds.variables[k])\n+                    datasets_with_k.append(ds)\n+            \n+            if len(vars_with_k) == len(datasets):\n+                # All datasets have this variable - normal case\n+                vars = ensure_common_dims(vars_with_k)\n+            else:\n+                # Some datasets are missing this variable - create fill values\n+                if not vars_with_k:\n+                    continue  # Skip if no datasets have this variable\n+                \n+                # Use first available variable as template\n+                template_var = vars_with_k[0]\n+                all_vars = []\n+                \n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        all_vars.append(ds.variables[k])\n+                    else:\n+                        # Create a variable filled with NaN/fill_value for missing datasets\n+                        fill_value = fill_value if fill_value is not None else (\n+                            dtypes.NA if template_var.dtype.kind in 'UO' else \n+                            dtypes.get_fill_value(template_var.dtype)\n+                        )\n+                        \n+                        # Create variable with same dims as template but filled with fill_value\n+                        fill_data = utils.full_like(template_var, fill_value, template_var.dtype)\n+                        fill_var = Variable(template_var.dims, fill_data, template_var.attrs)\n+                        all_vars.append(fill_var)\n+                \n+                vars = ensure_common_dims(all_vars)\n+            \n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars.append(ds.variables[k])\n+                    else:\n+                        # Create a NaN-filled variable with the same dimensions as other datasets\n+                        if vars:\n+                            # Use the first available variable as a template\n+                            template_var = vars[0]\n+                            fill_value = dtypes.NA if template_var.dtype.kind in 'fc' else dtypes.get_fill_value(template_var.dtype)\n+                            missing_var = Variable(template_var.dims, \n+                                                 utils.full_like(template_var.data, fill_value, dtype=template_var.dtype))\n+                            vars.append(missing_var)\n+                        else:\n+                            # This shouldn't happen if we're iterating over variables that exist somewhere\n+                            raise ValueError(\"%r is not present in all datasets.\" % k)\n+                vars = ensure_common_dims(vars)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n",
    "variant_1": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..68d9c419 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,16 +187,39 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+    def process_subset_opt(opt, subset):\n+        if isinstance(opt, str):\n+            if opt == \"different\":\n+                if compat == \"override\":\n+                    raise ValueError(\n+                        \"Cannot specify both %s='different' and compat='override'.\"\n+                        % subset\n+                    )\n+                # Collect all variables from all datasets first\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                # all nonindexes that are not the same in each dataset\n+                for k in all_vars:\n                     if k not in concat_over:\n+                        # Check if variable exists in all datasets first\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) < len(datasets):\n+                            # Variable doesn't exist in all datasets, so it's different\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            continue\n+                            \n                         # Compare the variable of all datasets vs. the one\n                         # of the first dataset. Perform the minimum amount of\n                         # loads in order to avoid multiple loads from disk\n                         # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n-                        for ds_rhs in datasets[1:]:\n+                        for ds_rhs in datasets_with_var[1:]:\n                             v_rhs = ds_rhs.variables[k].compute()\n                             computed.append(v_rhs)\n                             if not getattr(v_lhs, compat)(v_rhs):\n@@ -204,7 +227,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                                 equals[k] = False\n                                 # computed variables are not to be re-computed\n                                 # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n+                                for ds, v in zip(datasets_with_var[1:], computed):\n                                     ds.variables[k].data = v.data\n                                 break\n                         else:\n@@ -219,17 +242,24 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+        else:\n+            # For user-specified variable lists, check if variables exist in ANY dataset\n+            all_vars_in_subset = set()\n+            for ds in datasets:\n+                all_vars_in_subset.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +342,41 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a dummy variable filled with NaN for missing variables\n+                    # Get the variable from a dataset that has it to determine shape and dtype\n+                    ref_var = None\n+                    for ref_ds in datasets:\n+                        if var in ref_ds.variables:\n+                            ref_var = ref_ds.variables[var]\n+                            break\n+                    \n+                    if ref_var is not None:\n+                        # Create NaN-filled variable with same dims and dtype as reference\n+                        import numpy as np\n+                        shape = tuple(ds.dims.get(dim, ref_var.sizes.get(dim, 1)) \n+                                    for dim in ref_var.dims)\n+                        fill_value = np.nan if ref_var.dtype.kind in 'fc' else None\n+                        if fill_value is not None:\n+                            data = np.full(shape, fill_value, dtype=ref_var.dtype)\n+                        else:\n+                            # For non-numeric dtypes, use object array with None\n+                            data = np.full(shape, None, dtype=object)\n+                        \n+                        dummy_var = Variable(ref_var.dims, data, attrs=ref_var.attrs)\n+                        to_merge[var].append(dummy_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -356,15 +415,91 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    \n+    # Collect all unique variable names from all datasets\n+    all_var_names = set()\n+    for ds in datasets:\n+        all_var_names.update(ds.variables.keys())\n+    \n+    for k in all_var_names:\n         if k in concat_over:\n+            # Get variables from datasets that have this variable\n+            vars_with_k = []\n+            datasets_with_k = []\n+            for i, ds in enumerate(datasets):\n+                if k in ds.variables:\n+                    vars_with_k.append(ds.variables[k])\n+                    datasets_with_k.append(i)\n+            \n+            if not vars_with_k:\n+                continue\n+                \n+            # If not all datasets have this variable, create missing ones with fill_value\n+            if len(vars_with_k) < len(datasets):\n+                # Use the first available variable as template\n+                template_var = vars_with_k[0]\n+                vars = []\n+                \n+                for i, ds in enumerate(datasets):\n+                    if i in datasets_with_k:\n+                        vars.append(ds.variables[k])\n+                    else:\n+                        # Create missing variable filled with fill_value\n+                        missing_var = template_var._constructor(\n+                            template_var.dims,\n+                            fill_value,\n+                            template_var.attrs.copy(),\n+                            template_var.encoding.copy(),\n+                            fastpath=True\n+                        )\n+                        vars.append(missing_var)\n+            else:\n+                vars = vars_with_k\n+                \n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+                vars = ensure_common_dims(vars)\n             except KeyError:\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars.append(ds.variables[k])\n+                    else:\n+                        # Get the variable from a dataset that has it to determine shape and dtype\n+                        ref_var = None\n+                        for ref_ds in datasets:\n+                            if k in ref_ds.variables:\n+                                ref_var = ref_ds.variables[k]\n+                                break\n+                        \n+                        if ref_var is not None:\n+                            # Create a NaN-filled variable with the same dims and dtype\n+                            shape = tuple(ds.dims.get(d, ref_var.sizes[d]) for d in ref_var.dims)\n+                            if ref_var.dtype.kind in 'SU':  # string dtypes\n+                                fill_value = ''\n+                            elif ref_var.dtype.kind == 'b':  # boolean dtype\n+                                fill_value = False\n+                            else:\n+                                fill_value = dtypes.NA\n+                            \n+                            data = dtypes.full_like(ref_var, fill_value, dtype=ref_var.dtype)\n+                            if hasattr(data, 'reshape'):\n+                                data = data.reshape(shape)\n+                            \n+                            missing_var = Variable(ref_var.dims, data, ref_var.attrs)\n+                            vars.append(missing_var)\n+                        else:\n+                            raise ValueError(\"%r is not present in all datasets.\" % k)\n+                \n+                vars = ensure_common_dims(vars)\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n \n     result = Dataset(result_vars, attrs=result_attrs)\n     absent_coord_names = coord_names - set(result.variables)\n",
    "variant_2": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..558fa4e6 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,27 +187,47 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+    def process_subset_opt(opt, subset):\n+        if isinstance(opt, str):\n+            if opt == \"different\":\n+                if compat == \"override\":\n+                    raise ValueError(\n+                        \"Cannot specify both %s='different' and compat='override'.\"\n+                        % subset\n+                    )\n+                # Collect all variables from all datasets for comparison\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                # Check each variable across all datasets\n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n-                        # We'll need to know later on if variables are equal.\n+                        # Skip variables that don't exist in all datasets\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) <= 1:\n+                            continue\n+                            \n+                        # Compare the variable across datasets that have it\n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         computed = []\n-                        for ds_rhs in datasets[1:]:\n+                        variables_differ = False\n+                        \n+                        for ds_rhs in datasets_with_var[1:]:\n                             v_rhs = ds_rhs.variables[k].compute()\n                             computed.append(v_rhs)\n                             if not getattr(v_lhs, compat)(v_rhs):\n+                                variables_differ = True\n                                 concat_over.add(k)\n                                 equals[k] = False\n                                 # computed variables are not to be re-computed\n                                 # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n+                                for ds, v in zip(datasets_with_var[1:], computed):\n                                     ds.variables[k].data = v.data\n                                 break\n-                        else:\n+                        \n+                        if not variables_differ:\n                             equals[k] = True\n \n             elif opt == \"all\":\n@@ -219,17 +239,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # Check if variables exist in ANY dataset, not just the first one\n+            all_vars_in_subset = set()\n+            for ds in datasets:\n+                all_vars_in_subset.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"some variables in coords are not coordinates in \"\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"in any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +338,33 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    # Get the shape from the first available instance of this variable\n+                    ref_var = None\n+                    for ref_ds in datasets:\n+                        if var in ref_ds.variables:\n+                            ref_var = ref_ds.variables[var]\n+                            break\n+                    \n+                    if ref_var is not None:\n+                        # Create NaN-filled variable with same dtype and dims as reference\n+                        fill_value = dtypes.NA if ref_var.dtype.kind in 'SU' else dtypes.get_fill_value(ref_var.dtype)\n+                        placeholder_data = utils.full_like(ref_var.data, fill_value, dtype=ref_var.dtype)\n+                        placeholder_var = Variable(ref_var.dims, placeholder_data, ref_var.attrs)\n+                        to_merge[var].append(placeholder_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -356,10 +403,91 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    \n+    # Collect all unique variable names from all datasets\n+    all_var_names = set()\n+    for ds in datasets:\n+        all_var_names.update(ds.variables.keys())\n+    \n+    for k in all_var_names:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+            # Get variables from datasets that have this variable\n+            available_vars = []\n+            missing_datasets = []\n+            \n+            for i, ds in enumerate(datasets):\n+                if k in ds.variables:\n+                    available_vars.append(ds.variables[k])\n+                else:\n+                    missing_datasets.append(i)\n+            \n+            if not available_vars:\n+                continue  # Skip if no datasets have this variable\n+                \n+            # Create missing variables filled with NaN for datasets that don't have this variable\n+            if missing_datasets:\n+                # Use the first available variable as a template\n+                template_var = available_vars[0]\n+                vars_with_missing = []\n+                \n+                for i, ds in enumerate(datasets):\n+                    if i in missing_datasets:\n+                        # Create a NaN-filled variable with the same structure\n+                        missing_var = Variable(\n+                            template_var.dims,\n+                            utils.full_like(template_var.data, fill_value=dtypes.NA),\n+                            attrs=template_var.attrs\n+                        )\n+                        vars_with_missing.append(missing_var)\n+                    else:\n+                        vars_with_missing.append(ds.variables[k])\n+                \n+                vars = ensure_common_dims(vars_with_missing)\n+            else:\n+                vars = ensure_common_dims(available_vars)\n+                \n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+                for i, ds in enumerate(datasets):\n+                    if k in ds.variables:\n+                        available_vars.append(ds.variables[k])\n+                    else:\n+                        missing_datasets.append(i)\n+                \n+                if not available_vars:\n+                    # This shouldn't happen since k came from datasets[0].variables\n+                    raise ValueError(\"%r is not present in any datasets.\" % k)\n+                \n+                # If some datasets are missing this variable, create dummy variables\n+                if missing_datasets:\n+                    # Use the first available variable as a template\n+                    template_var = available_vars[0]\n+                    \n+                    # Create dummy variables for missing datasets\n+                    all_vars = []\n+                    var_idx = 0\n+                    for i, ds in enumerate(datasets):\n+                        if i in missing_datasets:\n+                            # Create a dummy variable with NaN values\n+                            dummy_shape = tuple(ds.dims.get(d, template_var.sizes.get(d, 1)) \n+                                              for d in template_var.dims)\n+                            dummy_data = utils.full_like(template_var, fill_value=dtypes.NA, \n+                                                       shape=dummy_shape)\n+                            dummy_var = Variable(template_var.dims, dummy_data, \n+                                               template_var.attrs)\n+                            all_vars.append(dummy_var)\n+                        else:\n+                            all_vars.append(available_vars[var_idx])\n+                            var_idx += 1\n+                    \n+                    vars = ensure_common_dims(all_vars)\n+                else:\n+                    vars = ensure_common_dims(available_vars)\n+                    \n             except KeyError:\n                 raise ValueError(\"%r is not present in all datasets.\" % k)\n             combined = concat_vars(vars, dim, positions)\n",
    "variant_3": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..b936032c 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,16 +187,32 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+    def process_subset_opt(opt, subset):\n+        if isinstance(opt, str):\n+            if opt == \"different\":\n+                if compat == \"override\":\n+                    raise ValueError(\n+                        \"Cannot specify both %s='different' and compat='override'.\"\n+                        % subset\n+                    )\n+                # Collect all variables from all datasets\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                # Check which variables differ across datasets\n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n-                        # We'll need to know later on if variables are equal.\n+                        # Skip variables that don't exist in all datasets\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) <= 1:\n+                            continue\n+                            \n+                        # Compare the variable across datasets that have it\n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         computed = []\n-                        for ds_rhs in datasets[1:]:\n+                        for ds_rhs in datasets_with_var[1:]:\n                             v_rhs = ds_rhs.variables[k].compute()\n                             computed.append(v_rhs)\n                             if not getattr(v_lhs, compat)(v_rhs):\n@@ -204,7 +220,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                                 equals[k] = False\n                                 # computed variables are not to be re-computed\n                                 # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n+                                for ds, v in zip(datasets_with_var[1:], computed):\n                                     ds.variables[k].data = v.data\n                                 break\n                         else:\n@@ -219,17 +235,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # For outer join behavior, check if variables exist in ANY dataset\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +334,39 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            # For outer join behavior, create missing variables with NaN fill\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a NaN-filled variable with the same shape as other datasets\n+                    if to_merge[var]:  # If we have other instances of this variable\n+                        template_var = to_merge[var][0]\n+                        # Create NaN-filled variable with same dtype and dims\n+                        fill_value = dtypes.NA if template_var.dtype.kind in 'fc' else dtypes.get_fill_value(template_var.dtype)\n+                        nan_var = Variable(template_var.dims, \n+                                         fill_value * template_var.shape, \n+                                         attrs=template_var.attrs)\n+                        to_merge[var].append(nan_var)\n+                    else:\n+                        # This is the first occurrence, we'll handle missing vars later\n+                        continue\n+\n+        for var in variables_to_merge:\n+            result_vars[var] = unique_variable(\n+                var, to_merge[var], compat=compat, equals=equals.get(var, None)\n+            )\n+    else:\n+        result_vars = {}\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -356,12 +405,74 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    \n+    # Collect all unique variable names from all datasets\n+    all_var_names = set()\n+    for ds in datasets:\n+        all_var_names.update(ds.variables.keys())\n+    \n+    for k in all_var_names:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+            # Get variables from datasets that have this variable\n+            available_vars = []\n+            missing_indices = []\n+            \n+            for i, ds in enumerate(datasets):\n+                if k in ds.variables:\n+                    available_vars.append(ds.variables[k])\n+                else:\n+                    missing_indices.append(i)\n+            \n+            if not available_vars:\n+                continue  # Skip if no datasets have this variable\n+            \n+            # Create missing variables filled with NaN for datasets that don't have this variable\n+            if missing_indices:\n+                # Use the first available variable as a template\n+                template_var = available_vars[0]\n+                \n+                # Create NaN-filled variables for missing datasets\n+                full_vars = []\n+                available_idx = 0\n+                \n+                for i, ds in enumerate(datasets):\n+                    if i in missing_indices:\n+                        # Create a NaN-filled variable with the same structure as template\n+                        shape = tuple(dims_sizes.get(d, concat_dim_lengths[i]) if d == dim \n+                                    else dims_sizes[d] for d in template_var.dims)\n+                        fill_value = dtypes.NA if template_var.dtype.kind in 'SUO' else np.nan\n+                        data = np.full(shape, fill_value, dtype=template_var.dtype)\n+                        missing_var = Variable(template_var.dims, data, template_var.attrs)\n+                        full_vars.append(missing_var)\n+                    else:\n+                        full_vars.append(available_vars[available_idx])\n+                        available_idx += 1\n+                \n+                vars = ensure_common_dims(full_vars)\n+            else:\n+                vars = ensure_common_dims(available_vars)\n+            \n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+                        # Create a dummy variable with NaN fill for missing variables\n+                        if vars_list:\n+                            # Use the first available variable as a template\n+                            template_var = vars_list[0]\n+                            dummy_shape = template_var.shape\n+                            dummy_data = utils.full_like(template_var, fill_value=fill_value, dtype=template_var.dtype)\n+                            dummy_var = Variable(template_var.dims, dummy_data, template_var.attrs)\n+                        else:\n+                            # If this is the first dataset and variable is missing, skip for now\n+                            # We'll handle this when we find the variable in a later dataset\n+                            break\n+                        vars_list.append(dummy_var)\n+                \n+                if vars_list and len(vars_list) == len(datasets):\n+                    vars = ensure_common_dims(vars_list)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n",
    "variant_4": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..365d6b9d 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,28 +187,51 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+    def process_subset_opt(opt, subset):\n+        if isinstance(opt, str):\n+            if opt == \"different\":\n+                if compat == \"override\":\n+                    raise ValueError(\n+                        \"Cannot specify both %s='different' and compat='override'.\"\n+                        % subset\n+                    )\n+                # Collect all variables from all datasets\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                # all nonindexes that are not the same in each dataset\n+                for k in all_vars:\n                     if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n-                        # We'll need to know later on if variables are equal.\n-                        computed = []\n-                        for ds_rhs in datasets[1:]:\n-                            v_rhs = ds_rhs.variables[k].compute()\n-                            computed.append(v_rhs)\n-                            if not getattr(v_lhs, compat)(v_rhs):\n-                                concat_over.add(k)\n-                                equals[k] = False\n-                                # computed variables are not to be re-computed\n-                                # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n-                                    ds.variables[k].data = v.data\n-                                break\n+                        # Check if variable exists in all datasets\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        \n+                        if len(datasets_with_var) < len(datasets):\n+                            # Variable missing from some datasets - automatically add to concat_over\n+                            concat_over.add(k)\n+                            equals[k] = False\n                         else:\n-                            equals[k] = True\n+                            # Compare the variable of all datasets vs. the one\n+                            # of the first dataset. Perform the minimum amount of\n+                            # loads in order to avoid multiple loads from disk\n+                            # while keeping the RAM footprint low.\n+                            v_lhs = datasets[0].variables[k].load()\n+                            # We'll need to know later on if variables are equal.\n+                            computed = []\n+                            for ds_rhs in datasets[1:]:\n+                                v_rhs = ds_rhs.variables[k].compute()\n+                                computed.append(v_rhs)\n+                                if not getattr(v_lhs, compat)(v_rhs):\n+                                    concat_over.add(k)\n+                                    equals[k] = False\n+                                    # computed variables are not to be re-computed\n+                                    # again in the future\n+                                    for ds, v in zip(datasets[1:], computed):\n+                                        ds.variables[k].data = v.data\n+                                    break\n+                            else:\n+                                equals[k] = True\n \n             elif opt == \"all\":\n                 concat_over.update(\n@@ -219,17 +242,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # Check if variables exist in ANY dataset, not just the first\n+            all_vars_in_subset = set()\n+            for ds in datasets:\n+                all_vars_in_subset.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +341,42 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a placeholder variable filled with NaN for missing variables\n+                    # Get the variable from a dataset that has it to determine shape and dtype\n+                    ref_var = None\n+                    for ref_ds in datasets:\n+                        if var in ref_ds.variables:\n+                            ref_var = ref_ds.variables[var]\n+                            break\n+                    \n+                    if ref_var is not None:\n+                        # Create NaN-filled variable with same dims and dtype as reference\n+                        import numpy as np\n+                        shape = tuple(ds.dims.get(dim, 1) if dim in ds.dims else ref_var.sizes[dim] \n+                                    for dim in ref_var.dims)\n+                        fill_value = np.nan if ref_var.dtype.kind in 'fc' else None\n+                        if fill_value is not None:\n+                            data = np.full(shape, fill_value, dtype=ref_var.dtype)\n+                        else:\n+                            # For non-numeric dtypes, create array of appropriate type\n+                            data = np.empty(shape, dtype=ref_var.dtype)\n+                            data.fill(None)\n+                        \n+                        placeholder_var = Variable(ref_var.dims, data, ref_var.attrs)\n+                        to_merge[var].append(placeholder_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -356,12 +415,76 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    \n+    # Collect all unique variable names from all datasets\n+    all_var_names = set()\n+    for ds in datasets:\n+        all_var_names.update(ds.variables.keys())\n+    \n+    for k in all_var_names:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+            # Get variables from datasets that have this variable\n+            vars_list = []\n+            missing_datasets = []\n+            \n+            for i, ds in enumerate(datasets):\n+                if k in ds.variables:\n+                    vars_list.append(ds.variables[k])\n+                else:\n+                    missing_datasets.append(i)\n+            \n+            if vars_list:\n+                # If some datasets are missing this variable, create dummy variables\n+                if missing_datasets:\n+                    # Use the first available variable as template\n+                    template_var = vars_list[0]\n+                    dummy_shape = list(template_var.shape)\n+                    \n+                    # Insert dummy variables for missing datasets\n+                    for missing_idx in missing_datasets:\n+                        # Create a variable filled with NaN with the same dims as template\n+                        dummy_var = Variable(\n+                            template_var.dims,\n+                            dtypes.get_fill_value(template_var.dtype) * \n+                            utils.ones_like(template_var.data, dtype=template_var.dtype),\n+                            template_var.attrs\n+                        )\n+                        vars_list.insert(missing_idx, dummy_var)\n+                \n+                vars = ensure_common_dims(vars_list)\n+                combined = concat_vars(vars, dim, positions)\n+                assert isinstance(combined, Variable)\n+                result_vars[k] = combined\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars.append(ds.variables[k])\n+                    else:\n+                        # Get the variable from a dataset that has it to determine shape and dtype\n+                        ref_var = None\n+                        for ref_ds in datasets:\n+                            if k in ref_ds.variables:\n+                                ref_var = ref_ds.variables[k]\n+                                break\n+                        \n+                        if ref_var is not None:\n+                            # Create a variable with the same dims but filled with NaN\n+                            import numpy as np\n+                            shape = tuple(ds.dims.get(d, ref_var.sizes[d]) for d in ref_var.dims)\n+                            if ref_var.dtype.kind in 'fc':  # float or complex\n+                                fill_value = np.nan\n+                            else:\n+                                fill_value = dtypes.get_fill_value(ref_var.dtype)\n+                            \n+                            data = np.full(shape, fill_value, dtype=ref_var.dtype)\n+                            missing_var = Variable(ref_var.dims, data, ref_var.attrs)\n+                            vars.append(missing_var)\n+                        else:\n+                            raise ValueError(\"%r is not present in all datasets.\" % k)\n+                \n+                vars = ensure_common_dims(vars)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n",
    "variant_5": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..913111a6 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,8 +187,29 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+    def process_subset_opt(opt, subset):\n+        if isinstance(opt, str):\n+            if opt == \"different\":\n+                if compat == \"override\":\n+                    raise ValueError(\n+                        \"Cannot specify both %s='different' and compat='override'.\"\n+                        % subset\n+                    )\n+                # Collect all variables from all datasets first\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                # Check which variables differ across datasets\n+                for k in all_vars:\n                     if k not in concat_over:\n+                        # Skip variables that don't exist in all datasets\n+                        if not all(k in getattr(ds, subset) for ds in datasets):\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            continue\n+                            \n                         # Compare the variable of all datasets vs. the one\n                         # of the first dataset. Perform the minimum amount of\n                         # loads in order to avoid multiple loads from disk\n@@ -219,17 +240,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # For outer join behavior, check if variables exist in ANY dataset, not just the first\n+            all_vars_in_subset = set()\n+            for ds in datasets:\n+                all_vars_in_subset.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"some variables in coords are not coordinates in \"\n+                        \"any of the datasets: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"in any of the datasets: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +339,35 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            # For outer join behavior, handle missing variables by creating NaN-filled ones\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a NaN-filled variable with appropriate shape and dtype\n+                    ref_var = None\n+                    for other_ds in datasets:\n+                        if var in other_ds.variables:\n+                            ref_var = other_ds.variables[var]\n+                            break\n+                    \n+                    if ref_var is not None:\n+                        # Create variable with same dims and dtype but filled with NaN\n+                        shape = tuple(ds.dims.get(d, ref_var.sizes[d]) if d in ref_var.dims else ref_var.sizes[d] \n+                                    for d in ref_var.dims)\n+                        fill_value = dtypes.NA if ref_var.dtype.kind in 'SUO' else dtypes.get_fill_value(ref_var.dtype)\n+                        data = utils.full_like(ref_var.data, fill_value, shape=shape, dtype=ref_var.dtype)\n+                        nan_var = Variable(ref_var.dims, data, ref_var.attrs.copy())\n+                        to_merge[var].append(nan_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -356,12 +406,90 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    \n+    # Collect all unique variable names from all datasets\n+    all_var_names = set()\n+    for ds in datasets:\n+        all_var_names.update(ds.variables.keys())\n+    \n+    for k in all_var_names:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+            # Get variables from datasets that have this variable\n+            vars_list = []\n+            for ds in datasets:\n+                if k in ds.variables:\n+                    vars_list.append(ds.variables[k])\n+                else:\n+                    # Create a NaN-filled variable with appropriate shape\n+                    if vars_list:\n+                        # Use the first available variable as template\n+                        template_var = vars_list[0]\n+                        shape = template_var.shape\n+                        dims = template_var.dims\n+                        dtype = template_var.dtype\n+                    else:\n+                        # Find any dataset that has this variable to use as template\n+                        template_var = None\n+                        for other_ds in datasets:\n+                            if k in other_ds.variables:\n+                                template_var = other_ds.variables[k]\n+                                break\n+                        if template_var is None:\n+                            continue  # Skip if variable not found anywhere\n+                        shape = template_var.shape\n+                        dims = template_var.dims\n+                        dtype = template_var.dtype\n+                    \n+                    # Create NaN-filled variable\n+                    import numpy as np\n+                    if np.issubdtype(dtype, np.floating):\n+                        fill_value = np.nan\n+                    elif np.issubdtype(dtype, np.integer):\n+                        fill_value = 0\n+                    else:\n+                        fill_value = None\n+                    \n+                    nan_data = np.full(shape, fill_value, dtype=dtype)\n+                    nan_var = Variable(dims, nan_data, template_var.attrs)\n+                    vars_list.append(nan_var)\n+            \n+            if vars_list:\n+                vars = ensure_common_dims(vars_list)\n+                combined = concat_vars(vars, dim, positions)\n+                assert isinstance(combined, Variable)\n+                result_vars[k] = combined\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars.append(ds.variables[k])\n+                    else:\n+                        # Get the variable from a dataset that has it to determine shape and dtype\n+                        ref_var = None\n+                        for ref_ds in datasets:\n+                            if k in ref_ds.variables:\n+                                ref_var = ref_ds.variables[k]\n+                                break\n+                        \n+                        if ref_var is not None:\n+                            # Create a variable filled with NaN with the same dims as the reference\n+                            import numpy as np\n+                            fill_value = np.nan if ref_var.dtype.kind in 'fc' else None\n+                            shape = tuple(ds.dims.get(d, ref_var.sizes[d]) for d in ref_var.dims)\n+                            \n+                            if fill_value is not None:\n+                                data = np.full(shape, fill_value, dtype=ref_var.dtype)\n+                            else:\n+                                # For non-numeric dtypes, use the variable's default fill value\n+                                data = np.full(shape, ref_var._FillValue if hasattr(ref_var, '_FillValue') else 0, dtype=ref_var.dtype)\n+                            \n+                            missing_var = Variable(ref_var.dims, data, ref_var.attrs.copy())\n+                            vars.append(missing_var)\n+                        else:\n+                            raise ValueError(\"%r is not present in all datasets.\" % k)\n+                \n+                vars = ensure_common_dims(vars)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n",
    "variant_6": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..8fcaafe8 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,16 +187,39 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+    def process_subset_opt(opt, subset):\n+        if isinstance(opt, str):\n+            if opt == \"different\":\n+                if compat == \"override\":\n+                    raise ValueError(\n+                        \"Cannot specify both %s='different' and compat='override'.\"\n+                        % subset\n+                    )\n+                # Collect all variables from all datasets\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                # all nonindexes that are not the same in each dataset\n+                for k in all_vars:\n                     if k not in concat_over:\n+                        # Check if variable exists in all datasets\n+                        datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                        if len(datasets_with_var) < len(datasets):\n+                            # Variable missing from some datasets - include in concat_over\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            continue\n+                            \n                         # Compare the variable of all datasets vs. the one\n                         # of the first dataset. Perform the minimum amount of\n                         # loads in order to avoid multiple loads from disk\n                         # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n+                        v_lhs = datasets_with_var[0].variables[k].load()\n                         # We'll need to know later on if variables are equal.\n                         computed = []\n-                        for ds_rhs in datasets[1:]:\n+                        for ds_rhs in datasets_with_var[1:]:\n                             v_rhs = ds_rhs.variables[k].compute()\n                             computed.append(v_rhs)\n                             if not getattr(v_lhs, compat)(v_rhs):\n@@ -204,7 +227,7 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                                 equals[k] = False\n                                 # computed variables are not to be re-computed\n                                 # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n+                                for ds, v in zip(datasets_with_var[1:], computed):\n                                     ds.variables[k].data = v.data\n                                 break\n                         else:\n@@ -219,17 +242,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # For outer join behavior, check if variables exist in ANY dataset\n+            all_vars_in_subset = set()\n+            for ds in datasets:\n+                all_vars_in_subset.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n-                        \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"some variables in coords are not coordinates in \"\n+                        \"any of the datasets: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"in any of the datasets: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +341,31 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            # Instead of raising error for missing variables, handle them gracefully\n+            present_vars = set(ds.variables) & variables_to_merge\n+            absent_vars = variables_to_merge - present_vars\n+            \n+            for var in present_vars:\n+                to_merge[var].append(ds.variables[var])\n+            \n+            # For absent variables, create NaN-filled placeholder variables\n+            for var in absent_vars:\n+                if var in coord_names:\n+                    # For coordinates, we need to be more careful about dimensions\n+                    # Skip creating placeholders for coordinates that don't exist\n+                    to_merge[var].append(None)\n+                else:\n+                    # For data variables, create NaN-filled variables with appropriate shape\n+                    to_merge[var].append(None)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -356,12 +404,83 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    \n+    # Collect all unique variable names from all datasets for outer join behavior\n+    all_var_names = set()\n+    for ds in datasets:\n+        all_var_names.update(ds.variables.keys())\n+    \n+    for k in all_var_names:\n         if k in concat_over:\n+            # Check which datasets have this variable\n+            available_vars = []\n+            missing_datasets = []\n+            \n+            for i, ds in enumerate(datasets):\n+                if k in ds.variables:\n+                    available_vars.append((i, ds.variables[k]))\n+                else:\n+                    missing_datasets.append(i)\n+            \n+            if not available_vars:\n+                continue  # Skip if no datasets have this variable\n+                \n+            # Get a reference variable for shape/dtype information\n+            ref_var = available_vars[0][1]\n+            \n+            # Create list of variables, filling missing ones with NaN\n+            vars_to_concat = []\n+            for i, ds in enumerate(datasets):\n+                if k in ds.variables:\n+                    vars_to_concat.append(ds.variables[k])\n+                else:\n+                    # Create a variable filled with NaN with same shape as reference\n+                    fill_value = dtypes.NA if ref_var.dtype.kind in 'SUO' else dtypes.get_fill_value(ref_var.dtype)\n+                    missing_var = Variable(ref_var.dims, \n+                                         utils.full_like(ref_var, fill_value, dtype=ref_var.dtype))\n+                    vars_to_concat.append(missing_var)\n+            \n             try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+                vars = ensure_common_dims(vars_to_concat)\n+                combined = concat_vars(vars, dim, positions)\n+                assert isinstance(combined, Variable)\n+                result_vars[k] = combined\n+            except Exception:\n+                # If we can't concatenate (e.g., incompatible dimensions), skip this variable\n+                continue\n+                for ds in datasets:\n+                    if k in ds.variables:\n+                        vars.append(ds.variables[k])\n+                    else:\n+                        # Get the variable from a dataset that has it to determine shape and attributes\n+                        reference_var = None\n+                        for ref_ds in datasets:\n+                            if k in ref_ds.variables:\n+                                reference_var = ref_ds.variables[k]\n+                                break\n+                        \n+                        if reference_var is not None:\n+                            # Create a variable with the same dims but filled with NaN\n+                            import numpy as np\n+                            fill_value = np.nan if reference_var.dtype.kind in 'fcmM' else None\n+                            shape = tuple(ds.dims.get(d, reference_var.sizes[d]) for d in reference_var.dims)\n+                            \n+                            if fill_value is not None:\n+                                data = np.full(shape, fill_value, dtype=reference_var.dtype)\n+                            else:\n+                                # For non-numeric dtypes, create an array of the appropriate type\n+                                data = np.empty(shape, dtype=reference_var.dtype)\n+                                data.fill(reference_var._data.flat[0] if reference_var.size > 0 else None)\n+                            \n+                            missing_var = Variable(reference_var.dims, data, reference_var.attrs)\n+                            vars.append(missing_var)\n+                        else:\n+                            raise ValueError(\"%r is not present in all datasets.\" % k)\n+                \n+                vars = ensure_common_dims(vars)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n",
    "variant_7": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..d079b783 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,28 +187,51 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n-                    if k not in concat_over:\n-                        # Compare the variable of all datasets vs. the one\n-                        # of the first dataset. Perform the minimum amount of\n-                        # loads in order to avoid multiple loads from disk\n-                        # while keeping the RAM footprint low.\n-                        v_lhs = datasets[0].variables[k].load()\n-                        # We'll need to know later on if variables are equal.\n-                        computed = []\n-                        for ds_rhs in datasets[1:]:\n-                            v_rhs = ds_rhs.variables[k].compute()\n-                            computed.append(v_rhs)\n-                            if not getattr(v_lhs, compat)(v_rhs):\n-                                concat_over.add(k)\n-                                equals[k] = False\n-                                # computed variables are not to be re-computed\n-                                # again in the future\n-                                for ds, v in zip(datasets[1:], computed):\n-                                    ds.variables[k].data = v.data\n-                                break\n-                        else:\n-                            equals[k] = True\n+\n+def process_subset_opt(opt, subset):\n+    if isinstance(opt, str):\n+        if opt == \"different\":\n+            if compat == \"override\":\n+                raise ValueError(\n+                    \"Cannot specify both %s='different' and compat='override'.\"\n+                    % subset\n+                )\n+            # Collect all variables from all datasets\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            # Check which variables differ across datasets\n+            for k in all_vars:\n+                if k not in concat_over:\n+                    # Check if variable exists in all datasets\n+                    datasets_with_var = [ds for ds in datasets if k in getattr(ds, subset)]\n+                    if len(datasets_with_var) < len(datasets):\n+                        # Variable missing from some datasets - add to concat_over\n+                        concat_over.add(k)\n+                        equals[k] = False\n+                        continue\n+                    \n+                    # Compare the variable of all datasets vs. the one\n+                    # of the first dataset. Perform the minimum amount of\n+                    # loads in order to avoid multiple loads from disk\n+                    # while keeping the RAM footprint low.\n+                    v_lhs = datasets_with_var[0].variables[k].load()\n+                    # We'll need to know later on if variables are equal.\n+                    computed = []\n+                    for ds_rhs in datasets_with_var[1:]:\n+                        v_rhs = ds_rhs.variables[k].compute()\n+                        computed.append(v_rhs)\n+                        if not getattr(v_lhs, compat)(v_rhs):\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            # computed variables are not to be re-computed\n+                            # again in the future\n+                            for ds, v in zip(datasets_with_var[1:], computed):\n+                                ds.variables[k].data = v.data\n+                            break\n+                    else:\n+                        equals[k] = True\n \n             elif opt == \"all\":\n                 concat_over.update(\n@@ -219,17 +242,24 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+        else:\n+            # Collect all variables from all datasets for validation\n+            all_vars = set()\n+            for ds in datasets:\n+                all_vars.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +342,29 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a dummy variable filled with NaN for missing variables\n+                    if variables_to_merge and var in to_merge and to_merge[var]:\n+                        # Use the first available variable as template for shape/dims\n+                        template_var = to_merge[var][0]\n+                        dummy_var = Variable(\n+                            template_var.dims,\n+                            pd.array([fill_value] * template_var.size).reshape(template_var.shape),\n+                            template_var.attrs\n+                        )\n+                        to_merge[var].append(dummy_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -356,12 +403,78 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    \n+    # Collect all unique variable names from all datasets\n+    all_var_names = set()\n+    for ds in datasets:\n+        all_var_names.update(ds.variables.keys())\n+    \n+    for k in all_var_names:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+            # Get variables from datasets that have this variable\n+            available_vars = []\n+            missing_indices = []\n+            \n+            for i, ds in enumerate(datasets):\n+                if k in ds.variables:\n+                    available_vars.append(ds.variables[k])\n+                else:\n+                    missing_indices.append(i)\n+            \n+            if not available_vars:\n+                continue  # Skip if no datasets have this variable\n+                \n+            # Create missing variables with NaN fill for datasets that don't have this variable\n+            if missing_indices:\n+                # Use the first available variable as a template\n+                template_var = available_vars[0]\n+                vars_with_missing = []\n+                \n+                for i, ds in enumerate(datasets):\n+                    if i in missing_indices:\n+                        # Create a NaN-filled variable with the same shape as template\n+                        missing_var = Variable(\n+                            template_var.dims,\n+                            utils.full_like(template_var, fill_value, template_var.dtype),\n+                            template_var.attrs\n+                        )\n+                        vars_with_missing.append(missing_var)\n+                    else:\n+                        vars_with_missing.append(ds.variables[k])\n+                \n+                vars = ensure_common_dims(vars_with_missing)\n+            else:\n+                vars = ensure_common_dims(available_vars)\n+                \n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+                        available_vars.append(ds.variables[k])\n+                    else:\n+                        # Create a NaN-filled variable with appropriate shape for missing variables\n+                        # Use the first available variable as a template for dimensions and dtype\n+                        if available_vars:\n+                            template_var = available_vars[0]\n+                            # Create variable filled with NaN, matching template dimensions\n+                            fill_value = dtypes.NA if template_var.dtype.kind in 'cfmM' else dtypes.get_fill_value(template_var.dtype)\n+                            missing_var = Variable(template_var.dims, \n+                                                 utils.full_like(template_var.data, fill_value, dtype=template_var.dtype),\n+                                                 attrs=template_var.attrs.copy())\n+                            available_vars.append(missing_var)\n+                        else:\n+                            # Skip this variable if no datasets have it\n+                            continue\n+                \n+                if available_vars:\n+                    vars = ensure_common_dims(available_vars)\n+                else:\n+                    continue\n             except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+                # This should not happen with the new logic above, but keep as fallback\n+                continue\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n",
    "variant_8": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\nindex ecae2566..02d52056 100644\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -187,8 +187,29 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n                         % subset\n                     )\n                 # all nonindexes that are not the same in each dataset\n-                for k in getattr(datasets[0], subset):\n+\n+    def process_subset_opt(opt, subset):\n+        if isinstance(opt, str):\n+            if opt == \"different\":\n+                if compat == \"override\":\n+                    raise ValueError(\n+                        \"Cannot specify both %s='different' and compat='override'.\"\n+                        % subset\n+                    )\n+                # Collect all variables from all datasets first\n+                all_vars = set()\n+                for ds in datasets:\n+                    all_vars.update(getattr(ds, subset))\n+                \n+                # Check which variables are different across datasets\n+                for k in all_vars:\n                     if k not in concat_over:\n+                        # Skip variables that don't exist in all datasets\n+                        if not all(k in getattr(ds, subset) for ds in datasets):\n+                            concat_over.add(k)\n+                            equals[k] = False\n+                            continue\n+                            \n                         # Compare the variable of all datasets vs. the one\n                         # of the first dataset. Perform the minimum amount of\n                         # loads in order to avoid multiple loads from disk\n@@ -219,17 +240,23 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n             else:\n                 raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n         else:\n-            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n+\n+            # For outer join behavior, check if variables exist in ANY dataset, not just the first\n+            all_vars_in_subset = set()\n+            for ds in datasets:\n+                all_vars_in_subset.update(getattr(ds, subset))\n+            \n+            invalid_vars = [k for k in opt if k not in all_vars_in_subset]\n             if invalid_vars:\n                 if subset == \"coords\":\n                     raise ValueError(\n                         \"some variables in coords are not coordinates on \"\n-                        \"the first dataset: %s\" % (invalid_vars,)\n+                        \"any dataset: %s\" % (invalid_vars,)\n                     )\n                 else:\n                     raise ValueError(\n                         \"some variables in data_vars are not data variables \"\n-                        \"on the first dataset: %s\" % (invalid_vars,)\n+                        \"on any dataset: %s\" % (invalid_vars,)\n                     )\n             concat_over.update(opt)\n \n@@ -312,12 +339,37 @@ def _dataset_concat(\n         to_merge = {var: [] for var in variables_to_merge}\n \n         for ds in datasets:\n-            absent_merge_vars = variables_to_merge - set(ds.variables)\n-            if absent_merge_vars:\n-                raise ValueError(\n-                    \"variables %r are present in some datasets but not others. \"\n-                    % absent_merge_vars\n-                )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            # For outer join behavior, create missing variables with fill_value\n+            for var in variables_to_merge:\n+                if var in ds.variables:\n+                    to_merge[var].append(ds.variables[var])\n+                else:\n+                    # Create a variable filled with fill_value for missing variables\n+                    if var in variables_to_merge:\n+                        # Find a reference variable from another dataset to get shape/dims\n+                        ref_var = None\n+                        for other_ds in datasets:\n+                            if var in other_ds.variables:\n+                                ref_var = other_ds.variables[var]\n+                                break\n+                        \n+                        if ref_var is not None:\n+                            # Create variable with same dims but filled with fill_value\n+                            missing_var = Variable(\n+                                ref_var.dims,\n+                                dtypes.full_like(ref_var, fill_value, dtype=ref_var.dtype),\n+                                ref_var.attrs\n+                            )\n+                            to_merge[var].append(missing_var)\n \n             for var in variables_to_merge:\n                 to_merge[var].append(ds.variables[var])\n@@ -356,12 +408,67 @@ def _dataset_concat(\n \n     # stack up each variable to fill-out the dataset (in order)\n     # n.b. this loop preserves variable order, needed for groupby.\n-    for k in datasets[0].variables:\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    \n+    # Collect all unique variable names from all datasets\n+    all_var_names = set()\n+    for ds in datasets:\n+        all_var_names.update(ds.variables.keys())\n+    \n+    for k in all_var_names:\n         if k in concat_over:\n-            try:\n-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-            except KeyError:\n-                raise ValueError(\"%r is not present in all datasets.\" % k)\n+            # Get variables from datasets that have this variable\n+            vars_with_k = []\n+            datasets_with_k = []\n+            for i, ds in enumerate(datasets):\n+                if k in ds.variables:\n+                    vars_with_k.append(ds.variables[k])\n+                    datasets_with_k.append(i)\n+            \n+            if not vars_with_k:\n+                continue\n+                \n+            # If variable is missing from some datasets, create fill variables\n+            if len(vars_with_k) < len(datasets):\n+                # Use the first available variable as template\n+                template_var = vars_with_k[0]\n+                \n+                # Create list with all variables (including fill variables)\n+                all_vars = []\n+                var_idx = 0\n+                for i, ds in enumerate(datasets):\n+                    if i in datasets_with_k:\n+                        all_vars.append(vars_with_k[var_idx])\n+                        var_idx += 1\n+                    else:\n+                        # Create fill variable with same dims/coords but filled with fill_value\n+                        fill_var = template_var._constructor(\n+                            template_var.dims,\n+                            utils.full_like(template_var, fill_value, dtype=template_var.dtype),\n+                            template_var.attrs,\n+                            template_var.encoding,\n+                            fastpath=True\n+                        )\n+                        all_vars.append(fill_var)\n+                \n+                vars = ensure_common_dims(all_vars)\n+            else:\n+                vars = ensure_common_dims(vars_with_k)\n+                \n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+                    else:\n+                        # Create a placeholder variable filled with NaN\n+                        # Get the shape from the first available variable\n+                        first_var = next(ds.variables[k] for ds in datasets if k in ds.variables)\n+                        placeholder_data = utils.full_like(first_var._data, fill_value=fill_value, dtype=first_var.dtype)\n+                        placeholder_var = Variable(first_var.dims, placeholder_data, first_var.attrs)\n+                        vars_list.append(placeholder_var)\n+                \n+                vars = ensure_common_dims(vars_list)\n             combined = concat_vars(vars, dim, positions)\n             assert isinstance(combined, Variable)\n             result_vars[k] = combined\n"
  }
}