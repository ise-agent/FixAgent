{
  "zero_patches": {
    "Location 1 (/root/hy/projects_4/pytest/src/_pytest/skipping.py lines 261-264)": "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n",
    "Location 2 (/root/hy/projects_4/pytest/src/_pytest/skipping.py lines 279-293)": "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    \n    # xfail handling\n    if item._store.get(skipped_by_mark_key, True) and rep.skipped and rep.longrepr:\n        if isinstance(rep.longrepr, tuple):\n            # skipped by mark.skipif; change the location of the failure\n            # to point to the item definition, otherwise it will display\n            # the location of where the skip exception was raised within pytest\n            _, _, reason = rep.longrepr\n            filename, line = item.reportinfo()[:2]\n            assert line is not None\n            rep.longrepr = str(filename), line + 1, reason\n\n    evalxfail = item._store.get(evalxfail_key, None)\n    # unittests with @unittest.expectedFailure exposes the .expectedFailure\n    # attribute when compiled on 3.4+\n    if evalxfail is None:\n        if hasattr(item, \"expectedFailure\"):\n            evalxfail = True\n        elif item.config.option.runxfail:\n            evalxfail = False\n        else:\n            evalxfail = True\n\n    # pytest.xfail() was called\n    if rep.skipped and rep.longrepr and rep.longrepr[2] == \"imperative skip\":\n        rep.outcome = \"skipped\"\n\n    elif evalxfail:\n        if rep.passed:\n            if xfailed:\n                if call.when == \"call\":\n                    if xfailed.strict:\n                        rep.outcome = \"failed\"\n                        rep.longrepr = \"[XPASS(strict)] {}\".format(xfailed.reason)\n                    else:\n                        rep.outcome = \"passed\"\n                        rep.wasxfail = xfailed.reason\n                # exception in setup or teardown\n                else:\n                    rep.outcome = \"failed\"\n                    if xfailed.reason:\n                        rep.longrepr = \"Unexpected success: {}\".format(xfailed.reason)\n                    else:\n                        rep.longrepr = \"Unexpected success\"\n                    rep.outcome = \"failed\"\n        elif item.config.option.runxfail:\n            pass  # don't interfere\n        elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n            assert call.excinfo.value.msg is not None\n            rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n            rep.outcome = \"skipped\"\n        elif not rep.skipped and xfailed:\n            if call.excinfo:\n                raises = xfailed.raises\n                if raises is not None and not isinstance(call.excinfo.value, raises):\n                    rep.outcome = \"failed\"\n                else:\n                    rep.outcome = \"skipped\"\n                    rep.wasxfail = xfailed.reason\n            elif call.when == \"call\":\n                if xfailed.strict:\n                    rep.outcome = \"failed\"\n                    rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n                else:\n                    rep.outcome = \"passed\"\n                    rep.wasxfail = xfailed.reason\n",
    "Location 3 (/root/hy/projects_4/pytest/src/_pytest/skipping.py lines 248-251)": "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n    if not item.config.option.runxfail:\n        updated_xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = updated_xfailed\n",
    "Location 4 (/root/hy/projects_4/pytest/src/_pytest/skipping.py lines 233-245)": "\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
    "Location 5 (/root/hy/projects_4/pytest/src/_pytest/skipping.py lines 200-222)": "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_4/pytest/src/_pytest/skipping.py lines 261-264)": [
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n"
    ],
    "Location 2 (/root/hy/projects_4/pytest/src/_pytest/skipping.py lines 279-293)": [
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    \n    # unexpected success\n    if rep.when == \"call\" and xfailed and rep.outcome == \"passed\" and not rep.skipped:\n        if xfailed.strict:\n            rep.outcome = \"failed\"\n            rep.longrepr = \"[XPASS(strict)] {}\".format(xfailed.reason)\n        else:\n            rep.outcome = \"failed\"\n            if xfailed.reason:\n                rep.longrepr = \"Unexpected success: {}\".format(reason)\n            else:\n                rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    \n    # unexpected success\n    if rep.when == \"call\" and xfailed and rep.outcome == \"passed\" and not rep.skipped:\n        if xfailed.strict:\n            rep.outcome = \"failed\"\n            rep.longrepr = \"Unexpected success: {}\".format(xfailed.reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    \n    # unexpected success\n    if rep.when == \"call\" and xfailed and rep.passed and not hasattr(rep, \"wasxfail\"):\n        if xfailed.strict:\n            rep.outcome = \"failed\"\n            rep.longrepr = \"[XPASS(strict)] {}\".format(xfailed.reason)\n        else:\n            rep.outcome = \"passed\"\n            rep.wasxfail = xfailed.reason\n    # expected failure\n    elif rep.when == \"call\" and xfailed and not rep.passed and not rep.skipped:\n        if call.excinfo:\n            if xfailed.raises:\n                assert call.excinfo.value is not None\n                if isinstance(call.excinfo.value, xfailed.raises):\n                    rep.outcome = \"skipped\"\n                    rep.wasxfail = xfailed.reason\n                else:\n                    rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        else:\n            rep.outcome = \"failed\"\n            if xfailed.reason:\n                rep.longrepr = \"Unexpected success: {}\".format(reason)\n            else:\n                rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    \n    # unexpectedsuccess_key is only initialized during the call phase\n    if rep.when == \"call\" and xfailed and rep.passed:\n        if xfailed.strict:\n            rep.outcome = \"failed\"\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    \n    # unexpected success\n    if rep.when == \"call\" and xfailed and rep.outcome == \"passed\" and not rep.skipped:\n        if xfailed.strict:\n            rep.outcome = \"failed\"\n            rep.longrepr = \"Unexpected success: {}\".format(xfailed.reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    \n    # unexpected success\n    if rep.when == \"call\" and xfailed and rep.outcome == \"passed\" and not hasattr(rep, \"wasxfail\"):\n        if xfailed.strict:\n            rep.outcome = \"failed\"\n            rep.longrepr = \"[XPASS(strict)] {}\".format(xfailed.reason)\n        else:\n            rep.outcome = \"failed\"\n            rep.wasxfail = xfailed.reason\n\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    \n    # unexpected success\n    if rep.when == \"call\" and xfailed and rep.passed and not xfailed.strict:\n        if xfailed.reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    \n    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = xfailed\n    \n    # unexpected success\n    if rep.when == \"call\" and xfailed and rep.outcome == \"passed\" and not rep.skipped:\n        if xfailed.strict:\n            rep.outcome = \"failed\"\n            rep.longrepr = \"Unexpected success: {}\".format(xfailed.reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n"
    ],
    "Location 3 (/root/hy/projects_4/pytest/src/_pytest/skipping.py lines 248-251)": [
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = evaluate_xfail_marks(item)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n    if not item.config.option.runxfail:\n        updated_xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = updated_xfailed\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = evaluate_xfail_marks(item)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n    item._store[xfailed_key] = evaluate_xfail_marks(item)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = evaluate_xfail_marks(item)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n    if not item.config.option.runxfail:\n        updated_xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = updated_xfailed\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = evaluate_xfail_marks(item)\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n    if not item.config.option.runxfail:\n        updated_xfailed = evaluate_xfail_marks(item)\n        item._store[xfailed_key] = updated_xfailed\n"
    ],
    "Location 4 (/root/hy/projects_4/pytest/src/_pytest/skipping.py lines 233-245)": [
      "\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n"
    ],
    "Location 5 (/root/hy/projects_4/pytest/src/_pytest/skipping.py lines 200-222)": [
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..0312d32d3 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -229,6 +230,8 @@ xfailed_key = StoreKey[Optional[Xfail]]()\n unexpectedsuccess_key = StoreKey[str]()\n \n \n+@hookimpl(tryfirst=True)\n+\n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n@@ -243,6 +246,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n@@ -257,11 +261,18 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     yield\n \n \n+@hookimpl(hookwrapper=True)\n+\n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n     xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -276,6 +287,7 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n+\n     elif not rep.skipped and xfailed:\n         if call.excinfo:\n             raises = xfailed.raises\n",
    "variant_0": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..7f54e1b1a 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -229,6 +230,8 @@ xfailed_key = StoreKey[Optional[Xfail]]()\n unexpectedsuccess_key = StoreKey[str]()\n \n \n+@hookimpl(tryfirst=True)\n+\n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n@@ -243,6 +246,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n@@ -256,12 +260,31 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n \n     yield\n \n+    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n+    if not item.config.option.runxfail:\n+        updated_xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = updated_xfailed\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n+\n+\n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -276,21 +299,83 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n-    elif not rep.skipped and xfailed:\n-        if call.excinfo:\n-            raises = xfailed.raises\n-            if raises is not None and not isinstance(call.excinfo.value, raises):\n-                rep.outcome = \"failed\"\n-            else:\n-                rep.outcome = \"skipped\"\n-                rep.wasxfail = xfailed.reason\n-        elif call.when == \"call\":\n-            if xfailed.strict:\n-                rep.outcome = \"failed\"\n-                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n-            else:\n-                rep.outcome = \"passed\"\n-                rep.wasxfail = xfailed.reason\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    \n+    # xfail handling\n+    if item._store.get(skipped_by_mark_key, True) and rep.skipped and rep.longrepr:\n+        if isinstance(rep.longrepr, tuple):\n+            # skipped by mark.skipif; change the location of the failure\n+            # to point to the item definition, otherwise it will display\n+            # the location of where the skip exception was raised within pytest\n+            _, _, reason = rep.longrepr\n+            filename, line = item.reportinfo()[:2]\n+            assert line is not None\n+            rep.longrepr = str(filename), line + 1, reason\n+\n+    evalxfail = item._store.get(evalxfail_key, None)\n+    # unittests with @unittest.expectedFailure exposes the .expectedFailure\n+    # attribute when compiled on 3.4+\n+    if evalxfail is None:\n+        if hasattr(item, \"expectedFailure\"):\n+            evalxfail = True\n+        elif item.config.option.runxfail:\n+            evalxfail = False\n+        else:\n+            evalxfail = True\n+\n+    # pytest.xfail() was called\n+    if rep.skipped and rep.longrepr and rep.longrepr[2] == \"imperative skip\":\n+        rep.outcome = \"skipped\"\n+\n+    elif evalxfail:\n+        if rep.passed:\n+            if xfailed:\n+                if call.when == \"call\":\n+                    if xfailed.strict:\n+                        rep.outcome = \"failed\"\n+                        rep.longrepr = \"[XPASS(strict)] {}\".format(xfailed.reason)\n+                    else:\n+                        rep.outcome = \"passed\"\n+                        rep.wasxfail = xfailed.reason\n+                # exception in setup or teardown\n+                else:\n+                    rep.outcome = \"failed\"\n+                    if xfailed.reason:\n+                        rep.longrepr = \"Unexpected success: {}\".format(xfailed.reason)\n+                    else:\n+                        rep.longrepr = \"Unexpected success\"\n+                    rep.outcome = \"failed\"\n+        elif item.config.option.runxfail:\n+            pass  # don't interfere\n+        elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n+            assert call.excinfo.value.msg is not None\n+            rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n+            rep.outcome = \"skipped\"\n+        elif not rep.skipped and xfailed:\n+            if call.excinfo:\n+                raises = xfailed.raises\n+                if raises is not None and not isinstance(call.excinfo.value, raises):\n+                    rep.outcome = \"failed\"\n+                else:\n+                    rep.outcome = \"skipped\"\n+                    rep.wasxfail = xfailed.reason\n+            elif call.when == \"call\":\n+                if xfailed.strict:\n+                    rep.outcome = \"failed\"\n+                    rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n+                else:\n+                    rep.outcome = \"passed\"\n+                    rep.wasxfail = xfailed.reason\n \n     if (\n         item._store.get(skipped_by_mark_key, True)\n",
    "variant_1": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..e23acea4d 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -229,6 +230,8 @@ xfailed_key = StoreKey[Optional[Xfail]]()\n unexpectedsuccess_key = StoreKey[str]()\n \n \n+@hookimpl(tryfirst=True)\n+\n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n@@ -243,6 +246,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n@@ -256,12 +260,30 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n \n     yield\n \n+    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n+    if not item.config.option.runxfail:\n+        item._store[xfailed_key] = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n+\n+\n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -276,6 +298,36 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    \n+    # unexpected success\n+    if rep.when == \"call\" and xfailed and rep.outcome == \"passed\" and not rep.skipped:\n+        if xfailed.strict:\n+            rep.outcome = \"failed\"\n+            rep.longrepr = \"[XPASS(strict)] {}\".format(xfailed.reason)\n+        else:\n+            rep.outcome = \"failed\"\n+            if xfailed.reason:\n+                rep.longrepr = \"Unexpected success: {}\".format(reason)\n+            else:\n+                rep.longrepr = \"Unexpected success\"\n+        rep.outcome = \"failed\"\n+    elif item.config.option.runxfail:\n+        pass  # don't interfere\n+    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n+        assert call.excinfo.value.msg is not None\n+        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n+        rep.outcome = \"skipped\"\n     elif not rep.skipped and xfailed:\n         if call.excinfo:\n             raises = xfailed.raises\n",
    "variant_2": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..c8db286f0 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -229,6 +230,8 @@ xfailed_key = StoreKey[Optional[Xfail]]()\n unexpectedsuccess_key = StoreKey[str]()\n \n \n+@hookimpl(tryfirst=True)\n+\n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n@@ -243,6 +246,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n@@ -256,12 +260,31 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n \n     yield\n \n+    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n+    if not item.config.option.runxfail:\n+        updated_xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = updated_xfailed\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n+\n+\n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -276,6 +299,32 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    \n+    # unexpected success\n+    if rep.when == \"call\" and xfailed and rep.outcome == \"passed\" and not rep.skipped:\n+        if xfailed.strict:\n+            rep.outcome = \"failed\"\n+            rep.longrepr = \"Unexpected success: {}\".format(xfailed.reason)\n+        else:\n+            rep.longrepr = \"Unexpected success\"\n+        rep.outcome = \"failed\"\n+    elif item.config.option.runxfail:\n+        pass  # don't interfere\n+    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n+        assert call.excinfo.value.msg is not None\n+        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n+        rep.outcome = \"skipped\"\n     elif not rep.skipped and xfailed:\n         if call.excinfo:\n             raises = xfailed.raises\n",
    "variant_3": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..e509fa3ec 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -229,6 +230,8 @@ xfailed_key = StoreKey[Optional[Xfail]]()\n unexpectedsuccess_key = StoreKey[str]()\n \n \n+@hookimpl(tryfirst=True)\n+\n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n@@ -243,6 +246,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n@@ -256,12 +260,30 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n \n     yield\n \n+    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n+    if not item.config.option.runxfail:\n+        item._store[xfailed_key] = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n+\n+\n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -276,6 +298,52 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    \n+    # unexpected success\n+    if rep.when == \"call\" and xfailed and rep.passed and not hasattr(rep, \"wasxfail\"):\n+        if xfailed.strict:\n+            rep.outcome = \"failed\"\n+            rep.longrepr = \"[XPASS(strict)] {}\".format(xfailed.reason)\n+        else:\n+            rep.outcome = \"passed\"\n+            rep.wasxfail = xfailed.reason\n+    # expected failure\n+    elif rep.when == \"call\" and xfailed and not rep.passed and not rep.skipped:\n+        if call.excinfo:\n+            if xfailed.raises:\n+                assert call.excinfo.value is not None\n+                if isinstance(call.excinfo.value, xfailed.raises):\n+                    rep.outcome = \"skipped\"\n+                    rep.wasxfail = xfailed.reason\n+                else:\n+                    rep.outcome = \"failed\"\n+            else:\n+                rep.outcome = \"skipped\"\n+                rep.wasxfail = xfailed.reason\n+        else:\n+            rep.outcome = \"failed\"\n+            if xfailed.reason:\n+                rep.longrepr = \"Unexpected success: {}\".format(reason)\n+            else:\n+                rep.longrepr = \"Unexpected success\"\n+        rep.outcome = \"failed\"\n+    elif item.config.option.runxfail:\n+        pass  # don't interfere\n+    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n+        assert call.excinfo.value.msg is not None\n+        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n+        rep.outcome = \"skipped\"\n     elif not rep.skipped and xfailed:\n         if call.excinfo:\n             raises = xfailed.raises\n",
    "variant_4": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..d447e91d7 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -229,6 +230,8 @@ xfailed_key = StoreKey[Optional[Xfail]]()\n unexpectedsuccess_key = StoreKey[str]()\n \n \n+@hookimpl(tryfirst=True)\n+\n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n@@ -243,6 +246,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n@@ -256,12 +260,29 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n \n     yield\n \n+    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n+    item._store[xfailed_key] = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n+\n+\n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -276,6 +297,32 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    \n+    # unexpectedsuccess_key is only initialized during the call phase\n+    if rep.when == \"call\" and xfailed and rep.passed:\n+        if xfailed.strict:\n+            rep.outcome = \"failed\"\n+            rep.longrepr = \"Unexpected success: {}\".format(reason)\n+        else:\n+            rep.longrepr = \"Unexpected success\"\n+        rep.outcome = \"failed\"\n+    elif item.config.option.runxfail:\n+        pass  # don't interfere\n+    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n+        assert call.excinfo.value.msg is not None\n+        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n+        rep.outcome = \"skipped\"\n     elif not rep.skipped and xfailed:\n         if call.excinfo:\n             raises = xfailed.raises\n",
    "variant_5": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..212f44ed2 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -229,6 +230,8 @@ xfailed_key = StoreKey[Optional[Xfail]]()\n unexpectedsuccess_key = StoreKey[str]()\n \n \n+@hookimpl(tryfirst=True)\n+\n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n@@ -243,6 +246,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n@@ -256,12 +260,30 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n \n     yield\n \n+    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n+    if not item.config.option.runxfail:\n+        item._store[xfailed_key] = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n+\n+\n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -276,6 +298,32 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    \n+    # unexpected success\n+    if rep.when == \"call\" and xfailed and rep.outcome == \"passed\" and not rep.skipped:\n+        if xfailed.strict:\n+            rep.outcome = \"failed\"\n+            rep.longrepr = \"Unexpected success: {}\".format(xfailed.reason)\n+        else:\n+            rep.longrepr = \"Unexpected success\"\n+        rep.outcome = \"failed\"\n+    elif item.config.option.runxfail:\n+        pass  # don't interfere\n+    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n+        assert call.excinfo.value.msg is not None\n+        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n+        rep.outcome = \"skipped\"\n     elif not rep.skipped and xfailed:\n         if call.excinfo:\n             raises = xfailed.raises\n",
    "variant_6": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..4ec945acb 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -229,6 +230,8 @@ xfailed_key = StoreKey[Optional[Xfail]]()\n unexpectedsuccess_key = StoreKey[str]()\n \n \n+@hookimpl(tryfirst=True)\n+\n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n@@ -243,6 +246,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n@@ -256,12 +260,31 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n \n     yield\n \n+    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n+    if not item.config.option.runxfail:\n+        updated_xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = updated_xfailed\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n+\n+\n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -276,6 +299,33 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    \n+    # unexpected success\n+    if rep.when == \"call\" and xfailed and rep.outcome == \"passed\" and not hasattr(rep, \"wasxfail\"):\n+        if xfailed.strict:\n+            rep.outcome = \"failed\"\n+            rep.longrepr = \"[XPASS(strict)] {}\".format(xfailed.reason)\n+        else:\n+            rep.outcome = \"failed\"\n+            rep.wasxfail = xfailed.reason\n+\n+    elif item.config.option.runxfail:\n+        pass  # don't interfere\n+    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n+        assert call.excinfo.value.msg is not None\n+        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n+        rep.outcome = \"skipped\"\n     elif not rep.skipped and xfailed:\n         if call.excinfo:\n             raises = xfailed.raises\n",
    "variant_7": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..bd801527b 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -229,6 +230,8 @@ xfailed_key = StoreKey[Optional[Xfail]]()\n unexpectedsuccess_key = StoreKey[str]()\n \n \n+@hookimpl(tryfirst=True)\n+\n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n@@ -243,6 +246,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n@@ -256,12 +260,30 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n \n     yield\n \n+    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n+    if not item.config.option.runxfail:\n+        item._store[xfailed_key] = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n+\n+\n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -276,6 +298,31 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    \n+    # unexpected success\n+    if rep.when == \"call\" and xfailed and rep.passed and not xfailed.strict:\n+        if xfailed.reason:\n+            rep.longrepr = \"Unexpected success: {}\".format(reason)\n+        else:\n+            rep.longrepr = \"Unexpected success\"\n+        rep.outcome = \"failed\"\n+    elif item.config.option.runxfail:\n+        pass  # don't interfere\n+    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n+        assert call.excinfo.value.msg is not None\n+        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n+        rep.outcome = \"skipped\"\n     elif not rep.skipped and xfailed:\n         if call.excinfo:\n             raises = xfailed.raises\n",
    "variant_8": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..c8db286f0 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -229,6 +230,8 @@ xfailed_key = StoreKey[Optional[Xfail]]()\n unexpectedsuccess_key = StoreKey[str]()\n \n \n+@hookimpl(tryfirst=True)\n+\n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n@@ -243,6 +246,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n@@ -256,12 +260,31 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n \n     yield\n \n+    # Re-evaluate xfail marks after test execution to catch dynamically added markers\n+    if not item.config.option.runxfail:\n+        updated_xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = updated_xfailed\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n+\n+\n+@hookimpl(hookwrapper=True)\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -276,6 +299,32 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    xfailed = item._store.get(xfailed_key, None)\n+    \n+    # Re-evaluate xfail marks during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+    \n+    # unexpected success\n+    if rep.when == \"call\" and xfailed and rep.outcome == \"passed\" and not rep.skipped:\n+        if xfailed.strict:\n+            rep.outcome = \"failed\"\n+            rep.longrepr = \"Unexpected success: {}\".format(xfailed.reason)\n+        else:\n+            rep.longrepr = \"Unexpected success\"\n+        rep.outcome = \"failed\"\n+    elif item.config.option.runxfail:\n+        pass  # don't interfere\n+    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n+        assert call.excinfo.value.msg is not None\n+        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n+        rep.outcome = \"skipped\"\n     elif not rep.skipped and xfailed:\n         if call.excinfo:\n             raises = xfailed.raises\n"
  }
}