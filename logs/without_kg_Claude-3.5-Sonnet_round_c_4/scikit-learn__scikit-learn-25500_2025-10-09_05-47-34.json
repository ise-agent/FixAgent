{
  "zero_patches": {
    "Location 1 (/root/hy/projects_4/scikit-learn/sklearn/calibration.py lines 763-763)": "\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n",
    "Location 2 (/root/hy/projects_4/scikit-learn/sklearn/calibration.py lines 750-770)": "\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n",
    "Location 3 (/root/hy/projects_4/scikit-learn/sklearn/isotonic.py lines 410-413)": "\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n",
    "Location 4 (/root/hy/projects_4/scikit-learn/sklearn/isotonic.py lines 393-398)": "\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_4/scikit-learn/sklearn/calibration.py lines 763-763)": [
      "\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n",
      "\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n",
      "\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n",
      "\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n",
      "\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n",
      "\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n",
      "\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n",
      "\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n"
    ],
    "Location 2 (/root/hy/projects_4/scikit-learn/sklearn/calibration.py lines 750-770)": [
      "\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n",
      "\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n",
      "\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n",
      "\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n",
      "\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n",
      "\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n",
      "\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n",
      "\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n"
    ],
    "Location 3 (/root/hy/projects_4/scikit-learn/sklearn/isotonic.py lines 410-413)": [
      "\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n",
      "\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n",
      "\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n",
      "\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n",
      "\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n",
      "\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n",
      "\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n",
      "\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n"
    ],
    "Location 4 (/root/hy/projects_4/scikit-learn/sklearn/isotonic.py lines 393-398)": [
      "\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n",
      "\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n",
      "\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n",
      "\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n",
      "\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n",
      "\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n",
      "\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n",
      "\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 2c4a33616..f11153977 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -747,6 +747,7 @@ class _CalibratedClassifier:\n         \"\"\"\n         n_classes = len(self.classes)\n         pred_method, method_name = _get_prediction_method(self.estimator)\n+\n         predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n \n         label_encoder = LabelEncoder().fit(self.classes)\n@@ -760,7 +761,8 @@ class _CalibratedClassifier:\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            # Convert calibrator prediction to numpy array to handle pandas output\n+            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n \n         # Normalize the probabilities\n         if n_classes == 2:\n@@ -768,6 +770,7 @@ class _CalibratedClassifier:\n         else:\n             denominator = np.sum(proba, axis=1)[:, np.newaxis]\n             # In the edge case where for each class calibrator returns a null\n+            # In the edge case where for each class calibrator returns a null\n             # probability for a given sample, use the uniform distribution\n             # instead.\n             uniform_proba = np.full_like(proba, 1 / n_classes)\ndiff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex b05e59536..5f4b93f6c 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -390,6 +390,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         if self.out_of_bounds == \"clip\":\n             T = np.clip(T, self.X_min_, self.X_max_)\n \n+\n         res = self.f_(T)\n \n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n@@ -407,6 +408,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n \n         Returns\n         -------\n+\n         y_pred : ndarray of shape (n_samples,)\n             Transformed data.\n         \"\"\"\n",
    "variant_0": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 2c4a33616..316cd900f 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -745,6 +745,9 @@ class _CalibratedClassifier:\n         proba : array, shape (n_samples, n_classes)\n             The predicted probabilities. Can be exact zeros.\n         \"\"\"\n+        n_classes = len(self.classes)\n+        pred_method, method_name = _get_prediction_method(self.estimator)\n+\n         n_classes = len(self.classes)\n         pred_method, method_name = _get_prediction_method(self.estimator)\n         predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n@@ -760,7 +763,7 @@ class _CalibratedClassifier:\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n \n         # Normalize the probabilities\n         if n_classes == 2:\ndiff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex b05e59536..d44590922 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -390,6 +390,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         if self.out_of_bounds == \"clip\":\n             T = np.clip(T, self.X_min_, self.X_max_)\n \n+\n         res = self.f_(T)\n \n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n@@ -397,6 +398,17 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n \n         return res\n \n+    def predict(self, T):\n+        \"\"\"Predict new data by linear interpolation.\n+\n+        Parameters\n+        ----------\n+        T : array-like of shape (n_samples,) or (n_samples, 1)\n+            Data to transform.\n+\n+        Returns\n+        -------\n+\n     def predict(self, T):\n         \"\"\"Predict new data by linear interpolation.\n \n",
    "variant_1": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 2c4a33616..316cd900f 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -745,6 +745,9 @@ class _CalibratedClassifier:\n         proba : array, shape (n_samples, n_classes)\n             The predicted probabilities. Can be exact zeros.\n         \"\"\"\n+        n_classes = len(self.classes)\n+        pred_method, method_name = _get_prediction_method(self.estimator)\n+\n         n_classes = len(self.classes)\n         pred_method, method_name = _get_prediction_method(self.estimator)\n         predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n@@ -760,7 +763,7 @@ class _CalibratedClassifier:\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n \n         # Normalize the probabilities\n         if n_classes == 2:\ndiff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex b05e59536..d44590922 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -390,6 +390,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         if self.out_of_bounds == \"clip\":\n             T = np.clip(T, self.X_min_, self.X_max_)\n \n+\n         res = self.f_(T)\n \n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n@@ -397,6 +398,17 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n \n         return res\n \n+    def predict(self, T):\n+        \"\"\"Predict new data by linear interpolation.\n+\n+        Parameters\n+        ----------\n+        T : array-like of shape (n_samples,) or (n_samples, 1)\n+            Data to transform.\n+\n+        Returns\n+        -------\n+\n     def predict(self, T):\n         \"\"\"Predict new data by linear interpolation.\n \n",
    "variant_2": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 2c4a33616..316cd900f 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -745,6 +745,9 @@ class _CalibratedClassifier:\n         proba : array, shape (n_samples, n_classes)\n             The predicted probabilities. Can be exact zeros.\n         \"\"\"\n+        n_classes = len(self.classes)\n+        pred_method, method_name = _get_prediction_method(self.estimator)\n+\n         n_classes = len(self.classes)\n         pred_method, method_name = _get_prediction_method(self.estimator)\n         predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n@@ -760,7 +763,7 @@ class _CalibratedClassifier:\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n \n         # Normalize the probabilities\n         if n_classes == 2:\ndiff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex b05e59536..d44590922 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -390,6 +390,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         if self.out_of_bounds == \"clip\":\n             T = np.clip(T, self.X_min_, self.X_max_)\n \n+\n         res = self.f_(T)\n \n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n@@ -397,6 +398,17 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n \n         return res\n \n+    def predict(self, T):\n+        \"\"\"Predict new data by linear interpolation.\n+\n+        Parameters\n+        ----------\n+        T : array-like of shape (n_samples,) or (n_samples, 1)\n+            Data to transform.\n+\n+        Returns\n+        -------\n+\n     def predict(self, T):\n         \"\"\"Predict new data by linear interpolation.\n \n",
    "variant_3": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 2c4a33616..316cd900f 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -745,6 +745,9 @@ class _CalibratedClassifier:\n         proba : array, shape (n_samples, n_classes)\n             The predicted probabilities. Can be exact zeros.\n         \"\"\"\n+        n_classes = len(self.classes)\n+        pred_method, method_name = _get_prediction_method(self.estimator)\n+\n         n_classes = len(self.classes)\n         pred_method, method_name = _get_prediction_method(self.estimator)\n         predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n@@ -760,7 +763,7 @@ class _CalibratedClassifier:\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n \n         # Normalize the probabilities\n         if n_classes == 2:\ndiff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex b05e59536..d44590922 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -390,6 +390,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         if self.out_of_bounds == \"clip\":\n             T = np.clip(T, self.X_min_, self.X_max_)\n \n+\n         res = self.f_(T)\n \n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n@@ -397,6 +398,17 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n \n         return res\n \n+    def predict(self, T):\n+        \"\"\"Predict new data by linear interpolation.\n+\n+        Parameters\n+        ----------\n+        T : array-like of shape (n_samples,) or (n_samples, 1)\n+            Data to transform.\n+\n+        Returns\n+        -------\n+\n     def predict(self, T):\n         \"\"\"Predict new data by linear interpolation.\n \n",
    "variant_4": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 2c4a33616..316cd900f 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -745,6 +745,9 @@ class _CalibratedClassifier:\n         proba : array, shape (n_samples, n_classes)\n             The predicted probabilities. Can be exact zeros.\n         \"\"\"\n+        n_classes = len(self.classes)\n+        pred_method, method_name = _get_prediction_method(self.estimator)\n+\n         n_classes = len(self.classes)\n         pred_method, method_name = _get_prediction_method(self.estimator)\n         predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n@@ -760,7 +763,7 @@ class _CalibratedClassifier:\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n \n         # Normalize the probabilities\n         if n_classes == 2:\ndiff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex b05e59536..d44590922 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -390,6 +390,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         if self.out_of_bounds == \"clip\":\n             T = np.clip(T, self.X_min_, self.X_max_)\n \n+\n         res = self.f_(T)\n \n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n@@ -397,6 +398,17 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n \n         return res\n \n+    def predict(self, T):\n+        \"\"\"Predict new data by linear interpolation.\n+\n+        Parameters\n+        ----------\n+        T : array-like of shape (n_samples,) or (n_samples, 1)\n+            Data to transform.\n+\n+        Returns\n+        -------\n+\n     def predict(self, T):\n         \"\"\"Predict new data by linear interpolation.\n \n",
    "variant_5": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 2c4a33616..316cd900f 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -745,6 +745,9 @@ class _CalibratedClassifier:\n         proba : array, shape (n_samples, n_classes)\n             The predicted probabilities. Can be exact zeros.\n         \"\"\"\n+        n_classes = len(self.classes)\n+        pred_method, method_name = _get_prediction_method(self.estimator)\n+\n         n_classes = len(self.classes)\n         pred_method, method_name = _get_prediction_method(self.estimator)\n         predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n@@ -760,7 +763,7 @@ class _CalibratedClassifier:\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n \n         # Normalize the probabilities\n         if n_classes == 2:\ndiff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex b05e59536..d44590922 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -390,6 +390,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         if self.out_of_bounds == \"clip\":\n             T = np.clip(T, self.X_min_, self.X_max_)\n \n+\n         res = self.f_(T)\n \n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n@@ -397,6 +398,17 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n \n         return res\n \n+    def predict(self, T):\n+        \"\"\"Predict new data by linear interpolation.\n+\n+        Parameters\n+        ----------\n+        T : array-like of shape (n_samples,) or (n_samples, 1)\n+            Data to transform.\n+\n+        Returns\n+        -------\n+\n     def predict(self, T):\n         \"\"\"Predict new data by linear interpolation.\n \n",
    "variant_6": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 2c4a33616..316cd900f 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -745,6 +745,9 @@ class _CalibratedClassifier:\n         proba : array, shape (n_samples, n_classes)\n             The predicted probabilities. Can be exact zeros.\n         \"\"\"\n+        n_classes = len(self.classes)\n+        pred_method, method_name = _get_prediction_method(self.estimator)\n+\n         n_classes = len(self.classes)\n         pred_method, method_name = _get_prediction_method(self.estimator)\n         predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n@@ -760,7 +763,7 @@ class _CalibratedClassifier:\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n \n         # Normalize the probabilities\n         if n_classes == 2:\ndiff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex b05e59536..d44590922 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -390,6 +390,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         if self.out_of_bounds == \"clip\":\n             T = np.clip(T, self.X_min_, self.X_max_)\n \n+\n         res = self.f_(T)\n \n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n@@ -397,6 +398,17 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n \n         return res\n \n+    def predict(self, T):\n+        \"\"\"Predict new data by linear interpolation.\n+\n+        Parameters\n+        ----------\n+        T : array-like of shape (n_samples,) or (n_samples, 1)\n+            Data to transform.\n+\n+        Returns\n+        -------\n+\n     def predict(self, T):\n         \"\"\"Predict new data by linear interpolation.\n \n",
    "variant_7": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 2c4a33616..316cd900f 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -745,6 +745,9 @@ class _CalibratedClassifier:\n         proba : array, shape (n_samples, n_classes)\n             The predicted probabilities. Can be exact zeros.\n         \"\"\"\n+        n_classes = len(self.classes)\n+        pred_method, method_name = _get_prediction_method(self.estimator)\n+\n         n_classes = len(self.classes)\n         pred_method, method_name = _get_prediction_method(self.estimator)\n         predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n@@ -760,7 +763,7 @@ class _CalibratedClassifier:\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n \n         # Normalize the probabilities\n         if n_classes == 2:\ndiff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex b05e59536..d44590922 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -390,6 +390,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         if self.out_of_bounds == \"clip\":\n             T = np.clip(T, self.X_min_, self.X_max_)\n \n+\n         res = self.f_(T)\n \n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n@@ -397,6 +398,17 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n \n         return res\n \n+    def predict(self, T):\n+        \"\"\"Predict new data by linear interpolation.\n+\n+        Parameters\n+        ----------\n+        T : array-like of shape (n_samples,) or (n_samples, 1)\n+            Data to transform.\n+\n+        Returns\n+        -------\n+\n     def predict(self, T):\n         \"\"\"Predict new data by linear interpolation.\n \n",
    "variant_8": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 2c4a33616..621e3e328 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -747,10 +747,6 @@ class _CalibratedClassifier:\n         \"\"\"\n         n_classes = len(self.classes)\n         pred_method, method_name = _get_prediction_method(self.estimator)\n-        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n-\n-        label_encoder = LabelEncoder().fit(self.classes)\n-        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n \n         proba = np.zeros((_num_samples(X), n_classes))\n         for class_idx, this_pred, calibrator in zip(\n@@ -760,7 +756,7 @@ class _CalibratedClassifier:\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).ravel()\n \n         # Normalize the probabilities\n         if n_classes == 2:\ndiff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex b05e59536..d44590922 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -390,6 +390,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         if self.out_of_bounds == \"clip\":\n             T = np.clip(T, self.X_min_, self.X_max_)\n \n+\n         res = self.f_(T)\n \n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n@@ -397,6 +398,17 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n \n         return res\n \n+    def predict(self, T):\n+        \"\"\"Predict new data by linear interpolation.\n+\n+        Parameters\n+        ----------\n+        T : array-like of shape (n_samples,) or (n_samples, 1)\n+            Data to transform.\n+\n+        Returns\n+        -------\n+\n     def predict(self, T):\n         \"\"\"Predict new data by linear interpolation.\n \n"
  }
}