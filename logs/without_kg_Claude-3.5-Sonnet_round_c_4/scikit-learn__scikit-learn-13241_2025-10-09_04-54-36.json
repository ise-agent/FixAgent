{
  "zero_patches": {
    "Location 1 (/root/hy/projects_4/scikit-learn/sklearn/decomposition/kernel_pca.py lines 213-221)": "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
    "Location 2 (/root/hy/projects_4/scikit-learn/sklearn/decomposition/kernel_pca.py lines 200-212)": "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
    "Location 3 (/root/hy/projects_4/scikit-learn/sklearn/decomposition/kernel_pca.py lines 276-283)": "\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n",
    "Location 4 (/root/hy/projects_4/scikit-learn/sklearn/decomposition/kernel_pca.py lines 296-299)": "\n        # Sort eigenpairs in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # Find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with zero eigenvalues (null space)\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n\n        return K\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_4/scikit-learn/sklearn/decomposition/kernel_pca.py lines 213-221)": [
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n"
    ],
    "Location 2 (/root/hy/projects_4/scikit-learn/sklearn/decomposition/kernel_pca.py lines 200-212)": [
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n"
    ],
    "Location 3 (/root/hy/projects_4/scikit-learn/sklearn/decomposition/kernel_pca.py lines 276-283)": [
      "\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n",
      "\n    def fit_transform(self, X, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n",
      "\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n",
      "\n    def fit_transform(self, X, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n",
      "\n    def fit_transform(self, X, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n",
      "\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n",
      "\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n",
      "\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n"
    ],
    "Location 4 (/root/hy/projects_4/scikit-learn/sklearn/decomposition/kernel_pca.py lines 296-299)": [
      "\n        # Sort eigenpairs in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # Find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # Sort eigenvectors in decreasing order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # Find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # Normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # Find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n",
      "\n        # Sort eigenpairs in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # Find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenfunctions in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # Find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # Sort eigenvectors by eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Normalize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            if self.alphas_.shape[0] > 0:\n                # Find first non-zero element and make it positive\n                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n                if self.alphas_[first_nonzero_idx, i] < 0:\n                    self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n",
      "\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..11f947067 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # Normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # Find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -273,6 +282,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         self.fit(X, **params)\n \n         X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n@@ -293,6 +303,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "variant_0": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..55b5fb93e 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,32 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -260,6 +287,20 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.X_fit_ = X\n         return self\n \n+    def fit_transform(self, X, y=None, **params):\n+        \"\"\"Fit the model from data in X and transform X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            Training vector, where n_samples in the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        X_new : array-like, shape (n_samples, n_components)\n+        \"\"\"\n+\n     def fit_transform(self, X, y=None, **params):\n         \"\"\"Fit the model from data in X and transform X.\n \n@@ -293,10 +334,26 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Sort eigenpairs in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # Find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # Remove eigenvectors with zero eigenvalues (null space)\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        return K\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_1": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..3c3e8e4f6 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,32 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -260,6 +287,20 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.X_fit_ = X\n         return self\n \n+    def fit_transform(self, X, y=None, **params):\n+        \"\"\"Fit the model from data in X and transform X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            Training vector, where n_samples in the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        X_new : array-like, shape (n_samples, n_components)\n+        \"\"\"\n+\n     def fit_transform(self, X, y=None, **params):\n         \"\"\"Fit the model from data in X and transform X.\n \n@@ -293,10 +334,24 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Sort eigenpairs in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # Find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_2": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..01bf3aaec 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,32 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -273,6 +300,20 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n+    def fit_transform(self, X, **params):\n+        \"\"\"Fit the model from data in X and transform X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            Training vector, where n_samples in the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        X_new : array-like, shape (n_samples, n_components)\n+        \"\"\"\n         self.fit(X, **params)\n \n         X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n@@ -293,10 +334,24 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Sort eigenvectors in decreasing order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # Find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_3": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..0689610c3 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,32 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -260,6 +287,20 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.X_fit_ = X\n         return self\n \n+    def fit_transform(self, X, y=None, **params):\n+        \"\"\"Fit the model from data in X and transform X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            Training vector, where n_samples in the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        X_new : array-like, shape (n_samples, n_components)\n+        \"\"\"\n+\n     def fit_transform(self, X, y=None, **params):\n         \"\"\"Fit the model from data in X and transform X.\n \n@@ -293,7 +334,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n+\n+        # Normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # Find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n         return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n",
    "variant_4": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..a8d857c65 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,32 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -273,6 +300,20 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n+    def fit_transform(self, X, **params):\n+        \"\"\"Fit the model from data in X and transform X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            Training vector, where n_samples in the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        X_new : array-like, shape (n_samples, n_components)\n+        \"\"\"\n         self.fit(X, **params)\n \n         X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n@@ -293,10 +334,24 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Sort eigenpairs in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # Find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_5": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..e2999ebf7 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,32 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -273,6 +300,20 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n+    def fit_transform(self, X, **params):\n+        \"\"\"Fit the model from data in X and transform X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            Training vector, where n_samples in the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        X_new : array-like, shape (n_samples, n_components)\n+        \"\"\"\n         self.fit(X, **params)\n \n         X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n@@ -293,10 +334,24 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # sort eigenfunctions in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # Find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_6": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..b9fbd62ed 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,32 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -260,6 +287,20 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.X_fit_ = X\n         return self\n \n+    def fit_transform(self, X, y=None, **params):\n+        \"\"\"Fit the model from data in X and transform X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            Training vector, where n_samples in the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        X_new : array-like, shape (n_samples, n_components)\n+        \"\"\"\n+\n     def fit_transform(self, X, y=None, **params):\n         \"\"\"Fit the model from data in X and transform X.\n \n@@ -293,10 +334,24 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Sort eigenvectors by eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # Find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_7": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..ce314117c 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,32 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -260,6 +287,20 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.X_fit_ = X\n         return self\n \n+    def fit_transform(self, X, y=None, **params):\n+        \"\"\"Fit the model from data in X and transform X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            Training vector, where n_samples in the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        X_new : array-like, shape (n_samples, n_components)\n+        \"\"\"\n+\n     def fit_transform(self, X, y=None, **params):\n         \"\"\"Fit the model from data in X and transform X.\n \n@@ -293,6 +334,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "variant_8": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..ce314117c 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -215,6 +216,32 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # normalize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            if self.alphas_.shape[0] > 0:\n+                # find first non-zero element and make it positive\n+                first_nonzero_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+                if self.alphas_[first_nonzero_idx, i] < 0:\n+                    self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n@@ -260,6 +287,20 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.X_fit_ = X\n         return self\n \n+    def fit_transform(self, X, y=None, **params):\n+        \"\"\"Fit the model from data in X and transform X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            Training vector, where n_samples in the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        X_new : array-like, shape (n_samples, n_components)\n+        \"\"\"\n+\n     def fit_transform(self, X, y=None, **params):\n         \"\"\"Fit the model from data in X and transform X.\n \n@@ -293,6 +334,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n"
  }
}