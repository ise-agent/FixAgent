{
  "zero_patches": {
    "Location 1 (/root/hy/projects_4/scikit-learn/sklearn/mixture/base.py lines 67-67)": "\nclass BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n",
    "Location 2 (/root/hy/projects_4/scikit-learn/sklearn/mixture/base.py lines 240-243)": "\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.labels_ = self.predict(X)\n\n        return self\n",
    "Location 3 (/root/hy/projects_4/scikit-learn/sklearn/mixture/base.py lines 1-20)": "\n\"\"\"Base class for mixture models.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom time import time\n\nimport numpy as np\n\nfrom .. import cluster\nfrom ..base import BaseEstimator\nfrom ..base import DensityMixin\nfrom ..base import ClusterMixin\nfrom ..externals import six\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state\n",
    "Location 4 (/root/hy/projects_4/scikit-learn/sklearn/mixture/gaussian_mixture.py lines 448-448)": "\n    n_components : int, defaults to 1.\n        The number of mixture components.\n\n    n_clusters : int, defaults to 1.\n        Alias for n_components. The number of mixture components.\n        This parameter is provided for compatibility with the clusterer interface.\n",
    "Location 5 (/root/hy/projects_4/scikit-learn/sklearn/mixture/bayesian_mixture.py lines 200-250)": "\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the likelihood (of the training data with\n        respect to the model) of the best fit of inference.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_4/scikit-learn/sklearn/mixture/base.py lines 67-67)": [
      "\nclass BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n",
      "\nclass BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n",
      "\nclass BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n",
      "\nclass BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n",
      "\nclass BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n",
      "\nclass BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n",
      "\nclass BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n",
      "\nfrom __future__ import print_function\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom time import time\n\nimport numpy as np\n\nfrom .. import cluster\nfrom ..base import BaseEstimator\nfrom ..base import DensityMixin\nfrom ..base import ClusterMixin\nfrom ..externals import six\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state\nfrom ..utils.fixes import logsumexp\n\n\ndef _check_shape(X, n_components):\n    \"\"\"Validate the shape of the input parameter 'X'.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n\n    n_components : int\n        Number of components.\n\n    Returns\n    -------\n    X : array, shape (n_samples, n_features)\n    \"\"\"\n    X = check_array(X, dtype=[np.float64, np.float32])\n    if X.shape[0] < n_components:\n        raise ValueError('Expected n_samples >= n_components '\n                         'but got n_components = %d, n_samples = %d'\n                         % (n_components, X.shape[0]))\n    return X\n\n\ndef _check_X(X, n_components, n_features, ensure_min_samples=1):\n    \"\"\"Check the input data X.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n\n    n_components : int\n        Number of components.\n\n    Returns\n    -------\n    X : array, shape (n_samples, n_features)\n    \"\"\"\n    X = check_array(X, dtype=[np.float64, np.float32],\n                    ensure_min_samples=ensure_min_samples)\n    if X.shape[0] < n_components:\n        raise ValueError('Expected n_samples >= n_components '\n                         'but got n_components = %d, n_samples = %d'\n                         % (n_components, X.shape[0]))\n    if n_features is not None and X.shape[1] != n_features:\n        raise ValueError(\"Expected the input data X have %d features, \"\n                         \"but got %d features\"\n                         % (n_features, X.shape[1]))\n    return X\n\n\nclass BaseMixture(six.with_metaclass(ABCMeta, ClusterMixin, DensityMixin, BaseEstimator)):\n"
    ],
    "Location 2 (/root/hy/projects_4/scikit-learn/sklearn/mixture/base.py lines 240-243)": [
      "\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.labels_ = self.predict(X)\n\n        return self\n",
      "\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.labels_ = self.predict(X)\n\n        return self\n",
      "\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.labels_ = self.predict(X)\n\n        return self\n",
      "\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.labels_ = self.predict(X)\n\n        return self\n",
      "\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.labels_ = self.predict(X)\n\n        return self\n",
      "\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.labels_ = self.predict(X)\n\n        return self\n",
      "\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.labels_ = self.predict(X)\n\n        return self\n",
      "\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.labels_ = self.predict(X)\n\n        return self\n"
    ],
    "Location 3 (/root/hy/projects_4/scikit-learn/sklearn/mixture/base.py lines 1-20)": [
      "\n\"\"\"Base class for mixture models.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom time import time\n\nimport numpy as np\n\nfrom .. import cluster\nfrom ..base import BaseEstimator\nfrom ..base import DensityMixin\nfrom ..base import ClusterMixin\nfrom ..externals import six\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state\n",
      "\n\"\"\"Base class for mixture models.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom time import time\n\nimport numpy as np\n\nfrom .. import cluster\nfrom ..base import BaseEstimator\nfrom ..base import DensityMixin\nfrom ..base import ClusterMixin\nfrom ..externals import six\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state\n",
      "\n\"\"\"Base class for mixture models.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom time import time\n\nimport numpy as np\n\nfrom .. import cluster\nfrom ..base import BaseEstimator\nfrom ..base import DensityMixin\nfrom ..base import ClusterMixin\nfrom ..externals import six\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state\n",
      "\n\"\"\"Base class for mixture models.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom time import time\n\nimport numpy as np\n\nfrom .. import cluster\nfrom ..base import BaseEstimator\nfrom ..base import DensityMixin\nfrom ..base import ClusterMixin\nfrom ..externals import six\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state\n",
      "\n\"\"\"Base class for mixture models.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom time import time\n\nimport numpy as np\n\nfrom .. import cluster\nfrom ..base import BaseEstimator\nfrom ..base import DensityMixin\nfrom ..base import ClusterMixin\nfrom ..externals import six\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state\n",
      "\n\"\"\"Base class for mixture models.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom time import time\n\nimport numpy as np\n\nfrom .. import cluster\nfrom ..base import BaseEstimator\nfrom ..base import DensityMixin\nfrom ..base import ClusterMixin\nfrom ..externals import six\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state\n",
      "\n\"\"\"Base class for mixture models.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom time import time\n\nimport numpy as np\n\nfrom .. import cluster\nfrom ..base import BaseEstimator\nfrom ..base import DensityMixin\nfrom ..base import ClusterMixin\nfrom ..externals import six\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state\n",
      "\n\"\"\"Base class for mixture models.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom time import time\n\nimport numpy as np\n\nfrom .. import cluster\nfrom ..base import BaseEstimator\nfrom ..base import DensityMixin\nfrom ..base import ClusterMixin\nfrom ..externals import six\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state\n"
    ],
    "Location 4 (/root/hy/projects_4/scikit-learn/sklearn/mixture/gaussian_mixture.py lines 448-448)": [
      "\n    n_components : int, defaults to 1.\n        The number of mixture components.\n\n    n_clusters : int, defaults to None.\n        Alias for n_components. If provided, overrides n_components.\n        Added for compatibility with clustering interface.\n",
      "\n    n_components : int, defaults to 1.\n        The number of mixture components.\n\n    n_clusters : int, defaults to None.\n        Alias for n_components. When provided, overrides n_components.\n        This parameter is provided for compatibility with the clusterer interface.\n",
      "\n    n_components : int, defaults to 1.\n        The number of mixture components.\n\n    n_clusters : int, defaults to None.\n        Alias for n_components. If specified, overrides n_components.\n        Provided for compatibility with clustering interface.\n",
      "\n    n_components : int, defaults to 1.\n        The number of mixture components.\n\n    n_clusters : int, defaults to None.\n        Alias for n_components. If provided, overrides n_components.\n        This parameter exists for compatibility with the clusterer interface.\n",
      "\n    n_components : int, defaults to 1.\n        The number of mixture components.\n\n    n_clusters : int, defaults to None.\n        Alias for n_components. When specified, overrides n_components.\n        Provided for compatibility with clustering interface.\n",
      "\n    n_components : int, defaults to 1.\n        The number of mixture components.\n\n    n_clusters : int, defaults to None.\n        Alias for n_components. When specified, overrides n_components.\n        Provided for compatibility with clustering interface.\n",
      "\n    n_components : int, defaults to 1.\n        The number of mixture components.\n\n    n_clusters : int, defaults to None.\n        Alias for n_components. If provided, overrides n_components.\n        This parameter is provided for compatibility with the clusterer interface.\n",
      "\n    n_components : int, defaults to 1.\n        The number of mixture components.\n\n    n_clusters : int, defaults to None.\n        Alias for n_components for compatibility with clusterer interface.\n        If specified, overrides n_components.\n"
    ],
    "Location 5 (/root/hy/projects_4/scikit-learn/sklearn/mixture/bayesian_mixture.py lines 200-250)": [
      "\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the likelihood (of the training data with\n        respect to the model) of the best fit of inference.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n",
      "\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the likelihood (of the training data with\n        respect to the model) of the best fit of inference.\n\n    labels_ : array, shape (n_samples,)\n        Component labels for each point.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n",
      "\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the likelihood (of the training data with\n        respect to the model) of the best fit of inference.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n",
      "\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the likelihood (of the training data with\n        respect to the model) of the best fit of inference.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n",
      "\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the likelihood (of the training data with\n        respect to the model) of the best fit of inference.\n\n    labels_ : array, shape (n_samples,)\n        Component labels for each point.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n",
      "\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the likelihood (of the training data with\n        respect to the model) of the best fit of inference.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n",
      "\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the likelihood (of the training data with\n        respect to the model) of the best fit of inference.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n",
      "\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the likelihood (of the training data with\n        respect to the model) of the best fit of inference.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..7c54c067f 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Base class for mixture models.\"\"\"\n \n # Author: Wei Xue <xuewei4d@gmail.com>\n@@ -15,6 +16,7 @@ import numpy as np\n from .. import cluster\n from ..base import BaseEstimator\n from ..base import DensityMixin\n+from ..base import ClusterMixin\n from ..externals import six\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state\n@@ -64,7 +66,8 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n     return X\n \n \n-class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n+\n+class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n     \"\"\"Base class for mixture models.\n \n     This abstract class specifies an interface for all mixture classes and\n@@ -237,8 +240,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n                           'or check for degenerate data.'\n                           % (init + 1), ConvergenceWarning)\n \n+\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n \n         return self\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..8e4ec9858 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -197,6 +197,7 @@ class BayesianGaussianMixture(BaseMixture):\n         The shape depends on `covariance_type`::\n \n             (n_components,)                        if 'spherical',\n+\n             (n_features, n_features)               if 'tied',\n             (n_components, n_features)             if 'diag',\n             (n_components, n_features, n_features) if 'full'\n@@ -247,7 +248,6 @@ class BayesianGaussianMixture(BaseMixture):\n \n             (float, float) if 'dirichlet_process' (Beta parameters),\n             float          if 'dirichlet_distribution' (Dirichlet parameters).\n-\n         The higher concentration puts more mass in\n         the center and will lead to more components being active, while a lower\n         concentration parameter will lead to more mass at the edge of the\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..8f015e65c 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,6 +445,7 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n+\n     n_components : int, defaults to 1.\n         The number of mixture components.\n \n",
    "variant_0": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..18b50c5c5 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Base class for mixture models.\"\"\"\n \n # Author: Wei Xue <xuewei4d@gmail.com>\n@@ -15,6 +16,7 @@ import numpy as np\n from .. import cluster\n from ..base import BaseEstimator\n from ..base import DensityMixin\n+from ..base import ClusterMixin\n from ..externals import six\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state\n@@ -64,6 +66,7 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n     return X\n \n \n+\n class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n     \"\"\"Base class for mixture models.\n \n@@ -237,8 +240,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n                           'or check for degenerate data.'\n                           % (init + 1), ConvergenceWarning)\n \n+\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n \n         return self\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..9e53f25ca 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -192,6 +192,12 @@ class BayesianGaussianMixture(BaseMixture):\n     means_ : array-like, shape (n_components, n_features)\n         The mean of each mixture component.\n \n+    covariances_ : array-like\n+        The covariance of each mixture component.\n+        The shape depends on `covariance_type`::\n+\n+            (n_components,)                        if 'spherical',\n+\n     covariances_ : array-like\n         The covariance of each mixture component.\n         The shape depends on `covariance_type`::\n@@ -247,7 +253,6 @@ class BayesianGaussianMixture(BaseMixture):\n \n             (float, float) if 'dirichlet_process' (Beta parameters),\n             float          if 'dirichlet_distribution' (Dirichlet parameters).\n-\n         The higher concentration puts more mass in\n         the center and will lead to more components being active, while a lower\n         concentration parameter will lead to more mass at the edge of the\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..636a9719d 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,9 +445,15 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n+\n     n_components : int, defaults to 1.\n         The number of mixture components.\n \n+    n_clusters : int, defaults to 1.\n+        Alias for n_components. The number of mixture components.\n+        This parameter is provided for compatibility with the clusterer interface.\n+        The number of mixture components.\n+\n     covariance_type : {'full', 'tied', 'diag', 'spherical'},\n             defaults to 'full'.\n         String describing the type of covariance parameters to use.\n",
    "variant_1": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..7c54c067f 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Base class for mixture models.\"\"\"\n \n # Author: Wei Xue <xuewei4d@gmail.com>\n@@ -15,6 +16,7 @@ import numpy as np\n from .. import cluster\n from ..base import BaseEstimator\n from ..base import DensityMixin\n+from ..base import ClusterMixin\n from ..externals import six\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state\n@@ -64,7 +66,8 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n     return X\n \n \n-class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n+\n+class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n     \"\"\"Base class for mixture models.\n \n     This abstract class specifies an interface for all mixture classes and\n@@ -237,8 +240,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n                           'or check for degenerate data.'\n                           % (init + 1), ConvergenceWarning)\n \n+\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n \n         return self\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..9e53f25ca 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -192,6 +192,12 @@ class BayesianGaussianMixture(BaseMixture):\n     means_ : array-like, shape (n_components, n_features)\n         The mean of each mixture component.\n \n+    covariances_ : array-like\n+        The covariance of each mixture component.\n+        The shape depends on `covariance_type`::\n+\n+            (n_components,)                        if 'spherical',\n+\n     covariances_ : array-like\n         The covariance of each mixture component.\n         The shape depends on `covariance_type`::\n@@ -247,7 +253,6 @@ class BayesianGaussianMixture(BaseMixture):\n \n             (float, float) if 'dirichlet_process' (Beta parameters),\n             float          if 'dirichlet_distribution' (Dirichlet parameters).\n-\n         The higher concentration puts more mass in\n         the center and will lead to more components being active, while a lower\n         concentration parameter will lead to more mass at the edge of the\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..57e85578e 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,9 +445,15 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n+\n     n_components : int, defaults to 1.\n         The number of mixture components.\n \n+    n_clusters : int, defaults to None.\n+        Alias for n_components. If provided, overrides n_components.\n+        Added for compatibility with clustering interface.\n+        The number of mixture components.\n+\n     covariance_type : {'full', 'tied', 'diag', 'spherical'},\n             defaults to 'full'.\n         String describing the type of covariance parameters to use.\n",
    "variant_2": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..7c54c067f 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Base class for mixture models.\"\"\"\n \n # Author: Wei Xue <xuewei4d@gmail.com>\n@@ -15,6 +16,7 @@ import numpy as np\n from .. import cluster\n from ..base import BaseEstimator\n from ..base import DensityMixin\n+from ..base import ClusterMixin\n from ..externals import six\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state\n@@ -64,7 +66,8 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n     return X\n \n \n-class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n+\n+class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n     \"\"\"Base class for mixture models.\n \n     This abstract class specifies an interface for all mixture classes and\n@@ -237,8 +240,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n                           'or check for degenerate data.'\n                           % (init + 1), ConvergenceWarning)\n \n+\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n \n         return self\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..f5c63ea4c 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -192,6 +192,12 @@ class BayesianGaussianMixture(BaseMixture):\n     means_ : array-like, shape (n_components, n_features)\n         The mean of each mixture component.\n \n+    covariances_ : array-like\n+        The covariance of each mixture component.\n+        The shape depends on `covariance_type`::\n+\n+            (n_components,)                        if 'spherical',\n+\n     covariances_ : array-like\n         The covariance of each mixture component.\n         The shape depends on `covariance_type`::\n@@ -240,6 +246,9 @@ class BayesianGaussianMixture(BaseMixture):\n         Lower bound value on the likelihood (of the training data with\n         respect to the model) of the best fit of inference.\n \n+    labels_ : array, shape (n_samples,)\n+        Component labels for each point.\n+\n     weight_concentration_prior_ : tuple or float\n         The dirichlet concentration of each component on the weight\n         distribution (Dirichlet). The type depends on\n@@ -247,7 +256,6 @@ class BayesianGaussianMixture(BaseMixture):\n \n             (float, float) if 'dirichlet_process' (Beta parameters),\n             float          if 'dirichlet_distribution' (Dirichlet parameters).\n-\n         The higher concentration puts more mass in\n         the center and will lead to more components being active, while a lower\n         concentration parameter will lead to more mass at the edge of the\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..0ec3f9370 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,9 +445,15 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n+\n     n_components : int, defaults to 1.\n         The number of mixture components.\n \n+    n_clusters : int, defaults to None.\n+        Alias for n_components. When provided, overrides n_components.\n+        This parameter is provided for compatibility with the clusterer interface.\n+        The number of mixture components.\n+\n     covariance_type : {'full', 'tied', 'diag', 'spherical'},\n             defaults to 'full'.\n         String describing the type of covariance parameters to use.\n",
    "variant_3": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..18b50c5c5 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Base class for mixture models.\"\"\"\n \n # Author: Wei Xue <xuewei4d@gmail.com>\n@@ -15,6 +16,7 @@ import numpy as np\n from .. import cluster\n from ..base import BaseEstimator\n from ..base import DensityMixin\n+from ..base import ClusterMixin\n from ..externals import six\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state\n@@ -64,6 +66,7 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n     return X\n \n \n+\n class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n     \"\"\"Base class for mixture models.\n \n@@ -237,8 +240,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n                           'or check for degenerate data.'\n                           % (init + 1), ConvergenceWarning)\n \n+\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n \n         return self\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..c40dec0c7 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -193,6 +193,11 @@ class BayesianGaussianMixture(BaseMixture):\n         The mean of each mixture component.\n \n     covariances_ : array-like\n+        The covariance of each mixture component.\n+        The shape depends on `covariance_type`::\n+\n+            (n_components,)                        if 'spherical',\n+\n         The covariance of each mixture component.\n         The shape depends on `covariance_type`::\n \n@@ -247,7 +252,6 @@ class BayesianGaussianMixture(BaseMixture):\n \n             (float, float) if 'dirichlet_process' (Beta parameters),\n             float          if 'dirichlet_distribution' (Dirichlet parameters).\n-\n         The higher concentration puts more mass in\n         the center and will lead to more components being active, while a lower\n         concentration parameter will lead to more mass at the edge of the\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..c20e368e2 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,9 +445,15 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n+\n     n_components : int, defaults to 1.\n         The number of mixture components.\n \n+    n_clusters : int, defaults to None.\n+        Alias for n_components. If specified, overrides n_components.\n+        Provided for compatibility with clustering interface.\n+        The number of mixture components.\n+\n     covariance_type : {'full', 'tied', 'diag', 'spherical'},\n             defaults to 'full'.\n         String describing the type of covariance parameters to use.\n",
    "variant_4": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..7c54c067f 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Base class for mixture models.\"\"\"\n \n # Author: Wei Xue <xuewei4d@gmail.com>\n@@ -15,6 +16,7 @@ import numpy as np\n from .. import cluster\n from ..base import BaseEstimator\n from ..base import DensityMixin\n+from ..base import ClusterMixin\n from ..externals import six\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state\n@@ -64,7 +66,8 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n     return X\n \n \n-class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n+\n+class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n     \"\"\"Base class for mixture models.\n \n     This abstract class specifies an interface for all mixture classes and\n@@ -237,8 +240,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n                           'or check for degenerate data.'\n                           % (init + 1), ConvergenceWarning)\n \n+\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n \n         return self\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..9e53f25ca 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -192,6 +192,12 @@ class BayesianGaussianMixture(BaseMixture):\n     means_ : array-like, shape (n_components, n_features)\n         The mean of each mixture component.\n \n+    covariances_ : array-like\n+        The covariance of each mixture component.\n+        The shape depends on `covariance_type`::\n+\n+            (n_components,)                        if 'spherical',\n+\n     covariances_ : array-like\n         The covariance of each mixture component.\n         The shape depends on `covariance_type`::\n@@ -247,7 +253,6 @@ class BayesianGaussianMixture(BaseMixture):\n \n             (float, float) if 'dirichlet_process' (Beta parameters),\n             float          if 'dirichlet_distribution' (Dirichlet parameters).\n-\n         The higher concentration puts more mass in\n         the center and will lead to more components being active, while a lower\n         concentration parameter will lead to more mass at the edge of the\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..92d4cf73e 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,9 +445,15 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n+\n     n_components : int, defaults to 1.\n         The number of mixture components.\n \n+    n_clusters : int, defaults to None.\n+        Alias for n_components. If provided, overrides n_components.\n+        This parameter exists for compatibility with the clusterer interface.\n+        The number of mixture components.\n+\n     covariance_type : {'full', 'tied', 'diag', 'spherical'},\n             defaults to 'full'.\n         String describing the type of covariance parameters to use.\n",
    "variant_5": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..7c54c067f 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Base class for mixture models.\"\"\"\n \n # Author: Wei Xue <xuewei4d@gmail.com>\n@@ -15,6 +16,7 @@ import numpy as np\n from .. import cluster\n from ..base import BaseEstimator\n from ..base import DensityMixin\n+from ..base import ClusterMixin\n from ..externals import six\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state\n@@ -64,7 +66,8 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n     return X\n \n \n-class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n+\n+class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n     \"\"\"Base class for mixture models.\n \n     This abstract class specifies an interface for all mixture classes and\n@@ -237,8 +240,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n                           'or check for degenerate data.'\n                           % (init + 1), ConvergenceWarning)\n \n+\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n \n         return self\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..f5c63ea4c 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -192,6 +192,12 @@ class BayesianGaussianMixture(BaseMixture):\n     means_ : array-like, shape (n_components, n_features)\n         The mean of each mixture component.\n \n+    covariances_ : array-like\n+        The covariance of each mixture component.\n+        The shape depends on `covariance_type`::\n+\n+            (n_components,)                        if 'spherical',\n+\n     covariances_ : array-like\n         The covariance of each mixture component.\n         The shape depends on `covariance_type`::\n@@ -240,6 +246,9 @@ class BayesianGaussianMixture(BaseMixture):\n         Lower bound value on the likelihood (of the training data with\n         respect to the model) of the best fit of inference.\n \n+    labels_ : array, shape (n_samples,)\n+        Component labels for each point.\n+\n     weight_concentration_prior_ : tuple or float\n         The dirichlet concentration of each component on the weight\n         distribution (Dirichlet). The type depends on\n@@ -247,7 +256,6 @@ class BayesianGaussianMixture(BaseMixture):\n \n             (float, float) if 'dirichlet_process' (Beta parameters),\n             float          if 'dirichlet_distribution' (Dirichlet parameters).\n-\n         The higher concentration puts more mass in\n         the center and will lead to more components being active, while a lower\n         concentration parameter will lead to more mass at the edge of the\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..6fd1577ab 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,9 +445,15 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n+\n     n_components : int, defaults to 1.\n         The number of mixture components.\n \n+    n_clusters : int, defaults to None.\n+        Alias for n_components. When specified, overrides n_components.\n+        Provided for compatibility with clustering interface.\n+        The number of mixture components.\n+\n     covariance_type : {'full', 'tied', 'diag', 'spherical'},\n             defaults to 'full'.\n         String describing the type of covariance parameters to use.\n",
    "variant_6": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..7c54c067f 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Base class for mixture models.\"\"\"\n \n # Author: Wei Xue <xuewei4d@gmail.com>\n@@ -15,6 +16,7 @@ import numpy as np\n from .. import cluster\n from ..base import BaseEstimator\n from ..base import DensityMixin\n+from ..base import ClusterMixin\n from ..externals import six\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state\n@@ -64,7 +66,8 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n     return X\n \n \n-class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n+\n+class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator, ClusterMixin)):\n     \"\"\"Base class for mixture models.\n \n     This abstract class specifies an interface for all mixture classes and\n@@ -237,8 +240,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n                           'or check for degenerate data.'\n                           % (init + 1), ConvergenceWarning)\n \n+\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n \n         return self\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..9e53f25ca 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -192,6 +192,12 @@ class BayesianGaussianMixture(BaseMixture):\n     means_ : array-like, shape (n_components, n_features)\n         The mean of each mixture component.\n \n+    covariances_ : array-like\n+        The covariance of each mixture component.\n+        The shape depends on `covariance_type`::\n+\n+            (n_components,)                        if 'spherical',\n+\n     covariances_ : array-like\n         The covariance of each mixture component.\n         The shape depends on `covariance_type`::\n@@ -247,7 +253,6 @@ class BayesianGaussianMixture(BaseMixture):\n \n             (float, float) if 'dirichlet_process' (Beta parameters),\n             float          if 'dirichlet_distribution' (Dirichlet parameters).\n-\n         The higher concentration puts more mass in\n         the center and will lead to more components being active, while a lower\n         concentration parameter will lead to more mass at the edge of the\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..6fd1577ab 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,9 +445,15 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n+\n     n_components : int, defaults to 1.\n         The number of mixture components.\n \n+    n_clusters : int, defaults to None.\n+        Alias for n_components. When specified, overrides n_components.\n+        Provided for compatibility with clustering interface.\n+        The number of mixture components.\n+\n     covariance_type : {'full', 'tied', 'diag', 'spherical'},\n             defaults to 'full'.\n         String describing the type of covariance parameters to use.\n",
    "variant_7": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..18b50c5c5 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Base class for mixture models.\"\"\"\n \n # Author: Wei Xue <xuewei4d@gmail.com>\n@@ -15,6 +16,7 @@ import numpy as np\n from .. import cluster\n from ..base import BaseEstimator\n from ..base import DensityMixin\n+from ..base import ClusterMixin\n from ..externals import six\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state\n@@ -64,6 +66,7 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n     return X\n \n \n+\n class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n     \"\"\"Base class for mixture models.\n \n@@ -237,8 +240,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n                           'or check for degenerate data.'\n                           % (init + 1), ConvergenceWarning)\n \n+\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n \n         return self\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..9e53f25ca 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -192,6 +192,12 @@ class BayesianGaussianMixture(BaseMixture):\n     means_ : array-like, shape (n_components, n_features)\n         The mean of each mixture component.\n \n+    covariances_ : array-like\n+        The covariance of each mixture component.\n+        The shape depends on `covariance_type`::\n+\n+            (n_components,)                        if 'spherical',\n+\n     covariances_ : array-like\n         The covariance of each mixture component.\n         The shape depends on `covariance_type`::\n@@ -247,7 +253,6 @@ class BayesianGaussianMixture(BaseMixture):\n \n             (float, float) if 'dirichlet_process' (Beta parameters),\n             float          if 'dirichlet_distribution' (Dirichlet parameters).\n-\n         The higher concentration puts more mass in\n         the center and will lead to more components being active, while a lower\n         concentration parameter will lead to more mass at the edge of the\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..dc6328b95 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,9 +445,15 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n+\n     n_components : int, defaults to 1.\n         The number of mixture components.\n \n+    n_clusters : int, defaults to None.\n+        Alias for n_components. If provided, overrides n_components.\n+        This parameter is provided for compatibility with the clusterer interface.\n+        The number of mixture components.\n+\n     covariance_type : {'full', 'tied', 'diag', 'spherical'},\n             defaults to 'full'.\n         String describing the type of covariance parameters to use.\n",
    "variant_8": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..38670e26d 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Base class for mixture models.\"\"\"\n \n # Author: Wei Xue <xuewei4d@gmail.com>\n@@ -15,6 +16,7 @@ import numpy as np\n from .. import cluster\n from ..base import BaseEstimator\n from ..base import DensityMixin\n+from ..base import ClusterMixin\n from ..externals import six\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state\n@@ -64,7 +66,75 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n     return X\n \n \n-class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n+\n+from __future__ import print_function\n+\n+import warnings\n+from abc import ABCMeta, abstractmethod\n+from time import time\n+\n+import numpy as np\n+\n+from .. import cluster\n+from ..base import BaseEstimator\n+from ..base import DensityMixin\n+from ..base import ClusterMixin\n+from ..externals import six\n+from ..exceptions import ConvergenceWarning\n+from ..utils import check_array, check_random_state\n+from ..utils.fixes import logsumexp\n+\n+\n+def _check_shape(X, n_components):\n+    \"\"\"Validate the shape of the input parameter 'X'.\n+\n+    Parameters\n+    ----------\n+    X : array-like, shape (n_samples, n_features)\n+\n+    n_components : int\n+        Number of components.\n+\n+    Returns\n+    -------\n+    X : array, shape (n_samples, n_features)\n+    \"\"\"\n+    X = check_array(X, dtype=[np.float64, np.float32])\n+    if X.shape[0] < n_components:\n+        raise ValueError('Expected n_samples >= n_components '\n+                         'but got n_components = %d, n_samples = %d'\n+                         % (n_components, X.shape[0]))\n+    return X\n+\n+\n+def _check_X(X, n_components, n_features, ensure_min_samples=1):\n+    \"\"\"Check the input data X.\n+\n+    Parameters\n+    ----------\n+    X : array-like, shape (n_samples, n_features)\n+\n+    n_components : int\n+        Number of components.\n+\n+    Returns\n+    -------\n+    X : array, shape (n_samples, n_features)\n+    \"\"\"\n+    X = check_array(X, dtype=[np.float64, np.float32],\n+                    ensure_min_samples=ensure_min_samples)\n+    if X.shape[0] < n_components:\n+        raise ValueError('Expected n_samples >= n_components '\n+                         'but got n_components = %d, n_samples = %d'\n+                         % (n_components, X.shape[0]))\n+    if n_features is not None and X.shape[1] != n_features:\n+        raise ValueError(\"Expected the input data X have %d features, \"\n+                         \"but got %d features\"\n+                         % (n_features, X.shape[1]))\n+    return X\n+\n+\n+class BaseMixture(six.with_metaclass(ABCMeta, ClusterMixin, DensityMixin, BaseEstimator)):\n     \"\"\"Base class for mixture models.\n \n     This abstract class specifies an interface for all mixture classes and\n@@ -237,8 +307,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n                           'or check for degenerate data.'\n                           % (init + 1), ConvergenceWarning)\n \n+\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.labels_ = self.predict(X)\n \n         return self\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..9e53f25ca 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -192,6 +192,12 @@ class BayesianGaussianMixture(BaseMixture):\n     means_ : array-like, shape (n_components, n_features)\n         The mean of each mixture component.\n \n+    covariances_ : array-like\n+        The covariance of each mixture component.\n+        The shape depends on `covariance_type`::\n+\n+            (n_components,)                        if 'spherical',\n+\n     covariances_ : array-like\n         The covariance of each mixture component.\n         The shape depends on `covariance_type`::\n@@ -247,7 +253,6 @@ class BayesianGaussianMixture(BaseMixture):\n \n             (float, float) if 'dirichlet_process' (Beta parameters),\n             float          if 'dirichlet_distribution' (Dirichlet parameters).\n-\n         The higher concentration puts more mass in\n         the center and will lead to more components being active, while a lower\n         concentration parameter will lead to more mass at the edge of the\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..b2e0a0cac 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -445,9 +445,15 @@ class GaussianMixture(BaseMixture):\n \n     Parameters\n     ----------\n+\n     n_components : int, defaults to 1.\n         The number of mixture components.\n \n+    n_clusters : int, defaults to None.\n+        Alias for n_components for compatibility with clusterer interface.\n+        If specified, overrides n_components.\n+        The number of mixture components.\n+\n     covariance_type : {'full', 'tied', 'diag', 'spherical'},\n             defaults to 'full'.\n         String describing the type of covariance parameters to use.\n"
  }
}