{
  "zero_patches": {
    "Location 1 (/root/hy/projects_3/scikit-learn/sklearn/decomposition/kernel_pca.py lines 213-222)": "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        # make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
    "Location 2 (/root/hy/projects_3/scikit-learn/sklearn/decomposition/kernel_pca.py lines 200-211)": "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the component with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If that component is negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
    "Location 3 (/root/hy/projects_3/scikit-learn/sklearn/decomposition/kernel_pca.py lines 276-283)": "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        # Make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
    "Location 4 (/root/hy/projects_3/scikit-learn/sklearn/decomposition/kernel_pca.py lines 296-299)": "\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_3/scikit-learn/sklearn/decomposition/kernel_pca.py lines 213-222)": [
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        # make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        # make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        # make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        # ensure the component with largest absolute value is positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # find the component with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # if that component is negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        # make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # find the component with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # if it's negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        # ensure the component with largest absolute value is positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n"
    ],
    "Location 2 (/root/hy/projects_3/scikit-learn/sklearn/decomposition/kernel_pca.py lines 200-211)": [
      "\n        # compute eigenvectors\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        # ensure the component with largest absolute value is positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        # ensure the component with largest absolute value is positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the component with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If that component is negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the component with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If it's negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        # ensure the component with largest absolute value is positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the component with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If that component is negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the component with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If it's negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n"
    ],
    "Location 3 (/root/hy/projects_3/scikit-learn/sklearn/decomposition/kernel_pca.py lines 276-283)": [
      "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        # Make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # Sort eigenvectors by eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n\n        # Ensure sign consistency by making the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # If n_components is specified, keep only the top n_components\n        if self.n_components is not None:\n            self.alphas_ = self.alphas_[:, :self.n_components]\n            self.lambdas_ = self.lambdas_[:self.n_components]\n",
      "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        for i in range(self.alphas_.shape[1]):\n            # Find the component with the largest absolute value\n            max_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # Make it positive to ensure consistent sign\n            if self.alphas_[max_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # Flip eigenvectors' sign to enforce deterministic output\n        indices = np.argsort(self.lambdas_)[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.n_components is not None:\n            self.alphas_ = self.alphas_[:, :self.n_components]\n            self.lambdas_ = self.lambdas_[:self.n_components]\n\n        # Standardize eigenvector signs for deterministic output\n        for i in range(self.alphas_.shape[1]):\n            # Find the component with the largest absolute value\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            # If it's negative, flip the entire eigenvector\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n",
      "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        # Ensure the component with largest absolute value is positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # Sort eigenvectors in decreasing order of eigenvalues\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        # Make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        # Make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        # Sort eigenvectors and eigenvalues in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        # Make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n"
    ],
    "Location 4 (/root/hy/projects_3/scikit-learn/sklearn/decomposition/kernel_pca.py lines 296-299)": [
      "\n        # Sort eigenvectors and eigenvalues in descending order of eigenvalues\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # Standardize eigenvector signs for consistency\n        # Make the component with largest absolute value positive\n        for i in range(self.alphas_.shape[1]):\n            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n            if self.alphas_[max_abs_idx, i] < 0:\n                self.alphas_[:, i] *= -1\n\n        # Remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is not None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
      "\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n",
      "\n        # Flip eigenvectors' sign to enforce deterministic output\n        max_abs_rows = np.argmax(np.abs(self.alphas_), axis=0)\n        signs = np.sign(self.alphas_[max_abs_rows, range(self.alphas_.shape[1])])\n        self.alphas_ *= signs\n\n        return K.dot(self.alphas_ * np.sqrt(self.lambdas_))\n",
      "\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n",
      "\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n",
      "\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n",
      "\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n",
      "\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..3bee2fbc7 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,6 +197,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n+\n         if eigen_solver == 'dense':\n             self.lambdas_, self.alphas_ = linalg.eigh(\n                 K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n@@ -210,6 +211,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n                                                 maxiter=self.max_iter,\n                                                 v0=v0)\n \n+\n         # sort eigenvectors in descending order\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n@@ -220,6 +222,12 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+        # standardize eigenvector signs to ensure consistency\n+        # make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n@@ -273,6 +281,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         self.fit(X, **params)\n \n         X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n@@ -293,6 +302,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "variant_0": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..0fd14b509 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,29 +197,42 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n-        if eigen_solver == 'dense':\n-            self.lambdas_, self.alphas_ = linalg.eigh(\n-                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n-        elif eigen_solver == 'arpack':\n-            random_state = check_random_state(self.random_state)\n-            # initialize with [-1,1] as in ARPACK\n-            v0 = random_state.uniform(-1, 1, K.shape[0])\n-            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n-                                                which=\"LA\",\n-                                                tol=self.tol,\n-                                                maxiter=self.max_iter,\n-                                                v0=v0)\n \n         # sort eigenvectors in descending order\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the component with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If that component is negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # standardize eigenvector signs for consistency\n+        # make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n@@ -273,14 +286,23 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n-        if self.fit_inverse_transform:\n-            self._fit_inverse_transform(X_transformed, X)\n+        # Standardize eigenvector signs for consistency\n+        # Make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n-        return X_transformed\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def transform(self, X):\n         \"\"\"Transform X.\n@@ -293,6 +315,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "variant_1": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..dc19b23ae 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -188,6 +188,16 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             n_components = min(K.shape[0], self.n_components)\n \n+        # compute eigenvectors\n+        if self.eigen_solver == 'auto':\n+            if K.shape[0] > 200 and n_components < 10:\n+                eigen_solver = 'arpack'\n+            else:\n+                eigen_solver = 'dense'\n+        else:\n+            eigen_solver = self.eigen_solver\n+\n+\n         # compute eigenvectors\n         if self.eigen_solver == 'auto':\n             if K.shape[0] > 200 and n_components < 10:\n@@ -215,11 +225,34 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # standardize eigenvector signs for consistency\n+        # make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n@@ -273,14 +306,23 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n-        if self.fit_inverse_transform:\n-            self._fit_inverse_transform(X_transformed, X)\n+        # Standardize eigenvector signs for consistency\n+        # Make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n-        return X_transformed\n+        # Remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def transform(self, X):\n         \"\"\"Transform X.\n@@ -293,10 +335,23 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Sort eigenvectors and eigenvalues in descending order of eigenvalues\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # Standardize eigenvector signs for consistency\n+        # Make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_2": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..8efb75c35 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,29 +197,41 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n-        if eigen_solver == 'dense':\n-            self.lambdas_, self.alphas_ = linalg.eigh(\n-                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n-        elif eigen_solver == 'arpack':\n-            random_state = check_random_state(self.random_state)\n-            # initialize with [-1,1] as in ARPACK\n-            v0 = random_state.uniform(-1, 1, K.shape[0])\n-            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n-                                                which=\"LA\",\n-                                                tol=self.tol,\n-                                                maxiter=self.max_iter,\n-                                                v0=v0)\n \n         # sort eigenvectors in descending order\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # standardize eigenvector signs for consistency\n+        # ensure the component with largest absolute value is positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # standardize eigenvector signs for consistency\n+        # make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n@@ -273,14 +285,27 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors by eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n-        if self.fit_inverse_transform:\n-            self._fit_inverse_transform(X_transformed, X)\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n-        return X_transformed\n+        # Ensure sign consistency by making the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # If n_components is specified, keep only the top n_components\n+        if self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, :self.n_components]\n+            self.lambdas_ = self.lambdas_[:self.n_components]\n \n     def transform(self, X):\n         \"\"\"Transform X.\n@@ -293,6 +318,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "variant_3": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..00be540ac 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,29 +197,41 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n-        if eigen_solver == 'dense':\n-            self.lambdas_, self.alphas_ = linalg.eigh(\n-                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n-        elif eigen_solver == 'arpack':\n-            random_state = check_random_state(self.random_state)\n-            # initialize with [-1,1] as in ARPACK\n-            v0 = random_state.uniform(-1, 1, K.shape[0])\n-            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n-                                                which=\"LA\",\n-                                                tol=self.tol,\n-                                                maxiter=self.max_iter,\n-                                                v0=v0)\n \n         # sort eigenvectors in descending order\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # standardize eigenvector signs for consistency\n+        # ensure the component with largest absolute value is positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # standardize eigenvector signs for consistency\n+        # make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n@@ -273,14 +285,24 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n-        if self.fit_inverse_transform:\n-            self._fit_inverse_transform(X_transformed, X)\n+        # Standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the component with the largest absolute value\n+            max_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # Make it positive to ensure consistent sign\n+            if self.alphas_[max_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n-        return X_transformed\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def transform(self, X):\n         \"\"\"Transform X.\n@@ -293,10 +315,13 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        check_is_fitted(self, 'X_fit_')\n \n-        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        # Flip eigenvectors' sign to enforce deterministic output\n+        max_abs_rows = np.argmax(np.abs(self.alphas_), axis=0)\n+        signs = np.sign(self.alphas_[max_abs_rows, range(self.alphas_.shape[1])])\n+        self.alphas_ *= signs\n+\n+        return K.dot(self.alphas_ * np.sqrt(self.lambdas_))\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n",
    "variant_4": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..c11bef923 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,29 +197,42 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n-        if eigen_solver == 'dense':\n-            self.lambdas_, self.alphas_ = linalg.eigh(\n-                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n-        elif eigen_solver == 'arpack':\n-            random_state = check_random_state(self.random_state)\n-            # initialize with [-1,1] as in ARPACK\n-            v0 = random_state.uniform(-1, 1, K.shape[0])\n-            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n-                                                which=\"LA\",\n-                                                tol=self.tol,\n-                                                maxiter=self.max_iter,\n-                                                v0=v0)\n \n         # sort eigenvectors in descending order\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the component with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If that component is negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # standardize eigenvector signs for consistency\n+        # ensure the component with largest absolute value is positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n@@ -273,14 +286,24 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Flip eigenvectors' sign to enforce deterministic output\n+        indices = np.argsort(self.lambdas_)[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n-        if self.fit_inverse_transform:\n-            self._fit_inverse_transform(X_transformed, X)\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, :self.n_components]\n+            self.lambdas_ = self.lambdas_[:self.n_components]\n \n-        return X_transformed\n+        # Standardize eigenvector signs for deterministic output\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the component with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If it's negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n     def transform(self, X):\n         \"\"\"Transform X.\n@@ -293,6 +316,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "variant_5": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..c203e1221 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,29 +197,43 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n-        if eigen_solver == 'dense':\n-            self.lambdas_, self.alphas_ = linalg.eigh(\n-                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n-        elif eigen_solver == 'arpack':\n-            random_state = check_random_state(self.random_state)\n-            # initialize with [-1,1] as in ARPACK\n-            v0 = random_state.uniform(-1, 1, K.shape[0])\n-            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n-                                                which=\"LA\",\n-                                                tol=self.tol,\n-                                                maxiter=self.max_iter,\n-                                                v0=v0)\n \n         # sort eigenvectors in descending order\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the component with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If it's negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # find the component with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # if that component is negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n@@ -273,14 +287,23 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n-        if self.fit_inverse_transform:\n-            self._fit_inverse_transform(X_transformed, X)\n+        # Standardize eigenvector signs for consistency\n+        # Ensure the component with largest absolute value is positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n-        return X_transformed\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def transform(self, X):\n         \"\"\"Transform X.\n@@ -293,6 +316,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "variant_6": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..1ee3c9ef9 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,29 +197,41 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n-        if eigen_solver == 'dense':\n-            self.lambdas_, self.alphas_ = linalg.eigh(\n-                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n-        elif eigen_solver == 'arpack':\n-            random_state = check_random_state(self.random_state)\n-            # initialize with [-1,1] as in ARPACK\n-            v0 = random_state.uniform(-1, 1, K.shape[0])\n-            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n-                                                which=\"LA\",\n-                                                tol=self.tol,\n-                                                maxiter=self.max_iter,\n-                                                v0=v0)\n \n         # sort eigenvectors in descending order\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # standardize eigenvector signs for consistency\n+        # ensure the component with largest absolute value is positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # standardize eigenvector signs for consistency\n+        # make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n@@ -273,14 +285,23 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors in decreasing order of eigenvalues\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n-        if self.fit_inverse_transform:\n-            self._fit_inverse_transform(X_transformed, X)\n+        # Standardize eigenvector signs for consistency\n+        # Make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n-        return X_transformed\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def transform(self, X):\n         \"\"\"Transform X.\n@@ -293,6 +314,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "variant_7": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..e9bd3eb60 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,29 +197,43 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n-        if eigen_solver == 'dense':\n-            self.lambdas_, self.alphas_ = linalg.eigh(\n-                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n-        elif eigen_solver == 'arpack':\n-            random_state = check_random_state(self.random_state)\n-            # initialize with [-1,1] as in ARPACK\n-            v0 = random_state.uniform(-1, 1, K.shape[0])\n-            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n-                                                which=\"LA\",\n-                                                tol=self.tol,\n-                                                maxiter=self.max_iter,\n-                                                v0=v0)\n \n         # sort eigenvectors in descending order\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the component with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If that component is negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # find the component with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # if it's negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n@@ -273,14 +287,23 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n-        if self.fit_inverse_transform:\n-            self._fit_inverse_transform(X_transformed, X)\n+        # Standardize eigenvector signs for consistency\n+        # Make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n-        return X_transformed\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def transform(self, X):\n         \"\"\"Transform X.\n@@ -293,6 +316,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "variant_8": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..21d658024 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -197,29 +197,42 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         else:\n             eigen_solver = self.eigen_solver\n \n-        if eigen_solver == 'dense':\n-            self.lambdas_, self.alphas_ = linalg.eigh(\n-                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n-        elif eigen_solver == 'arpack':\n-            random_state = check_random_state(self.random_state)\n-            # initialize with [-1,1] as in ARPACK\n-            v0 = random_state.uniform(-1, 1, K.shape[0])\n-            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n-                                                which=\"LA\",\n-                                                tol=self.tol,\n-                                                maxiter=self.max_iter,\n-                                                v0=v0)\n \n         # sort eigenvectors in descending order\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n+        # standardize eigenvector signs for consistency\n+        for i in range(self.alphas_.shape[1]):\n+            # Find the component with the largest absolute value\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            # If it's negative, flip the entire eigenvector\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n         # remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # standardize eigenvector signs for consistency\n+        # ensure the component with largest absolute value is positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n         return K\n \n     def _fit_inverse_transform(self, X_transformed, X):\n@@ -273,14 +286,23 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n-        self.fit(X, **params)\n \n-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n+        # Sort eigenvectors and eigenvalues in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n \n-        if self.fit_inverse_transform:\n-            self._fit_inverse_transform(X_transformed, X)\n+        # Standardize eigenvector signs for consistency\n+        # Make the component with largest absolute value positive\n+        for i in range(self.alphas_.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(self.alphas_[:, i]))\n+            if self.alphas_[max_abs_idx, i] < 0:\n+                self.alphas_[:, i] *= -1\n \n-        return X_transformed\n+        # Remove eigenvectors with a zero eigenvalue (null space) if required\n+        if self.remove_zero_eig or self.n_components is not None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n \n     def transform(self, X):\n         \"\"\"Transform X.\n@@ -293,6 +315,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         -------\n         X_new : array-like, shape (n_samples, n_components)\n         \"\"\"\n+\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n"
  }
}