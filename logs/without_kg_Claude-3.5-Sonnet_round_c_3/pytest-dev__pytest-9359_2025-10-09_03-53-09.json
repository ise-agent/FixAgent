{
  "zero_patches": {
    "Location 1 (/root/hy/projects_3/pytest/src/_pytest/assertion/rewrite.py lines 605-607)": "\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
    "Location 2 (/root/hy/projects_3/pytest/src/_pytest/assertion/rewrite.py lines 603-604)": "\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
    "Location 3 (/root/hy/projects_3/pytest/src/_pytest/assertion/rewrite.py lines 576-580)": "\ntokens = tokenize.tokenize(io.BytesIO(src).readline)\nfor tp, source, (lineno, offset), _, line in tokens:\n    if tp == tokenize.NAME and source == \"assert\":\n        assert_lineno = lineno\n    elif assert_lineno is not None:\n",
    "Location 4 (/root/hy/projects_3/pytest/src/_pytest/assertion/rewrite.py lines 587-589)": "\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n",
    "Location 5 (/root/hy/projects_3/pytest/src/_pytest/assertion/rewrite.py lines 558-566)": "\n@functools.lru_cache(maxsize=1)\ndef _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n    ret: Dict[int, str] = {}\n\n    depth = 0\n    lines: List[str] = []\n    assert_lineno: Optional[int] = None\n    seen_lines: Set[int] = set()\n\n    def _write_and_reset() -> None:\n        nonlocal depth, lines, assert_lineno, seen_lines\n        assert assert_lineno is not None\n        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n        depth = 0\n        lines = []\n        assert_lineno = None\n        seen_lines = set()\n\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            if lines:\n                _write_and_reset()\n            assert_lineno = lineno\n            depth = 0\n            seen_lines = set()\n\n        if assert_lineno is not None:\n            if tp == tokenize.OP:\n                if source in \"([{\":\n                    depth += 1\n                elif source in \")]}\":\n                    depth -= 1\n                elif source in {\";\", \"=\"} and depth == 0:\n                    _write_and_reset()\n                    continue\n            elif tp in {tokenize.NEWLINE, tokenize.NL}:\n                if depth == 0:\n                    _write_and_reset()\n                    continue\n            elif tp in {tokenize.COMMENT, tokenize.TYPE_COMMENT}:\n                continue\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n                continue\n\n            # Collect line if we haven't seen it and it's not a decorator\n            if lineno == assert_lineno:\n                if not lines:\n                    lines.append(line[offset:])\n                    seen_lines.add(lineno)\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n\n    if lines:\n        _write_and_reset()\n    return ret\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_3/pytest/src/_pytest/assertion/rewrite.py lines 605-607)": [
      "\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n"
    ],
    "Location 2 (/root/hy/projects_3/pytest/src/_pytest/assertion/rewrite.py lines 603-604)": [
      "\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n",
      "\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n"
    ],
    "Location 3 (/root/hy/projects_3/pytest/src/_pytest/assertion/rewrite.py lines 576-580)": [
      "\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            assert_lineno = lineno\n        elif assert_lineno is not None:\n",
      "\ntokens = tokenize.tokenize(io.BytesIO(src).readline)\nfor tp, source, (lineno, offset), _, line in tokens:\n    if tp == tokenize.NAME and source == \"assert\":\n        assert_lineno = lineno\n    elif assert_lineno is not None:\n",
      "\ntokens = tokenize.tokenize(io.BytesIO(src).readline)\nfor tp, source, (lineno, offset), _, line in tokens:\n    if tp == tokenize.NAME and source == \"assert\":\n        assert_lineno = lineno\n    elif assert_lineno is not None:\n",
      "\ntokens = tokenize.tokenize(io.BytesIO(src).readline)\nfor tp, source, (lineno, offset), _, line in tokens:\n    if tp == tokenize.NAME and source == \"assert\":\n        assert_lineno = lineno\n    elif assert_lineno is not None:\n",
      "\ntokens = tokenize.tokenize(io.BytesIO(src).readline)\nfor tp, source, (lineno, offset), _, line in tokens:\n    if tp == tokenize.NAME and source == \"assert\":\n        assert_lineno = lineno\n    elif assert_lineno is not None:\n",
      "\ntokens = tokenize.tokenize(io.BytesIO(src).readline)\nfor tp, source, (lineno, offset), _, line in tokens:\n    if tp == tokenize.NAME and source == \"assert\":\n        assert_lineno = lineno\n    elif assert_lineno is not None:\n",
      "\ntokens = tokenize.tokenize(io.BytesIO(src).readline)\nfor tp, source, (lineno, offset), _, line in tokens:\n    if tp == tokenize.NAME and source == \"assert\":\n        assert_lineno = lineno\n    elif assert_lineno is not None:\n",
      "\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            assert_lineno = lineno\n        elif assert_lineno is not None:\n"
    ],
    "Location 4 (/root/hy/projects_3/pytest/src/_pytest/assertion/rewrite.py lines 587-589)": [
      "\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n",
      "\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n",
      "\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n",
      "\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n",
      "\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n",
      "\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n",
      "\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n",
      "\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n"
    ],
    "Location 5 (/root/hy/projects_3/pytest/src/_pytest/assertion/rewrite.py lines 558-566)": [
      "\n@functools.lru_cache(maxsize=1)\ndef _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n    ret: Dict[int, str] = {}\n\n    depth = 0\n    lines: List[str] = []\n    assert_lineno: Optional[int] = None\n    seen_lines: Set[int] = set()\n\n    def _write_and_reset() -> None:\n        nonlocal depth, lines, assert_lineno, seen_lines\n        assert assert_lineno is not None\n        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n        depth = 0\n        lines = []\n        assert_lineno = None\n        seen_lines = set()\n\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            if lines:\n                _write_and_reset()\n            assert_lineno = lineno\n            depth = 0\n\n        # if we have an assert, start recording lines\n        if assert_lineno is not None:\n            if tp in {tokenize.OP} and source in \"()[]{}\":\n                if source in \"([{\":\n                    depth += 1\n                elif source in \")]}\":\n                    depth -= 1\n\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n\n    return ret\n",
      "\n@functools.lru_cache(maxsize=1)\ndef _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n    ret: Dict[int, str] = {}\n\n    depth = 0\n    lines: List[str] = []\n    assert_lineno: Optional[int] = None\n    seen_lines: Set[int] = set()\n\n    def _write_and_reset() -> None:\n        nonlocal depth, lines, assert_lineno, seen_lines\n        assert assert_lineno is not None\n        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n        depth = 0\n        lines = []\n        assert_lineno = None\n        seen_lines = set()\n\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            if lines:\n                _write_and_reset()\n            assert_lineno = lineno\n            depth = 0\n\n        # Collect lines for the current assertion\n        if assert_lineno is not None:\n            if tp == tokenize.OP and source in \"([\":\n                depth += 1\n            elif tp == tokenize.OP and source in \")]\":\n                depth -= 1\n\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n            elif tp in {tokenize.NEWLINE, tokenize.NL}:\n                if depth <= 0:\n                    _write_and_reset()\n                    continue\n            elif tp == tokenize.NEWLINE:\n                _write_and_reset()\n            elif tp == tokenize.ENDMARKER:\n                _write_and_reset()\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n\n    return ret\n",
      "\n@functools.lru_cache(maxsize=1)\ndef _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n    ret: Dict[int, str] = {}\n\n    depth = 0\n    lines: List[str] = []\n    assert_lineno: Optional[int] = None\n    seen_lines: Set[int] = set()\n\n    def _write_and_reset() -> None:\n        nonlocal depth, lines, assert_lineno, seen_lines\n        assert assert_lineno is not None\n        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n        depth = 0\n        lines = []\n        assert_lineno = None\n        seen_lines = set()\n\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            if lines:\n                _write_and_reset()\n            assert_lineno = lineno\n            depth = 0\n            lines = []\n            seen_lines = set()\n\n        if assert_lineno is not None:\n            if tp == tokenize.OP:\n                if source in \"([{\":\n                    depth += 1\n                elif source in \")]}\":\n                    depth -= 1\n                    if depth < 0:\n                        _write_and_reset()\n                        continue\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                if depth == 0:\n                    _write_and_reset()\n                    continue\n\n            # Capture assertion lines\n            if lineno == assert_lineno:\n                if not lines:\n                    lines.append(line[offset:])\n                    seen_lines.add(lineno)\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n\n    return ret\n",
      "\n@functools.lru_cache(maxsize=1)\ndef _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n    ret: Dict[int, str] = {}\n\n    depth = 0\n    lines: List[str] = []\n    assert_lineno: Optional[int] = None\n    seen_lines: Set[int] = set()\n\n    def _write_and_reset() -> None:\n        nonlocal depth, lines, assert_lineno, seen_lines\n        assert assert_lineno is not None\n        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n        depth = 0\n        lines = []\n        assert_lineno = None\n        seen_lines = set()\n\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            if lines:\n                _write_and_reset()\n            assert_lineno = lineno\n            depth = 0\n            lines = []\n            seen_lines = set()\n\n        if assert_lineno is not None:\n            if tp == tokenize.OP:\n                if source in \"([\":\n                    depth += 1\n                elif source in \")]\":\n                    depth -= 1\n\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n            elif depth > 0:\n                if lineno not in seen_lines:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n\n    if lines:\n        _write_and_reset()\n    return ret\n",
      "\n@functools.lru_cache(maxsize=1)\ndef _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n    ret: Dict[int, str] = {}\n\n    depth = 0\n    lines: List[str] = []\n    assert_lineno: Optional[int] = None\n    seen_lines: Set[int] = set()\n\n    def _write_and_reset() -> None:\n        nonlocal depth, lines, assert_lineno, seen_lines\n        assert assert_lineno is not None\n        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n        depth = 0\n        lines = []\n        assert_lineno = None\n        seen_lines = set()\n\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            if lines:\n                _write_and_reset()\n            assert_lineno = lineno\n\n        elif assert_lineno is not None:\n            if tp == tokenize.OP and source in \"([\":\n                depth += 1\n\n            elif tp == tokenize.OP and source in \")]\":\n                depth -= 1\n\n            if not lines:\n                lines.append(line[offset:])\n                seen_lines.add(lineno)\n\n            elif tp in {tokenize.NEWLINE, tokenize.NL}:\n                if depth <= 0:\n                    _write_and_reset()\n\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                _write_and_reset()\n\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n\n    if lines:\n        _write_and_reset()\n    return ret\n",
      "\n@functools.lru_cache(maxsize=1)\ndef _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n    ret: Dict[int, str] = {}\n\n    depth = 0\n    lines: List[str] = []\n    assert_lineno: Optional[int] = None\n    seen_lines: Set[int] = set()\n\n    def _write_and_reset() -> None:\n        nonlocal depth, lines, assert_lineno, seen_lines\n        assert assert_lineno is not None\n        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n        depth = 0\n        lines = []\n        assert_lineno = None\n        seen_lines = set()\n\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            if lines:\n                _write_and_reset()\n            assert_lineno = lineno\n            depth = 0\n\n        if assert_lineno is not None:\n            if tp == tokenize.OP:\n                if source in \"([{\":\n                    depth += 1\n                elif source in \")]}\":\n                    depth -= 1\n                    if depth < 0:\n                        _write_and_reset()\n                        continue\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                if depth == 0:\n                    _write_and_reset()\n                    continue\n            elif tp == tokenize.NL:\n                if not lines:\n                    lines.append(line[offset:])\n                    seen_lines.add(lineno)\n                elif lines and lineno not in seen_lines:\n                    # Check if this line is a decorator (starts with @)\n                    stripped_line = line.lstrip()\n                    if stripped_line.startswith('@'):\n                        # Stop collecting lines when we hit a decorator\n                        _write_and_reset()\n                    else:\n                        lines.append(line)\n                        seen_lines.add(lineno)\n\n    return ret\n",
      "\n@functools.lru_cache(maxsize=1)\ndef _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n    ret: Dict[int, str] = {}\n\n    depth = 0\n    lines: List[str] = []\n    assert_lineno: Optional[int] = None\n    seen_lines: Set[int] = set()\n\n    def _write_and_reset() -> None:\n        nonlocal depth, lines, assert_lineno, seen_lines\n        assert assert_lineno is not None\n        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n        depth = 0\n        lines = []\n        assert_lineno = None\n        seen_lines = set()\n\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            if lines:\n                _write_and_reset()\n            assert_lineno = lineno\n        elif assert_lineno is not None:\n            if tp == tokenize.OP and source in \"([{\":\n                depth += 1\n            elif tp == tokenize.OP and source in \")]}\":\n                depth -= 1\n            elif tp in {tokenize.NEWLINE, tokenize.NL}:\n                if depth <= 0:\n                    _write_and_reset()\n                    continue\n            elif tp == tokenize.ENDMARKER:\n                if lines:\n                    _write_and_reset()\n                break\n\n            # Capture assertion lines, but stop if we encounter a decorator\n            if tp != tokenize.COMMENT:\n                if assert_lineno == lineno:\n                    if not lines:\n                        lines.append(line[offset:])\n                        seen_lines.add(lineno)\n                elif lines and lineno not in seen_lines:\n                    # Check if this line is a decorator (starts with @)\n                    stripped_line = line.lstrip()\n                    if stripped_line.startswith('@'):\n                        # Stop collecting lines when we hit a decorator\n                        _write_and_reset()\n                    else:\n                        lines.append(line)\n                        seen_lines.add(lineno)\n\n    return ret\n",
      "\n@functools.lru_cache(maxsize=1)\ndef _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n    \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n    ret: Dict[int, str] = {}\n\n    depth = 0\n    lines: List[str] = []\n    assert_lineno: Optional[int] = None\n    seen_lines: Set[int] = set()\n\n    def _write_and_reset() -> None:\n        nonlocal depth, lines, assert_lineno, seen_lines\n        assert assert_lineno is not None\n        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n        depth = 0\n        lines = []\n        assert_lineno = None\n        seen_lines = set()\n\n    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n    for tp, source, (lineno, offset), _, line in tokens:\n        if tp == tokenize.NAME and source == \"assert\":\n            if lines:\n                _write_and_reset()\n            assert_lineno = lineno\n            depth = 0\n            lines = []\n            seen_lines = set()\n\n        if assert_lineno is not None:\n            if tp == tokenize.OP:\n                if source in \"([{\":\n                    depth += 1\n                elif source in \")]}\":\n                    depth -= 1\n                    if depth < 0:\n                        _write_and_reset()\n                        continue\n            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                if depth == 0:\n                    _write_and_reset()\n                    continue\n            elif tp == tokenize.COMMENT:\n                continue\n\n            if lineno == assert_lineno:\n                if not lines:\n                    lines.append(line[offset:])\n                    seen_lines.add(lineno)\n            elif lines and lineno not in seen_lines:\n                # Check if this line is a decorator (starts with @)\n                stripped_line = line.lstrip()\n                if stripped_line.startswith('@'):\n                    # Stop collecting lines when we hit a decorator\n                    _write_and_reset()\n                else:\n                    lines.append(line)\n                    seen_lines.add(lineno)\n\n    return ret\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\nindex 456681ab2..7175e5638 100644\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -555,6 +555,7 @@ def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n \n \n @functools.lru_cache(maxsize=1)\n+\n def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n     ret: Dict[int, str] = {}\n@@ -563,7 +564,6 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     lines: List[str] = []\n     assert_lineno: Optional[int] = None\n     seen_lines: Set[int] = set()\n-\n     def _write_and_reset() -> None:\n         nonlocal depth, lines, assert_lineno, seen_lines\n         assert assert_lineno is not None\n@@ -573,6 +573,7 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n         assert_lineno = None\n         seen_lines = set()\n \n+\n     tokens = tokenize.tokenize(io.BytesIO(src).readline)\n     for tp, source, (lineno, offset), _, line in tokens:\n         if tp == tokenize.NAME and source == \"assert\":\n@@ -584,6 +585,7 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n             elif tp == tokenize.OP and source in \")]}\":\n                 depth -= 1\n \n+\n             if not lines:\n                 lines.append(line[offset:])\n                 seen_lines.add(lineno)\n@@ -600,11 +602,19 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n                 else:\n                     lines.append(line[:offset])\n                 _write_and_reset()\n+\n             elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                 _write_and_reset()\n+\n             elif lines and lineno not in seen_lines:\n-                lines.append(line)\n-                seen_lines.add(lineno)\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n \n     return ret\n \n",
    "variant_0": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\nindex 456681ab2..bddd7acc7 100644\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -554,6 +554,8 @@ def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n         yield from traverse_node(child)\n \n \n+@functools.lru_cache(maxsize=1)\n+\n @functools.lru_cache(maxsize=1)\n def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n@@ -576,14 +578,71 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     tokens = tokenize.tokenize(io.BytesIO(src).readline)\n     for tp, source, (lineno, offset), _, line in tokens:\n         if tp == tokenize.NAME and source == \"assert\":\n+            if lines:\n+                _write_and_reset()\n             assert_lineno = lineno\n-        elif assert_lineno is not None:\n+            depth = 0\n+            seen_lines = set()\n+\n+        if assert_lineno is not None:\n+            if tp == tokenize.OP:\n+                if source in \"([{\":\n+                    depth += 1\n+                elif source in \")]}\":\n+                    depth -= 1\n+                elif source in {\";\", \"=\"} and depth == 0:\n+                    _write_and_reset()\n+                    continue\n+            elif tp in {tokenize.NEWLINE, tokenize.NL}:\n+                if depth == 0:\n+                    _write_and_reset()\n+                    continue\n+            elif tp in {tokenize.COMMENT, tokenize.TYPE_COMMENT}:\n+                continue\n+            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n+                _write_and_reset()\n+                continue\n+\n+            # Collect line if we haven't seen it and it's not a decorator\n+            if lineno == assert_lineno:\n+                if not lines:\n+                    lines.append(line[offset:])\n+                    seen_lines.add(lineno)\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+    if lines:\n+        _write_and_reset()\n+    return ret\n+    def _write_and_reset() -> None:\n+        nonlocal depth, lines, assert_lineno, seen_lines\n+        assert assert_lineno is not None\n+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n+        depth = 0\n+        lines = []\n+        assert_lineno = None\n+        seen_lines = set()\n+\n+\n+tokens = tokenize.tokenize(io.BytesIO(src).readline)\n+for tp, source, (lineno, offset), _, line in tokens:\n+    if tp == tokenize.NAME and source == \"assert\":\n+        assert_lineno = lineno\n+    elif assert_lineno is not None:\n             # keep track of depth for the assert-message `,` lookup\n             if tp == tokenize.OP and source in \"([{\":\n                 depth += 1\n             elif tp == tokenize.OP and source in \")]}\":\n                 depth -= 1\n \n+\n             if not lines:\n                 lines.append(line[offset:])\n                 seen_lines.add(lineno)\n@@ -600,11 +659,28 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n                 else:\n                     lines.append(line[:offset])\n                 _write_and_reset()\n+\n             elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                 _write_and_reset()\n             elif lines and lineno not in seen_lines:\n-                lines.append(line)\n-                seen_lines.add(lineno)\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n \n     return ret\n \n",
    "variant_1": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\nindex 456681ab2..61e3a00fd 100644\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -554,6 +554,8 @@ def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n         yield from traverse_node(child)\n \n \n+@functools.lru_cache(maxsize=1)\n+\n @functools.lru_cache(maxsize=1)\n def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n@@ -573,6 +575,48 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n         assert_lineno = None\n         seen_lines = set()\n \n+    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n+    for tp, source, (lineno, offset), _, line in tokens:\n+        if tp == tokenize.NAME and source == \"assert\":\n+            if lines:\n+                _write_and_reset()\n+            assert_lineno = lineno\n+            depth = 0\n+\n+        # if we have an assert, start recording lines\n+        if assert_lineno is not None:\n+            if tp in {tokenize.OP} and source in \"()[]{}\":\n+                if source in \"([{\":\n+                    depth += 1\n+                elif source in \")]}\":\n+                    depth -= 1\n+\n+            if not lines:\n+                lines.append(line[offset:])\n+                seen_lines.add(lineno)\n+            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n+                _write_and_reset()\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+    return ret\n+    def _write_and_reset() -> None:\n+        nonlocal depth, lines, assert_lineno, seen_lines\n+        assert assert_lineno is not None\n+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n+        depth = 0\n+        lines = []\n+        assert_lineno = None\n+        seen_lines = set()\n+\n+\n     tokens = tokenize.tokenize(io.BytesIO(src).readline)\n     for tp, source, (lineno, offset), _, line in tokens:\n         if tp == tokenize.NAME and source == \"assert\":\n@@ -584,6 +628,7 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n             elif tp == tokenize.OP and source in \")]}\":\n                 depth -= 1\n \n+\n             if not lines:\n                 lines.append(line[offset:])\n                 seen_lines.add(lineno)\n@@ -600,11 +645,28 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n                 else:\n                     lines.append(line[:offset])\n                 _write_and_reset()\n+\n             elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                 _write_and_reset()\n             elif lines and lineno not in seen_lines:\n-                lines.append(line)\n-                seen_lines.add(lineno)\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n \n     return ret\n \n",
    "variant_2": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\nindex 456681ab2..2ff528d86 100644\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -554,6 +554,8 @@ def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n         yield from traverse_node(child)\n \n \n+@functools.lru_cache(maxsize=1)\n+\n @functools.lru_cache(maxsize=1)\n def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n@@ -576,14 +578,62 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     tokens = tokenize.tokenize(io.BytesIO(src).readline)\n     for tp, source, (lineno, offset), _, line in tokens:\n         if tp == tokenize.NAME and source == \"assert\":\n+            if lines:\n+                _write_and_reset()\n             assert_lineno = lineno\n-        elif assert_lineno is not None:\n+            depth = 0\n+\n+        # Collect lines for the current assertion\n+        if assert_lineno is not None:\n+            if tp == tokenize.OP and source in \"([\":\n+                depth += 1\n+            elif tp == tokenize.OP and source in \")]\":\n+                depth -= 1\n+\n+            if not lines:\n+                lines.append(line[offset:])\n+                seen_lines.add(lineno)\n+            elif tp in {tokenize.NEWLINE, tokenize.NL}:\n+                if depth <= 0:\n+                    _write_and_reset()\n+                    continue\n+            elif tp == tokenize.NEWLINE:\n+                _write_and_reset()\n+            elif tp == tokenize.ENDMARKER:\n+                _write_and_reset()\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+    return ret\n+    def _write_and_reset() -> None:\n+        nonlocal depth, lines, assert_lineno, seen_lines\n+        assert assert_lineno is not None\n+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n+        depth = 0\n+        lines = []\n+        assert_lineno = None\n+        seen_lines = set()\n+\n+\n+tokens = tokenize.tokenize(io.BytesIO(src).readline)\n+for tp, source, (lineno, offset), _, line in tokens:\n+    if tp == tokenize.NAME and source == \"assert\":\n+        assert_lineno = lineno\n+    elif assert_lineno is not None:\n             # keep track of depth for the assert-message `,` lookup\n             if tp == tokenize.OP and source in \"([{\":\n                 depth += 1\n             elif tp == tokenize.OP and source in \")]}\":\n                 depth -= 1\n \n+\n             if not lines:\n                 lines.append(line[offset:])\n                 seen_lines.add(lineno)\n@@ -600,11 +650,28 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n                 else:\n                     lines.append(line[:offset])\n                 _write_and_reset()\n+\n             elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                 _write_and_reset()\n             elif lines and lineno not in seen_lines:\n-                lines.append(line)\n-                seen_lines.add(lineno)\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n \n     return ret\n \n",
    "variant_3": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\nindex 456681ab2..2821f161e 100644\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -554,6 +554,8 @@ def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n         yield from traverse_node(child)\n \n \n+@functools.lru_cache(maxsize=1)\n+\n @functools.lru_cache(maxsize=1)\n def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n@@ -576,14 +578,65 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     tokens = tokenize.tokenize(io.BytesIO(src).readline)\n     for tp, source, (lineno, offset), _, line in tokens:\n         if tp == tokenize.NAME and source == \"assert\":\n+            if lines:\n+                _write_and_reset()\n             assert_lineno = lineno\n-        elif assert_lineno is not None:\n+            depth = 0\n+            lines = []\n+            seen_lines = set()\n+\n+        if assert_lineno is not None:\n+            if tp == tokenize.OP:\n+                if source in \"([{\":\n+                    depth += 1\n+                elif source in \")]}\":\n+                    depth -= 1\n+                    if depth < 0:\n+                        _write_and_reset()\n+                        continue\n+            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n+                if depth == 0:\n+                    _write_and_reset()\n+                    continue\n+\n+            # Capture assertion lines\n+            if lineno == assert_lineno:\n+                if not lines:\n+                    lines.append(line[offset:])\n+                    seen_lines.add(lineno)\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+    return ret\n+    def _write_and_reset() -> None:\n+        nonlocal depth, lines, assert_lineno, seen_lines\n+        assert assert_lineno is not None\n+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n+        depth = 0\n+        lines = []\n+        assert_lineno = None\n+        seen_lines = set()\n+\n+\n+tokens = tokenize.tokenize(io.BytesIO(src).readline)\n+for tp, source, (lineno, offset), _, line in tokens:\n+    if tp == tokenize.NAME and source == \"assert\":\n+        assert_lineno = lineno\n+    elif assert_lineno is not None:\n             # keep track of depth for the assert-message `,` lookup\n             if tp == tokenize.OP and source in \"([{\":\n                 depth += 1\n             elif tp == tokenize.OP and source in \")]}\":\n                 depth -= 1\n \n+\n             if not lines:\n                 lines.append(line[offset:])\n                 seen_lines.add(lineno)\n@@ -600,11 +653,28 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n                 else:\n                     lines.append(line[:offset])\n                 _write_and_reset()\n+\n             elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                 _write_and_reset()\n             elif lines and lineno not in seen_lines:\n-                lines.append(line)\n-                seen_lines.add(lineno)\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n \n     return ret\n \n",
    "variant_4": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\nindex 456681ab2..921edd084 100644\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -554,6 +554,8 @@ def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n         yield from traverse_node(child)\n \n \n+@functools.lru_cache(maxsize=1)\n+\n @functools.lru_cache(maxsize=1)\n def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n@@ -576,14 +578,64 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     tokens = tokenize.tokenize(io.BytesIO(src).readline)\n     for tp, source, (lineno, offset), _, line in tokens:\n         if tp == tokenize.NAME and source == \"assert\":\n+            if lines:\n+                _write_and_reset()\n             assert_lineno = lineno\n-        elif assert_lineno is not None:\n+            depth = 0\n+            lines = []\n+            seen_lines = set()\n+\n+        if assert_lineno is not None:\n+            if tp == tokenize.OP:\n+                if source in \"([\":\n+                    depth += 1\n+                elif source in \")]\":\n+                    depth -= 1\n+\n+            if not lines:\n+                lines.append(line[offset:])\n+                seen_lines.add(lineno)\n+            elif depth > 0:\n+                if lineno not in seen_lines:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n+                _write_and_reset()\n+\n+    if lines:\n+        _write_and_reset()\n+    return ret\n+    def _write_and_reset() -> None:\n+        nonlocal depth, lines, assert_lineno, seen_lines\n+        assert assert_lineno is not None\n+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n+        depth = 0\n+        lines = []\n+        assert_lineno = None\n+        seen_lines = set()\n+\n+\n+tokens = tokenize.tokenize(io.BytesIO(src).readline)\n+for tp, source, (lineno, offset), _, line in tokens:\n+    if tp == tokenize.NAME and source == \"assert\":\n+        assert_lineno = lineno\n+    elif assert_lineno is not None:\n             # keep track of depth for the assert-message `,` lookup\n             if tp == tokenize.OP and source in \"([{\":\n                 depth += 1\n             elif tp == tokenize.OP and source in \")]}\":\n                 depth -= 1\n \n+\n             if not lines:\n                 lines.append(line[offset:])\n                 seen_lines.add(lineno)\n@@ -600,11 +652,28 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n                 else:\n                     lines.append(line[:offset])\n                 _write_and_reset()\n+\n             elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                 _write_and_reset()\n             elif lines and lineno not in seen_lines:\n-                lines.append(line)\n-                seen_lines.add(lineno)\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n \n     return ret\n \n",
    "variant_5": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\nindex 456681ab2..e3d3b8bd0 100644\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -554,6 +554,8 @@ def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n         yield from traverse_node(child)\n \n \n+@functools.lru_cache(maxsize=1)\n+\n @functools.lru_cache(maxsize=1)\n def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n@@ -576,14 +578,63 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     tokens = tokenize.tokenize(io.BytesIO(src).readline)\n     for tp, source, (lineno, offset), _, line in tokens:\n         if tp == tokenize.NAME and source == \"assert\":\n+            if lines:\n+                _write_and_reset()\n             assert_lineno = lineno\n+\n         elif assert_lineno is not None:\n+            if tp == tokenize.OP and source in \"([\":\n+                depth += 1\n+\n+            elif tp == tokenize.OP and source in \")]\":\n+                depth -= 1\n+\n+            if not lines:\n+                lines.append(line[offset:])\n+                seen_lines.add(lineno)\n+\n+            elif tp in {tokenize.NEWLINE, tokenize.NL}:\n+                if depth <= 0:\n+                    _write_and_reset()\n+\n+            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n+                _write_and_reset()\n+\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+    if lines:\n+        _write_and_reset()\n+    return ret\n+    def _write_and_reset() -> None:\n+        nonlocal depth, lines, assert_lineno, seen_lines\n+        assert assert_lineno is not None\n+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n+        depth = 0\n+        lines = []\n+        assert_lineno = None\n+        seen_lines = set()\n+\n+\n+tokens = tokenize.tokenize(io.BytesIO(src).readline)\n+for tp, source, (lineno, offset), _, line in tokens:\n+    if tp == tokenize.NAME and source == \"assert\":\n+        assert_lineno = lineno\n+    elif assert_lineno is not None:\n             # keep track of depth for the assert-message `,` lookup\n             if tp == tokenize.OP and source in \"([{\":\n                 depth += 1\n             elif tp == tokenize.OP and source in \")]}\":\n                 depth -= 1\n \n+\n             if not lines:\n                 lines.append(line[offset:])\n                 seen_lines.add(lineno)\n@@ -600,11 +651,28 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n                 else:\n                     lines.append(line[:offset])\n                 _write_and_reset()\n+\n             elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                 _write_and_reset()\n             elif lines and lineno not in seen_lines:\n-                lines.append(line)\n-                seen_lines.add(lineno)\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n \n     return ret\n \n",
    "variant_6": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\nindex 456681ab2..96a051cfa 100644\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -554,6 +554,8 @@ def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n         yield from traverse_node(child)\n \n \n+@functools.lru_cache(maxsize=1)\n+\n @functools.lru_cache(maxsize=1)\n def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n@@ -576,14 +578,61 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     tokens = tokenize.tokenize(io.BytesIO(src).readline)\n     for tp, source, (lineno, offset), _, line in tokens:\n         if tp == tokenize.NAME and source == \"assert\":\n+            if lines:\n+                _write_and_reset()\n             assert_lineno = lineno\n-        elif assert_lineno is not None:\n+            depth = 0\n+\n+        if assert_lineno is not None:\n+            if tp == tokenize.OP:\n+                if source in \"([{\":\n+                    depth += 1\n+                elif source in \")]}\":\n+                    depth -= 1\n+                    if depth < 0:\n+                        _write_and_reset()\n+                        continue\n+            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n+                if depth == 0:\n+                    _write_and_reset()\n+                    continue\n+            elif tp == tokenize.NL:\n+                if not lines:\n+                    lines.append(line[offset:])\n+                    seen_lines.add(lineno)\n+                elif lines and lineno not in seen_lines:\n+                    # Check if this line is a decorator (starts with @)\n+                    stripped_line = line.lstrip()\n+                    if stripped_line.startswith('@'):\n+                        # Stop collecting lines when we hit a decorator\n+                        _write_and_reset()\n+                    else:\n+                        lines.append(line)\n+                        seen_lines.add(lineno)\n+\n+    return ret\n+    def _write_and_reset() -> None:\n+        nonlocal depth, lines, assert_lineno, seen_lines\n+        assert assert_lineno is not None\n+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n+        depth = 0\n+        lines = []\n+        assert_lineno = None\n+        seen_lines = set()\n+\n+\n+tokens = tokenize.tokenize(io.BytesIO(src).readline)\n+for tp, source, (lineno, offset), _, line in tokens:\n+    if tp == tokenize.NAME and source == \"assert\":\n+        assert_lineno = lineno\n+    elif assert_lineno is not None:\n             # keep track of depth for the assert-message `,` lookup\n             if tp == tokenize.OP and source in \"([{\":\n                 depth += 1\n             elif tp == tokenize.OP and source in \")]}\":\n                 depth -= 1\n \n+\n             if not lines:\n                 lines.append(line[offset:])\n                 seen_lines.add(lineno)\n@@ -600,11 +649,28 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n                 else:\n                     lines.append(line[:offset])\n                 _write_and_reset()\n+\n             elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                 _write_and_reset()\n             elif lines and lineno not in seen_lines:\n-                lines.append(line)\n-                seen_lines.add(lineno)\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n \n     return ret\n \n",
    "variant_7": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\nindex 456681ab2..f57d4b25d 100644\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -554,6 +554,8 @@ def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n         yield from traverse_node(child)\n \n \n+@functools.lru_cache(maxsize=1)\n+\n @functools.lru_cache(maxsize=1)\n def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n@@ -576,14 +578,62 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     tokens = tokenize.tokenize(io.BytesIO(src).readline)\n     for tp, source, (lineno, offset), _, line in tokens:\n         if tp == tokenize.NAME and source == \"assert\":\n+            if lines:\n+                _write_and_reset()\n             assert_lineno = lineno\n         elif assert_lineno is not None:\n+            if tp == tokenize.OP and source in \"([{\":\n+                depth += 1\n+            elif tp == tokenize.OP and source in \")]}\":\n+                depth -= 1\n+            elif tp in {tokenize.NEWLINE, tokenize.NL}:\n+                if depth <= 0:\n+                    _write_and_reset()\n+                    continue\n+            elif tp == tokenize.ENDMARKER:\n+                if lines:\n+                    _write_and_reset()\n+                break\n+\n+            # Capture assertion lines, but stop if we encounter a decorator\n+            if tp != tokenize.COMMENT:\n+                if assert_lineno == lineno:\n+                    if not lines:\n+                        lines.append(line[offset:])\n+                        seen_lines.add(lineno)\n+                elif lines and lineno not in seen_lines:\n+                    # Check if this line is a decorator (starts with @)\n+                    stripped_line = line.lstrip()\n+                    if stripped_line.startswith('@'):\n+                        # Stop collecting lines when we hit a decorator\n+                        _write_and_reset()\n+                    else:\n+                        lines.append(line)\n+                        seen_lines.add(lineno)\n+\n+    return ret\n+    def _write_and_reset() -> None:\n+        nonlocal depth, lines, assert_lineno, seen_lines\n+        assert assert_lineno is not None\n+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n+        depth = 0\n+        lines = []\n+        assert_lineno = None\n+        seen_lines = set()\n+\n+\n+tokens = tokenize.tokenize(io.BytesIO(src).readline)\n+for tp, source, (lineno, offset), _, line in tokens:\n+    if tp == tokenize.NAME and source == \"assert\":\n+        assert_lineno = lineno\n+    elif assert_lineno is not None:\n             # keep track of depth for the assert-message `,` lookup\n             if tp == tokenize.OP and source in \"([{\":\n                 depth += 1\n             elif tp == tokenize.OP and source in \")]}\":\n                 depth -= 1\n \n+\n             if not lines:\n                 lines.append(line[offset:])\n                 seen_lines.add(lineno)\n@@ -600,11 +650,28 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n                 else:\n                     lines.append(line[:offset])\n                 _write_and_reset()\n+\n             elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                 _write_and_reset()\n             elif lines and lineno not in seen_lines:\n-                lines.append(line)\n-                seen_lines.add(lineno)\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n \n     return ret\n \n",
    "variant_8": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\nindex 456681ab2..f3f667981 100644\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -554,6 +554,8 @@ def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n         yield from traverse_node(child)\n \n \n+@functools.lru_cache(maxsize=1)\n+\n @functools.lru_cache(maxsize=1)\n def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n     \"\"\"Return a mapping from {lineno: \"assertion test expression\"}.\"\"\"\n@@ -573,6 +575,57 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n         assert_lineno = None\n         seen_lines = set()\n \n+    tokens = tokenize.tokenize(io.BytesIO(src).readline)\n+    for tp, source, (lineno, offset), _, line in tokens:\n+        if tp == tokenize.NAME and source == \"assert\":\n+            if lines:\n+                _write_and_reset()\n+            assert_lineno = lineno\n+            depth = 0\n+            lines = []\n+            seen_lines = set()\n+\n+        if assert_lineno is not None:\n+            if tp == tokenize.OP:\n+                if source in \"([{\":\n+                    depth += 1\n+                elif source in \")]}\":\n+                    depth -= 1\n+                    if depth < 0:\n+                        _write_and_reset()\n+                        continue\n+            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n+                if depth == 0:\n+                    _write_and_reset()\n+                    continue\n+            elif tp == tokenize.COMMENT:\n+                continue\n+\n+            if lineno == assert_lineno:\n+                if not lines:\n+                    lines.append(line[offset:])\n+                    seen_lines.add(lineno)\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+    return ret\n+    def _write_and_reset() -> None:\n+        nonlocal depth, lines, assert_lineno, seen_lines\n+        assert assert_lineno is not None\n+        ret[assert_lineno] = \"\".join(lines).rstrip().rstrip(\"\\\\\")\n+        depth = 0\n+        lines = []\n+        assert_lineno = None\n+        seen_lines = set()\n+\n+\n     tokens = tokenize.tokenize(io.BytesIO(src).readline)\n     for tp, source, (lineno, offset), _, line in tokens:\n         if tp == tokenize.NAME and source == \"assert\":\n@@ -584,6 +637,7 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n             elif tp == tokenize.OP and source in \")]}\":\n                 depth -= 1\n \n+\n             if not lines:\n                 lines.append(line[offset:])\n                 seen_lines.add(lineno)\n@@ -600,11 +654,28 @@ def _get_assertion_exprs(src: bytes) -> Dict[int, str]:\n                 else:\n                     lines.append(line[:offset])\n                 _write_and_reset()\n+\n             elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:\n                 _write_and_reset()\n             elif lines and lineno not in seen_lines:\n-                lines.append(line)\n-                seen_lines.add(lineno)\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n+\n+            elif lines and lineno not in seen_lines:\n+                # Check if this line is a decorator (starts with @)\n+                stripped_line = line.lstrip()\n+                if stripped_line.startswith('@'):\n+                    # Stop collecting lines when we hit a decorator\n+                    _write_and_reset()\n+                else:\n+                    lines.append(line)\n+                    seen_lines.add(lineno)\n \n     return ret\n \n"
  }
}