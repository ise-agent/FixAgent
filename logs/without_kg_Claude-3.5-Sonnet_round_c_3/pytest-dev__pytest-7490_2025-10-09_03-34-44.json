{
  "zero_patches": {
    "Location 1 (/root/hy/projects_3/pytest/src/_pytest/skipping.py lines 249-251)": "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    # Always re-evaluate xfail markers during call phase to catch dynamically added markers\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n",
    "Location 2 (/root/hy/projects_3/pytest/src/_pytest/skipping.py lines 241-244)": "\n    if not item.config.option.runxfail:\n        # Always evaluate xfail marks during setup to ensure we catch any existing markers\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
    "Location 3 (/root/hy/projects_3/pytest/src/_pytest/skipping.py lines 200-222)": "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
    "Location 4 (/root/hy/projects_3/pytest/src/_pytest/skipping.py lines 264-294)": "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
    "Location 5 (/root/hy/projects_3/pytest/src/_pytest/skipping.py lines 279-293)": "\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n"
  },
  "variant_patches": {
    "Location 1 (/root/hy/projects_3/pytest/src/_pytest/skipping.py lines 249-251)": [
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    # Always re-evaluate xfail marks during call phase to catch dynamically added markers\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    # Always re-evaluate xfail marks during call phase to catch dynamically added markers\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    # Always re-evaluate xfail markers during call phase to catch dynamically added markers\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    # Always re-evaluate xfail markers during call phase to catch dynamically added markers\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    # Always re-evaluate xfail markers during call phase to catch dynamically added markers\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    # Always re-evaluate xfail markers during call phase to catch dynamically added markers\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    # Always re-evaluate xfail marks during call phase to catch dynamically added markers\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    # Always re-evaluate xfail marks during call phase to catch dynamically added markers\n    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n"
    ],
    "Location 2 (/root/hy/projects_3/pytest/src/_pytest/skipping.py lines 241-244)": [
      "\n    if not item.config.option.runxfail:\n        # Always re-evaluate xfail marks during setup to ensure we have the most current state\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n    if not item.config.option.runxfail:\n        # Always re-evaluate xfail marks in case they were added dynamically\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n    if not item.config.option.runxfail:\n        # Always evaluate xfail marks during setup to get initial state\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n    if not item.config.option.runxfail:\n        # Always re-evaluate xfail marks during setup to ensure we have the most current state\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n    if not item.config.option.runxfail:\n        # Always evaluate xfail marks to catch dynamically added markers\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n    if not item.config.option.runxfail:\n        # Always re-evaluate xfail marks during setup to ensure we catch\n        # any markers that may have been added dynamically\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n    if not item.config.option.runxfail:\n        # Always re-evaluate xfail marks during setup to ensure we have the most current state\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n",
      "\n    if not item.config.option.runxfail:\n        # Always re-evaluate xfail marks to catch dynamically added markers\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n"
    ],
    "Location 3 (/root/hy/projects_3/pytest/src/_pytest/skipping.py lines 200-222)": [
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n",
      "\ndef evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n    for mark in item.iter_markers(name=\"xfail\"):\n        run = mark.kwargs.get(\"run\", True)\n        strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n        raises = mark.kwargs.get(\"raises\", None)\n        if \"condition\" not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs[\"condition\"],)\n\n        # Unconditional.\n        if not conditions:\n            reason = mark.kwargs.get(\"reason\", \"\")\n            return Xfail(reason, run, strict, raises)\n\n        # If any of the conditions are true.\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n\n    return None\n"
    ],
    "Location 4 (/root/hy/projects_3/pytest/src/_pytest/skipping.py lines 264-294)": [
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    # Re-evaluate xfail markers during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        if xfailed is not None:\n            item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n    \n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    # Re-evaluate xfail markers during call phase to catch dynamically added markers\n    if call.when == \"call\":\n        xfailed = evaluate_xfail_marks(item)\n        if xfailed is not None:\n            item._store[xfailed_key] = xfailed\n    else:\n        xfailed = item._store.get(xfailed_key, None)\n    \n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n"
    ],
    "Location 5 (/root/hy/projects_3/pytest/src/_pytest/skipping.py lines 279-293)": [
      "\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n",
      "\n    elif not rep.skipped and xfailed:\n        if call.excinfo:\n            raises = xfailed.raises\n            if raises is not None and not isinstance(call.excinfo.value, raises):\n                rep.outcome = \"failed\"\n            else:\n                rep.outcome = \"skipped\"\n                rep.wasxfail = xfailed.reason\n        elif call.when == \"call\":\n            if xfailed.strict:\n                rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n"
    ]
  },
  "combined_diffs": {
    "raw_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..539a592a8 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -238,6 +239,7 @@ def pytest_runtest_setup(item: Item) -> None:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+\n     if not item.config.option.runxfail:\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n@@ -246,9 +248,9 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+    # Always re-evaluate xfail markers during call phase to catch dynamic markers\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n@@ -261,6 +263,7 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n+\n     xfailed = item._store.get(xfailed_key, None)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n",
    "variant_0": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..eb5d77e5c 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -238,7 +239,9 @@ def pytest_runtest_setup(item: Item) -> None:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+\n     if not item.config.option.runxfail:\n+        # Always evaluate xfail marks during setup to ensure we catch any existing markers\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n@@ -246,9 +249,17 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n+    # Always re-evaluate xfail markers during call phase to catch dynamically added markers\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n@@ -257,6 +268,11 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     yield\n \n \n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+\n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n",
    "variant_1": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..bf010dfaa 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -238,7 +239,9 @@ def pytest_runtest_setup(item: Item) -> None:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+\n     if not item.config.option.runxfail:\n+        # Always re-evaluate xfail marks during setup to ensure we have the most current state\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n@@ -246,9 +249,17 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n+    # Always re-evaluate xfail marks during call phase to catch dynamically added markers\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n@@ -257,6 +268,11 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     yield\n \n \n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+\n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n",
    "variant_2": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..799c57bd9 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -238,7 +239,9 @@ def pytest_runtest_setup(item: Item) -> None:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+\n     if not item.config.option.runxfail:\n+        # Always re-evaluate xfail marks in case they were added dynamically\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n@@ -246,9 +249,17 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n+    # Always re-evaluate xfail marks during call phase to catch dynamically added markers\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n@@ -261,7 +272,19 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    # Re-evaluate xfail markers during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        if xfailed is not None:\n+            item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n+    \n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n",
    "variant_3": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..9815ad9e3 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -238,7 +239,9 @@ def pytest_runtest_setup(item: Item) -> None:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+\n     if not item.config.option.runxfail:\n+        # Always evaluate xfail marks during setup to get initial state\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n@@ -246,9 +249,17 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n+    # Always re-evaluate xfail markers during call phase to catch dynamically added markers\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n@@ -257,6 +268,11 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     yield\n \n \n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+\n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n",
    "variant_4": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..5401309c0 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -238,7 +239,9 @@ def pytest_runtest_setup(item: Item) -> None:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+\n     if not item.config.option.runxfail:\n+        # Always re-evaluate xfail marks during setup to ensure we have the most current state\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n@@ -246,9 +249,17 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n+    # Always re-evaluate xfail markers during call phase to catch dynamically added markers\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n@@ -257,6 +268,11 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     yield\n \n \n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+\n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n",
    "variant_5": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..f74c66a89 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -238,7 +239,9 @@ def pytest_runtest_setup(item: Item) -> None:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+\n     if not item.config.option.runxfail:\n+        # Always evaluate xfail marks to catch dynamically added markers\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n@@ -246,9 +249,17 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n+    # Always re-evaluate xfail markers during call phase to catch dynamically added markers\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n@@ -257,6 +268,11 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     yield\n \n \n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+\n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n",
    "variant_6": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..5009121cd 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -238,7 +239,10 @@ def pytest_runtest_setup(item: Item) -> None:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+\n     if not item.config.option.runxfail:\n+        # Always re-evaluate xfail marks during setup to ensure we catch\n+        # any markers that may have been added dynamically\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n@@ -246,9 +250,17 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n+    # Always re-evaluate xfail markers during call phase to catch dynamically added markers\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n@@ -261,7 +273,19 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+    # Re-evaluate xfail markers during call phase to catch dynamically added markers\n+    if call.when == \"call\":\n+        xfailed = evaluate_xfail_marks(item)\n+        if xfailed is not None:\n+            item._store[xfailed_key] = xfailed\n+    else:\n+        xfailed = item._store.get(xfailed_key, None)\n+    \n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n",
    "variant_7": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..bf010dfaa 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -238,7 +239,9 @@ def pytest_runtest_setup(item: Item) -> None:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+\n     if not item.config.option.runxfail:\n+        # Always re-evaluate xfail marks during setup to ensure we have the most current state\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n@@ -246,9 +249,17 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n+    # Always re-evaluate xfail marks during call phase to catch dynamically added markers\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n@@ -257,6 +268,11 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     yield\n \n \n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+\n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n",
    "variant_8": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..d6121daf8 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -197,6 +197,7 @@ class Xfail:\n     raises = attr.ib(type=Optional[Tuple[\"Type[BaseException]\", ...]])\n \n \n+\n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n     for mark in item.iter_markers(name=\"xfail\"):\n@@ -238,7 +239,9 @@ def pytest_runtest_setup(item: Item) -> None:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+\n     if not item.config.option.runxfail:\n+        # Always re-evaluate xfail marks to catch dynamically added markers\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n@@ -246,9 +249,17 @@ def pytest_runtest_setup(item: Item) -> None:\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n-    xfailed = item._store.get(xfailed_key, None)\n-    if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n+    # Always re-evaluate xfail marks during call phase to catch dynamically added markers\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n+    if not item.config.option.runxfail:\n+        if xfailed and not xfailed.run:\n+            xfail(\"[NOTRUN] \" + xfailed.reason)\n+\n+    yield\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n@@ -257,6 +268,11 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     yield\n \n \n+@hookimpl(hookwrapper=True)\n+def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n+    outcome = yield\n+    rep = outcome.get_result()\n+\n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n"
  }
}